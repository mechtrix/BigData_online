---
title: "(P)artitioning (A)round (M)edoids"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
    include-in-header:
      text: |
        <script>
        MathJax = {
          loader: {
            load: ['[tex]/boldsymbol']
          },
          tex: {
            tags: "all",
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
            packages: {
              '[+]': ['boldsymbol']
            }
          }
        };
        </script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
  - pseudocode
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false


library(tidyverse)
library(cluster)
library(broom)
library(factoextra)
library(patchwork)


```


## Robustness to Noise and Outliers
   
   - **PAM**: Uses medoids (actual data points) as cluster centers, which makes it more robust to noise and outliers. The medoid is less influenced by outliers compared to the centroid used in K-means.
   
   - **K-means**: Uses centroids (mean of the points in a cluster) as cluster centers, which can be significantly influenced by outliers and noise.

## Interpretability
   
   - **PAM**: Since medoids are actual data points from the dataset, the cluster centers are more interpretable and meaningful in real-world scenarios.
   
   - **K-means**: Centroids are not actual data points but rather the mean of the cluster members, which might be less intuitive to interpret.

## Suitable for Arbitrary Distance Metrics
   
   - **PAM**: Can use any distance metric, making it more flexible for different types of data (e.g., categorical, ordinal, or non-Euclidean).
   
   - **K-means**: Typically uses Euclidean distance, which may not be suitable for all types of data or similarity measures.

## Cluster Shape

   - **PAM**: Can handle clusters of various shapes and sizes better than K-means because it doesn't assume spherical clusters centered around a mean.
   
   - **K-means**: Assumes clusters are spherical and equally sized, which might not always be the case.

## Convergence to a Global Optimum
   
   - **PAM**: Uses a more exhaustive search (swap between medoids and non-medoids) to find the optimal set of medoids, which can lead to better overall clustering results.
   
   - **K-means**: Relies on iterative refinement, which can get stuck in local optima and is sensitive to the initial placement of centroids.

## Disadvantages to k-means

1. **Scalability**:
   
   - **PAM**: Computationally more expensive and less scalable to large datasets.
   - **K-means**: Computationally efficient and more suitable for large datasets.

2. **Implementation Complexity**:
   
   - **PAM**: More complex to implement and requires more computation time.
   
   - **K-means**: Simpler and faster to implement.

## the idea


{{< include img/_pam_anim.qmd >}}

## doing it

- `data("USArrests")`
- $50$ observations
- $4$ variables:
  - `Murder`: number of murder arrests per $100,000$
  - `Assault`: number of assault arrests per $100,000$
  - `UrbanPop`: percent urban population
  - `Rape`: number of rape arrests per $100,000$

::: {.attribution}

[@practical_kassambara_2017]

:::

## histograms

```{r}
data("USArrests")

long <- USArrests |> 
  pivot_longer(
    cols = c(Murder, Assault,UrbanPop,Rape),
    names_to = "Crime",
    values_to = "value"
  )

long |> 
  ggplot(
    aes(
      x = value
    )
  )+
  geom_histogram(
    color = "white",
    fill = "steelblue",
    bins = 20
  )+
  facet_wrap(
    ~Crime,
    scales = "free"
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  theme_bw(
    base_size = 15
  )

```

## density

```{r}

long |> 
  ggplot(
    aes(
      x = value
    )
  )+
  geom_density(
    color = "white",
    fill = "steelblue"
  )+
  facet_wrap(
    ~Crime,
    scales = "free"
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  theme_bw(
    base_size = 15
  )

```

## multivariate

```{r}

plot(USArrests)

```

## optimal number of clusters

```{r}
df <- scale(USArrests) # different scales!

fviz_nbclust(df,cluster::pam,method = "silhouette")+
  theme_classic(
    base_size = 15
  )

```

## average silhouette method

- compute the average distance ($a(i)$) of each point within **its** cluster (within cluster distance)

- compute the distance of each point to all points in the **nearest** cluster (next closest cluster)

Silhouette Score:

\begin{align}
s(i) = \frac{b(i)-a(i)}{max(a(i),(b(i)))}
\end{align}

## average silhouette method: example on toy data

```{r}
cluster_data <- data.frame(
  feature_01 = c(1,1,2.5,3.5,5,5.5,7,
                 3,4,5,5,6,8.5,8.5
                 ),
  feature_02 = c(5.5,4,1,3,2.5,3.5,1.5,
                 9,6,9,6,7.5,5,3.5
                 ),
  cluster = c(rep("triangle",7),
              rep("circle",7)
              ) |> as.factor()
)

cluster_data |> 
  ggplot(
    aes(
      x = feature_01,
      y = feature_02,
      shape = cluster
    )
  )+
  geom_point(
    size = 5
  )+
  theme_bw(
    base_size = 15
  )+
  theme(legend.position = "bottom")

```


## modeling (k = 2)

```{r}
#| echo: true

clust_mdl <- pam(cluster_data,2)

point_assigned <- augment(clust_mdl,cluster_data)
```


```{r}

raw <- point_assigned |> 
  rowid_to_column(var="idx") |> 
  ggplot(
    aes(
      x = feature_01,
      y = feature_02,
      shape = cluster,
      color = cluster,
      label = idx
    )
  )+
  geom_point(
    size = 8
  )+
  geom_text(
    color = "white"
  )+
  labs(
    title = "cluster in data"
  )+
  scale_color_brewer(palette = "Set1")+
  theme_bw(
    base_size = 15
  )+
  theme(legend.position = "bottom")

new <- point_assigned |> 
  rowid_to_column(var = "idx") |> 
  ggplot(
    aes(
      x = feature_01,
      y = feature_02,
      shape = .cluster,
      color = .cluster,
      label = idx
    )
  )+
  geom_point(
    size = 8
  )+
  geom_text(
    color = "white"
  )+
  labs(
    title = "cluter in PAM"
  )+
  scale_color_brewer(palette = "Set1")+
  theme_bw(
    base_size = 15
  )+
  theme(legend.position = "bottom")

raw+new

```

## silhouette plot (n = 2)

```{r}
#| output: true

fviz_silhouette(
  pam(cluster_data,2),
  label = TRUE,
  print.summary = TRUE)+
  theme_bw(
    base_size = 15
  )+
  scale_fill_brewer(
    palette = "Set1"
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )



```

## silhouette plot (n = 2,3,4,5)

```{r}
#| fig-height: 10
#| fig-align: "center"

plt_sil <- function(dataset,n){
  out <- fviz_silhouette(
  pam(
    dataset,
    n
    ),
  label = TRUE,
  print.summary = FALSE)+
  theme_bw(
    base_size = 15
  )+
  scale_fill_brewer(
    palette = "Set1"
  )
  return(out)
}

plt_sil(cluster_data,2)/plt_sil(cluster_data,3) / plt_sil(cluster_data,4) / plt_sil(cluster_data,5)



```

## cluster plot on toy data

```{r}

fviz_cluster(pam(cluster_data,2))+fviz_cluster(pam(cluster_data,4))&theme_bw(base_size = 15)&theme(legend.position = "bottom")

```

## back to business

```{r}
fviz_nbclust(df,pam,method = "silhouette")+
  theme_classic(
    base_size = 15
  )

```

### for k = 2,3,4,5


```{r}
#| fig-width: 15
#| fig-height: 15

plt_sil(scale(USArrests),2)/plt_sil(scale(USArrests),3) / plt_sil(scale(USArrests),4) / plt_sil(scale(USArrests),5)

```


### elbow plot

```{r}
library(cluster)     # For PAM
library(factoextra)  # For visualization

# Prepare the data
data("USArrests")
df <- na.omit(USArrests)       # Remove missing values if any
df <- scale(df)                # Standardize the data

# Compute the total within-cluster dissimilarity for different k
wss <- numeric(10)

for (k in 1:10) {
  pam_fit <- pam(df, k)
  wss[k] <- pam_fit$objective[1]
}

# Create the elbow plot
plot(1:10, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of clusters K",
     ylab = "Total within-cluster sum of squares",
     main = "Elbow Method for Optimal K in PAM")
```


## cluster plot

```{r}
fviz_cluster(pam(df,2))+
  theme_bw(base_size = 15)+
  theme(legend.position = "bottom")+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")

mdl_arrest <- pam(df,2)

pts <- augment(mdl_arrest,USArrests)
```

---

```{r}
pts |> 
  ggplot(
    aes(
      x = UrbanPop,
      y = Assault,
      color = .cluster
    )
  )+
  geom_point(
    aes()
  )+
  geom_text(
    aes(
      label = .rownames
    )
  )

```

---

```{r}
pts |> 
  ggplot(
    aes(
      x = UrbanPop,
      y = Murder,
      color = .cluster
    )
  )+
  geom_point(
    aes()
  )+
  geom_text(
    aes(
      label = .rownames
    )
  )

```

---

```{r}
pts |> 
  ggplot(
    aes(
      x = UrbanPop,
      y = Rape,
      color = .cluster
    )
  )+
  geom_point(
    aes()
  )+
  geom_text(
    aes(
      label = .rownames
    )
  )

```
## cluster output

```{r}
#| output: true

pam(df,2)

```



# References
