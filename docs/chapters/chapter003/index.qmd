---
title: "Data Quality"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(ggthemes)
library(SensorLab)
library(rstatix)
library(ggh4x)
library(gt)
library(qqplotr)
library(SixSigma)
library(Hmisc)
library(gtsummary)
library(mice)

```

# Data Quality

![](img/ML_def.jpg){fig-align="center"}

## Quality

Quality (dictionary)
: How good or bad something is. [Cambridge Dictionary](https://dictionary.cambridge.org/dictionary/english/quality)

"... Quality is a perceptual, conditional, and somewhat subjective attribute and may be understood differently by different people. ..." [Wiki](https://en.wikipedia.org/wiki/Quality_(business))

## The Cathedral and the Bazaar 

{{< include img/_cathedral_and_bazaar_anim.qmd >}}

::: {.attribution}
[@9780596001087]
:::

## ISO 8000 

ISO
: International Standards Organization

::: {.incremental}

* set of standards for data quality for the exchange between organizations and systems

* Master data in the form of characteristic data that are 
  - exchanged between organizations and systems, and that 
  - conform to the data specifications that 
  - can be validated by computer software.

:::

::: {.attribution}
[@iso8000]
:::

### Part 1: General Requirements {.smaller}

::: {.incremental}

* Part 1.a The master data message shall unambiguously state all information necessary for the receiver to determine its meaning
* Part 1.b A formal syntax must be specified using a formal notation
* Part 1.c Any data specification required by the message shall be in a computer interpretable language
* Part 1.d The message must explicitly indicate both the data specifications it  fulfills and the formal syntax (or syntaxes) to which it complies
* Part 1.e It must be possible to check the correctness of the master data message against both its formal syntax and its data specifications
* Part 1.f The references within the master data message to data dictionary entries must be in the form of unambiguous identifiers conforming to an internationally recognized scheme

:::


### Part 2: Syntax of the message

::: {.incremental}

* The message shall contain in its header a reference to the formal syntax to which it complies.
* The reference shall be an unambiguous identifier for the specific version of the formal syntax used to encode the message.
* The formal syntax shall be available to all interested parties.
* ```<?xml version=1.0”?>```

:::

### Part 3: Semantic encoding {.smaller}

* Part 1.f The references within the master data message to data dictionary entries must be in the form of unambiguous identifiers conforming to an internationally recognized scheme

URI
:   Uniform Resource Identifier - a unique sequence of characters that identifies an abstract or physical resource, such as resources on a webpage, mail address, phone number, books, real-world objects such as people and places, concepts.

RFC
:   Request for Comments - the first technical specifications of the inner workings of the internet [RFC 1149—A standard for the transmission of IP datagrams on avian carriers](https://www.rfc-editor.org/rfc/rfc1149.txt)

### Semantic encoding example

(property1, value1), (property2, value2), ., (propertyN, valueN)

~~Message: (Name, “John Doe”), (Income, “A”), ...~~

Message: (ICTIP.Property.ABC.101, “John Doe”), (ICTIP.Property.ABC.105, “A”), ...

#### Data dictionary!!!

::: {.incremental}

1. unique identifier (URI) - [RFC 3986](https://datatracker.ietf.org/doc/html/rfc3986)

2. term (name)

3. clear definition
:::

```
Identifier: ICTIP.Property.ABC.105
Term: Individual_Income_Bracket
Definition:  Range of individual income given in increments of $25,000 starting at $0 and coded with single letters “A”, “B”, “C”, “D”, and “E” 
“A” for [0 - 25,000]
“B” for [25,001 - 50,000]
“C” for [50,001 - 75,000]
“D” for [75,001 - 100,000]
“E” for [100,001 and above]
```
## Data Quality  {.smaller}

"... Data are of high quality if they are “Fit for Use” in their intended operational,
decision-making and other roles. ... "

![](img/data_quality_dimensions.svg){fig-align="center"}

::: {.attribution}
[@978-0-387-69502-0]
:::

## Data Quality issues

::: {.incremental}

* [Data Entry for the London Summer Olympics](https://www.bbc.com/news/uk-16409480?utm_source=SolvinFinance&utm_medium=Blog_Post&utm_campaign=The_Wall_of_Shame_for_the_Worst_Excel_Errors)
  + an employee simple typed $20,000$ seats available, while there were only $10,000$
  + buyers were able to exchange the already bought tickets

* [The London Whale](https://en.wikipedia.org/wiki/2012_JPMorgan_Chase_trading_loss)
  + using a risk model that involved copy and pasting different multiple spreadsheats
  + not only were $\$2bn$ losses wrongly calculated, they were actually counted as gains summing up to a loss of $\$6.2bn$

:::

## Data Quality Tools

### Data Elements

![](img/data_elements.png)

#### Data Types

![Possible Data Type](img/013_DataTypes.svg)

#### Nominal Data 

![Examples of Nominal Data](img/014_NominalData.svg)

#### Ordinal Data

![Example Ordinal Data](img/015_OrdinalData.svg)

#### Discrete Data

![Example Discrete Data](img/016_DiscreteData.svg)

#### Continous Data

![Example Continous Data](img/017_ContinousData.svg)

“... All actual sample spaces are discrete, and all observable random variables have discrete distributions. 
The continuous distribution is a mathematical construction, suitable for mathematical treatment, but not practically observable. ...” 

::: {.attribution}
[@Bibby_1980]
:::

### Requirements Documents

::: {.incremental}

* Why should the database be established?
* How are the data to be collected?
* How do analysts plan to use the data?
* What database-design issues may affect these data uses?

:::

### Tests

#### Deterministic Tests

::: {.incremental}

* Range Test
* If-then Test
* Ratio Control Test (similar to range test)
* Zero Control Test (the components of a sum a listed seperartly)
* Other internal consistency tests

:::

#### Statistical Tests

::: {.incremental}

* Outlier detection
  + well covered and not detailed here
* (E)xploratory (D)ata (A)nalysis 

:::

#### Minimizing Processing Errors 

::: {.incremental}

* "...  review of the codes assigned on a schedule is oftentimes not a matter of correcting wrong codes, but merely a matter of honest differences of opinion between coder and reviewer. ..."

* "... even though the two sets of instructions supposedly say the same thing in different words. ..."

* "... it is impossible to define a perfect job of coding except in terms of the <span style="color:red;">distributions</span> produced because there is no way of determining whether the individual codes have been assigned correctly. ..."

:::

::: {.attribution}
[@DemingUnknownTitle2006]
:::

#### Recommendations 

::: {.incremental}

* Readily correct obvious errors that are easy to fix

* Preserve the orginal dataset (Excel)

* Save intermediate versions (Version Control)

* Do not over edit.

* Do consistency checks.

* Build in redundancy (ZIP Code AND postal adress)

:::

::: {.attribution}
[@978-0-387-69502-0]
:::


### Measuring Data Quality 

::: {.incremental}

* "... trust and data quality are inextricably linked ..."

* "... The business user should invest in, on a prioritized basis, establishing data quality metrics for datasets and stores of interest, as well as in building data quality reporting processes for them. ...

:::

::: {.attribution}
[@FleckensteinUnknownTitle2018]
:::

### Data Quality Tools 

{{< include img/_data_quality_tools_anim.qmd >}}

---

::: {.r-fit-text .v-c}

It's a mess!

:::

::: {.attribution}
[@FleckensteinUnknownTitle2018]
:::

### Data Quality in Production

::: {.r-stack}

::: {.fragment .fade-out}

“... All actual sample spaces are discrete, and all observable random variables have discrete distributions. 
The continuous distribution is a mathematical construction, suitable for mathematical treatment, but not practically observable. ...” 

::: {.attribution}
[@Bibby_1980]
:::

:::

::: {.fragment .fade-in-then-out}

![](img/production_probabilities.svg)

:::

::: {.fragment .fade-in}

```{r}
#| out-width: 95%

fun.1 <- function(x) 0.9^x
fun.2 <- function(x) 0.95^x
fun.3 <- function(x) 0.997^x
fun.4 <- function(x) 0.99975^x
fun.5 <- function(x) 0.98^x


limit_x <- c(0,5)

ggplot(data = data.frame(x = 0), mapping = aes(x = x))+
  stat_function(fun = fun.2,mapping = aes(linetype = "95%"),lwd=1) + 
  stat_function(fun = fun.3,mapping = aes(linetype = "99.7%"),lwd=1) + 
  stat_function(fun = fun.4,mapping = aes(linetype = "99.975%"),lwd=1) + 
  stat_function(fun = fun.5,mapping = aes(linetype = "98%"),lwd=1) + 
  labs(title = 'Probability for success in sequence',
       x='Step Nr',
       y='Probability for success',
       linetype='Probability in\nsingle step')+
  scale_x_continuous(limits = c(1,limit_x[2]),breaks = seq(limit_x[1],limit_x[2],by=1),expand = c(0,0))+
  scale_y_continuous(breaks = seq(0.2,1,by=0.025))+
  scale_color_brewer(palette = 'Spectral')+
  theme_minimal(
    base_size = 15
  )+
  theme(panel.grid.minor = element_blank())

```


:::

:::

#### How good is good enough

{{< include img/_six_sigma_anim.qmd >}}


#### Process Capability - idea

![](img/003_bullseye.svg)


#### High Accuracy - Low Precision

![](img/004_HALP.svg)


#### Low Accuracy - Low Precision

![](img/005_LALP.svg)

#### Low Accuracy - High Precision

![](img/006_LAHP.svg)

#### High Accuracy - High Precision

![](img/007_HAHP.svg)

#### Computing Process Capabilities

::: {.r-stack}

::: {.fragment .fade-out}

\begin{align}
C_{p} &= \frac{USL-LSL}{6\sigma}\\
C_{pk} &= \frac{\min(USL-\mu,\mu-LSL)}{3\sigma}\\
\end{align}

:::

::: {.fragment .fade-in}

```{r}
#| out-width: 95%

# source("gt_Cpk_ppm.R")

ppm <- data.frame(
  Cpk = seq(0.5,2,0.01)
) %>% 
  mutate(
    ppm = cmp_ppm_cpk(Cpk)
  )


ppm %>% 
  ggplot(aes(x = Cpk,y= ppm))+
  geom_line(
    linewidth = 2
  )+
  scale_x_continuous(
    breaks = seq(0,3,0.2),
    expand = c(0,0,0,0),
  )+
  scale_y_continuous(
    labels = scales::label_number()
  )+
  geom_vline(
    xintercept = 1.33,
    linetype = "dashed"
  )+
  geom_vline(
    xintercept = 1.68,
    linetype = "dashed"
  )+
  geom_text(
    aes(
      x = 1.33,
      y = 70000,
      label = paste0('ppm: ',cmp_ppm_cpk(1.33)," at Cpk: 1.33")
    ),
    angle = 90,
    nudge_x = -0.05,
    size = 8
  )+
  geom_text(
    aes(
      x = 1.67,
      y = 70000,
      label = paste0('ppm: ',cmp_ppm_cpk(1.67)," at Cpk: 1.67")
    ),
    angle = 90,
    nudge_x = -0.05,
    size = 8
  )+
  geom_text(
    aes(
      x = 2,
      y = 70000,
      label = paste0('ppm: ',cmp_ppm_cpk(2)," at Cpk: 2.00")
    ),
    angle = 90,
    nudge_x = -0.05,
    size = 8
  )+
  labs(
    title = "ppm vs.Cpk"
  )+
  theme_few()
  
```

:::

:::

#### Measurement System Analysis Type 1 (MSA1)


A type 1 gage study assesses only the variation that comes from the gage. Specifically, this study assesses the effects of bias and repeatability on measurements from one operator and one reference part.

::: {.incremental}

* Focus on gage as the only source of variation
* First Step
* 50 Measurements
* repeated on a reference part

:::

##### Potential Capability index $C_g$ {.smaller}

\begin{align}
C_g = \frac{K/100*Tol}{L*\sigma} \label{Cg}
\end{align}

$Tol$
:   Tolerance

$C_g$
:   Capability Gage

K
: percentage of the tolerance ($20\%$)

$\sigma$
:   standard deviations of the tolerance

L
:   number of standard deviations that represent the process (6)

##### Capability index with systematic error $C_{gk}$ {.smaller}

\begin{align}
C_{gk} = \frac{(0.5*K/100)*Tol - |\bar{x}-x_{true}|}{3*\sigma} \label{Cgk}
\end{align}

$Tol$
:   Tolerance

$\bar{x}$
:   mean of the measurements

K
: percentage of the tolerance ($20\%$)

$x_{true}$
:   the "true" value of the reference (calibration)

$\sigma$
:   standard deviation of the measurements

##### theoretical example

::: {.r-stack}

::: {.fragment .fade-out}

![ring gauge $20mm$](img/msa1_ring.jpg){width=50%}

* $x_{true} = 20.3020$

:::

::: {.fragment .fade-in-then-out}

![](img/msa1_data.gif){width=95%}

:::

:::

---

{{< include img/_will_it_blend_anim.qmd >}}


##### Disitribution of the data

::: {.r-stack}


::: {.fragment .fade-in}

```{r}
#| include: false

measured_data <- c(20.3030,
                   20.2960,
                   20.3110,
                   20.2980,
                   20.3110,
                   20.3080,
                   20.3130,
                   20.3030,
                   20.3060,
                   20.3020,
                   20.3010,
                   20.3010,
                   20.2970,
                   20.2950,
                   20.3090,
                   20.3020,
                   20.3030,
                   20.3100,
                   20.2960,
                   20.3030,
                   20.3040,
                   20.3000,
                   20.2950,
                   20.3010,
                   20.3080,
                   20.2940,
                   20.3080,
                   20.3040,
                   20.3060,
                   20.3070,
                   20.3030,
                   20.3070,
                   20.3020,
                   20.3070,
                   20.3040,
                   20.3020,
                   20.2980,
                   20.3090,
                   20.2990,
                   20.3030,
                   20.3060,
                   20.3050,
                   20.3040,
                   20.3120,
                   20.2980,
                   20.3040,
                   20.3060,
                   20.3050,
                   20.3000,
                   20.3050)

measured_data <- data.frame(measured_data = measured_data) %>% 
  rowid_to_column(var = "No")

x_true <- 20.3020

Tol <- 0.3

measured_data %>% 
  shapiro_test(measured_data)

binwidth <- 0.001

p_hist <- measured_data %>% 
  ggplot(
    aes(
      x = measured_data
    )
  )+
  geom_histogram(
    binwidth = binwidth,
    color = "white"
  )+
  scale_x_continuous(
    expand = c(0,0,0,0),
    breaks = scales::pretty_breaks(n = 10)
  )+
  scale_y_continuous(
    breaks = scales::pretty_breaks(n = 5)
  )+
  labs(
    title = "Distribution of measured values",
    y = "count",
    x = "measured data"
  )+
  theme_minimal()

p_dens1 <- p_hist+geom_density(
    aes(
      y = after_stat(count*binwidth)
    ),
    linewidth = 1.5,
    color = "red"
  )

p_dens2 <- p_dens1+stat_theodensity(
    aes(
      y = after_stat(count*binwidth)
    ),
    geom = "area",
    fill = "gray",
    alpha = 0.6
  )


x_mean <- measured_data %>% 
  pull(measured_data) %>% 
  mean() %>% 
  round(.,digits = 4)

x_sd <- measured_data %>% 
  pull(measured_data) %>% 
  sd()%>% 
  round(.,digits = 4)

Cg <- round(((20/100)*Tol)/(6*x_sd),digits = 2)
  

Cgk <-  round(((0.5*20/100)*Tol-abs(x_mean-x_true))/(3*x_sd),digits = 2)

```


```{r}
#| out-width: 95%

p_hist

```

:::

::: {.fragment .fade-in}


```{r}
#| out-width: 95%

p_dens1

```

:::

::: {.fragment .fade-in}


```{r}
#| out-width: 95%

p_dens2

```

:::

::: {.fragment .fade-in}


```{r}
#| out-width: 95%


# qq_data <- data.frame(
#   dataset = measured_data
# )
# 
# glimpse(measured_data)

measured_data |>
  ggplot(
    aes(
      sample = measured_data
    )
  )+
  qqplotr::stat_qq_band()+
  qqplotr::stat_qq_line()+
  qqplotr::stat_qq_point()+
  theme_minimal(
    base_size = 15
  )

```

:::

:::

##### Formal test for distribution 

::: {.incremental}

* $H_0$: Data is normal distributed.
* $H_a$: Data is not normal distributed.

:::

::: {.fragment .fade-in}

```{r}
#| echo: true
#| output: true


measured_data %>% 
  shapiro_test(measured_data)

```


:::

::: {.attribution}
[@shapiro1965analysis]
:::

##### $C_g$ and $C_{gk}$

* $C_g, C_{gk} > 1.33$ empirical, must be cross-checked with customer
* calculation according to $C_g$ and $C_{gk}$


```{r}
#| label: tbl-CgCgk
#| out-width: 75%
#| tbl-cap: $C_g, C_{gk}$ for the measured values

data.frame(Cg = Cg, Cgk = Cgk) %>% 
  gt() %>% 
  tab_options(
  table.font.size = 30
)

```

##### graphical depiction


```{r}
#| label: fig-msa1-plt
#| out-width: 75%
#| fig-cap: MSA 1 graphical depiction.



plt_msa <- measured_data %>% 
  ggplot(
    aes(
      x = No,
      y = measured_data
    )
  )+
  geom_point()+
  geom_line()+
  geom_hline(
    aes(
      yintercept = mean(measured_data),
      linetype = "mean"
    )
  )+
  geom_hline(
    aes(
      yintercept = mean(measured_data)+0.1*Tol/2,
      linetype = "20% Tol"
    ),
    color = "green4",
    linewidth = 1.5
  )+
  geom_hline(
    aes(
      yintercept = mean(measured_data)-0.1*Tol/2,
      linetype = "20% Tol"
    ),
    color = "green4",
    linewidth = 1.5
  )+
  geom_hline(
    aes(
      yintercept = mean(measured_data)+sd(measured_data),
      linetype = "+/-sd"
    )
  )+
  geom_hline(
    aes(
      yintercept = mean(measured_data)-sd(measured_data),
      linetype = "+/-sd"
    )
  )+
  geom_hline(
    aes(
      yintercept = x_true,
      linetype = "x_true"
    ),
    linewidth = 1
  )+
  labs(
    title = "MSA 1 Data vs. index",
    x = "No",
    y = "measured data",
    linetype = ""
  )+
  scale_x_continuous(
    breaks = seq(0,100,5)
  )+
  scale_y_continuous(
    breaks = scales::pretty_breaks(n = 10)
  )+
  theme_minimal(base_size = 15)+
  theme(
    legend.position = "bottom"
  )

plt_msa

```

##### ... the era of AI?

::: {.r-stack}

::: {.fragment .fade-out}

![](img/i_blistering.svg){width=1500px}

:::

::: {.fragment .fade-in-then-out}

![](img/mm_algo_wf.svg){width=2500px}

:::

::: {.fragment .fade-in-then-out}

![](img/mm_algo_out01.svg){width=2500px}

:::

::: {.fragment .fade-in-then-out}

![](img/mm_algo_out02.svg){width=2500px}

:::

::: {.fragment .fade-in-then-out}

![](img/res_algo_rr.svg){width=2500px}

:::

:::

#### Gage R&R 

::: {.r-stack}


::: {.fragment .fade-out}

A Gage R&R study assesses the variation in measurements from a specific process by measuring the same parts multiple times with the same instrument by different operators. It helps determine the reliability of the measurement system and identifies areas for improvement.

:::

::: {.fragment .fade-in}

{{< include img/_gageRR_anim.qmd >}}

:::

:::

::: {.attribution}
[@CanoUnknownTitle2012]
:::

#### Definitions {.smaller}

::: {.fragment .fade-in}

Accuracy
:   The closeness of agreement between a test result and the accepted reference value.

:::

::: {.fragment .fade-in}

Trueness
:   The closeness of agreement between the average value obtained from a large series of test results and an accepted reference value.

:::

::: {.fragment .fade-in}

Precision 
:   The closeness of agreement between independent test results obtained under stipulated conditions.

:::

::: {.fragment .fade-in}

Repeatability 
:   Precision under repeatability conditions (where independent test results are obtained using the same method on identical test items in the same laboratory by the same operator using the same equipment within short intervals of time).

:::

::: {.fragment .fade-in}

Reproducibility 
:   Precision under reproducibility conditions (where test results are obtained using the same method on identical test items in different laboratories with different operators using different equipment).

:::

::: {.attribution}
[@CanoUnknownTitle2012]
:::

#### Introductory example

* A battery manufacturer makes several types of batteries for domestic use.
* Voltage is **C**ritical **T**o **Q**uality (CTQ)

::: {.incremental}

* the parts are the batteries $a = 3$
* the appraisers are the voltmeters $b = 2$
* measurement is taken three times $n = 3$
* $a \times b \times n = 3 \times 2 \times 3 = 18$ measurements

:::

#### The data

```{r}
#| out-width: 75%

data("ss.data.batteries")

pos <- position_jitter(width = 0.3, seed = 2)

ss.data.batteries %>% 
  ggplot(
    aes(
      x = run,  
      y = voltage
      )
    ) +
  geom_point(
    
  )+
  facet_nested(
    ~battery+voltmeter,
    labeller = label_both
  )+
  theme_bw(base_size = 20)+
  theme(
    legend.position = "bottom"
  )

```

#### The analysis

```{r}
#| echo: true
#| output: true

anova(lm(voltage ~ battery + voltmeter + battery * voltmeter, 
         data = ss.data.batteries))

```

---

::: {.r-fit-text .v-c}

WOW!

:::

#### Variance decomposition - the theory 

![](img/gage_rr.svg){width=95%}

##### Repeatability

\begin{align}
\sigma^2_{Repeatability} = MSE
\end{align}

* directly obtainable in ANOVA table

##### Reproducibility {.smaller}

::: {.r-stack}

::: {.fragment .fade-out}

\begin{align}
\sigma^2_{Reproducibilty} = \sigma^2_{Appraiser} + \sigma^2_{Interaction}
\end{align}

:::

::: {.fragment .fade-in-then-out}

\begin{align}
\sigma^2_{Appraiser} = \frac{MSB-MSAB}{a \times n}
\end{align}

$\sigma^2_{Appraiser}$
:   Variance introduced by appraisers

$MSB$
:   Mean of squares - B

$MSAB$
:   Mean squares of interaction - AB

$a$
:   number of levels for factor - number of batteries: 3

$n$
:   number of replicated measures: 3

:::

::: {.fragment .fade-in-then-out}

\begin{align}
\sigma^2_{Interaction} = \frac{MSBA-MSE}{n}
\end{align}

$\sigma^2_{Interaction}$
:   Variance introduced by interaction

$MSAB$
:   Mean squares of interaction - AB

$MSE$
:   Mean squares of error

$n$
:   number of replicated measures: 3

:::

:::

##### Gage R&R

\begin{align}
\sigma^2_{Gage\;R\&R} = \sigma^2_{Repeatability} + \sigma^2_{Reproducibility}
\end{align}

::: {.fragment .fade-in}
All variance is calculated that comes from the Gage!
:::

::: {.fragment .fade-in}
Are we finished?
:::

::: {.fragment .fade-in}
We measure *something*, so what about the part?
:::

##### Part to Part {.smaller}

\begin{align}
\sigma^2_{Part\; to \; Part} = \frac{MSA-MSAB}{b \times n}
\end{align}

$\sigma^2_{Part\; to \; Part}$
:   Variance introduced by the parts

$MSA$
:   Mean of squares - A

$MSAB$
:   Mean squares of interaction - AB

$b$
:   number of appraisers - number of voltmeters: 2

$n$
:   number of replicated measures: 3

##### Total Variability

{{< include img/_gageRR_total_anim.qmd >}}

#### Variance decomposition - the values

```{r}
raw_gagerr <- anova(lm(voltage ~ battery + voltmeter + battery * voltmeter,  data=ss.data.batteries))

n <- 3

a <- 3

b <- 2

```

\begin{align}
\sigma^2_{Repeatability} &= `r raw_gagerr[3][4,1] %>% round(.,digits = 4)` \nonumber \\
\sigma^2_{Appraiser} &= `r ((raw_gagerr[3][2,1] - raw_gagerr[3][3,1])/(a*n))%>% round(.,digits = 4)` \nonumber \\
\sigma^2_{Interaction} &= `r ((raw_gagerr[3][3,1]-raw_gagerr[3][3,1])/(n))%>% round(.,digits = 4)` <0 \rightarrow 0 \nonumber \\
\sigma^2_{Reproducibility} &= `r ((raw_gagerr[3][2,1] - raw_gagerr[3][3,1])/(a*n))%>% round(.,digits = 4)` \nonumber \\
\sigma^2_{Gage\;R\&R} &= `r (raw_gagerr[3][4,1] + (raw_gagerr[3][2,1] - raw_gagerr[3][3,1])/(a*n))%>% round(.,digits = 4)` \nonumber \\
\sigma^2_{Part\; to \; Part} &= `r ((raw_gagerr[3][1,1] - raw_gagerr[3][3,1])/(b*n))%>% round(.,digits = 4)` \nonumber \\
\sigma^2_{Total} &= `r ((raw_gagerr[3][1,1] - raw_gagerr[3][3,1])/(b*n) + raw_gagerr[3][4,1] + (raw_gagerr[3][2,1] - raw_gagerr[3][3,1])/(a*n))%>% round(.,digits = 4)` \nonumber
\end{align}

#### ANOVA

{{< include img/_ANOVA_nd_anim.qmd >}}

##### Sum of squared error (SSE) and Mean Squared Error (MSE)

{{< include img/_SSE_anim.qmd >}}

##### ANOVA complete model

```{r}
#| out-width: 75%

drive_shaft <- read.csv("https://raw.githubusercontent.com/mechtrix/BigData_online/refs/heads/main/data/drive_shaft.csv")
# 
grp_dat <- drive_shaft %>% filter(group %in% c("group01","group04","group03")) %>%
  ungroup() %>%
  group_by(group) %>%
  mutate(
    fit = mean(diameter),
    error = fit-diameter,
    sq_error = error^2
  )

mean_dat <- drive_shaft %>% filter(group %in% c("group01","group04","group03")) %>%
  mutate(
    fit = mean(diameter),
    error = fit-diameter,
    sq_error = error^2
  )

mean_grp_dat <-  grp_dat %>%
  group_by(group) %>%
  summarise(
    mean_diameter = mean(diameter)
  )

grp_dat %>%
  ggplot()+
  geom_segment(
    aes(
      x = sample_no,
      xend = sample_no,
      y = fit,
      yend = diameter
    )
  )+
  geom_point(
    aes(x = sample_no,y = diameter)
  )+
  geom_hline(
    data = mean_grp_dat,
    aes(
      yintercept = mean_diameter
      )
    )+
  facet_wrap(
    ~group)+
  labs(
    title = "computations of error for the complete model",
    subtitle = "model = mean per group",
    x = "sample no",
    y = "diameter"
  )+
  theme_few()

```

##### ANOVA reduced model

```{r}
#| out-width: 75%

mean_dat %>%
  ggplot()+
  geom_segment(
    aes(
      x = sample_no,
      xend = sample_no,
      y = fit,
      yend = diameter
    )
  )+
  geom_point(
    aes(x = sample_no,y = diameter)
  )+
  geom_hline(
    # aes(
      yintercept = mean_dat$fit[1],
      # linetype = "overall mean"
    # )
  )+
  facet_wrap(
    ~group
  )+
  labs(
    title = "computations of error for the reduced model",
    subtitle = "model = overall mean",
    x = "sample no",
    y = "diameter"
  )+
  theme_few()


```

##### F(isher) Disitribution

```{r}
#| include: false

# create PRNG
set.seed(123)


# sample norms
norms <- rnorm(2^16)
norms_sq <- norms^2

data_nomal <- data.frame(x = seq(-3, 3, by = 0.01), 
                   y = dnorm(seq(-3, 3, by = 0.01))) %>% 
  mutate(
    x2 = x^2,
    y2 = y^2
  )

plt1 <- ggplot(data.frame(norms),aes(x = norms))+
  geom_ribbon(data = data_nomal,aes(x = x, ymax = y*19000,ymin = 0),fill = "azure3")+
  geom_line(data = data_nomal,aes(x = x, y = y*19000))+
  geom_histogram(alpha = 0.7,color = "white")+
  scale_x_continuous(
    expand = c(0,0,0,0),
    breaks = seq(-5,5)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0),
  )+
  labs(title = "normally distributed, random data points (n=65536)",
       x = "standard deviations",
       y = "counts")+
  theme_few()
  
plt2 <- ggplot(data.frame(norms_sq),aes(x = norms_sq))  +
  geom_histogram(aes(y = ..density..))+
  geom_density() +
  scale_x_continuous(
    expand = c(0,0,0,0),
    limits = c(0,10)
    # breaks = seq(-5,5)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0),
  )+
  labs(title = "squared normally distributed data",
       x = "standard deviations"
       )+
  theme_few()


plt3 <- data.frame(
  dof = seq(2,20,2)
  ) %>% 
  
  mutate(
  x = list(seq(0,50,0.1)),
  d = map2(x,dof,dchisq)
) %>% 
  mutate(dof = as.factor(dof)) %>% 
  unnest(cols = c(x,d)) %>% 
  ggplot(aes(x = x, y = d, linetype = dof))+
  geom_path()+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(title = "PDF of Chi square distribution with varying dof",
       )+
  theme_few()


```


```{r,include=FALSE}

x_vec <-  seq(0.1,2,0.01)

comp_f <- function(dof1,dof2,x = seq(0.1,8,0.1)){
  
  d <- df(x = x, df1 = dof1, df2 = dof2)
  
}

tmp <- expand_grid(
  # dof1 = seq(2,30,3),
  dof1 = c(1,5,10,15,20,25,30,50,100), 
  dof2 = c(1,5,10,15,20,25,30,50,100),
  # dof2 = seq(2,30,3)
  ) %>% 
  mutate(
    x = list(x_vec),
    d = map2(.x = dof1, .y = dof2, function(x,y) comp_f(dof1 = x,dof2 = y, x = x_vec))
  ) %>% 
  mutate(
    dof1 = as.factor(dof1),
    dof2 = as.factor(dof2)) %>% 
  unnest(cols = c(d,x))



plt_dof1_min <- tmp %>% 
  filter(dof1==1) %>%
  ggplot(aes(x = x, y = d))+#, linetype = dof1,color=dof2))+
  geom_line()+
  geom_vline(
    data = tmp %>% 
      filter(dof1==1) %>%
      group_by(dof2) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  geom_text(
    data = tmp %>% 
      filter(dof1==1) %>%
      group_by(dof2) %>% 
      summarise(
        y = max(d),
        x = x[which.max(d)]),
    aes(x = x,y = y, label = round(y,digits = 2)),
    nudge_x = 0.25,
    nudge_y = 0.1,
    )+
  labs(
    title = "varying dof2, dof1 = 1",
  )+
  theme_few()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  facet_wrap(~dof2,labeller = label_both)


plt_dof1_max <- tmp %>% 
  filter(dof1==100) %>%
  ggplot(aes(x = x, y = d))+#, linetype = dof1,color=dof2))+
  geom_line()+
  geom_vline(
    data = tmp %>% 
      filter(dof1==100) %>%
      group_by(dof1) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  labs(
    title = "varying dof1, dof2 = 100",
  )+
  geom_text(
    data = tmp %>% 
      filter(dof1==100) %>%
      group_by(dof2) %>% 
      summarise(
        y = max(d),
        x = x[which.max(d)]),
    aes(x = x,y = y, label = round(y,digits = 2)),
    nudge_x = 0.25,
    nudge_y = 0.1,
    )+
  labs(
    title = "varying dof2, dof1 = 100",
  )+
  theme_few()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  facet_wrap(~dof2,labeller = label_both)


plt_dof2_min <- tmp %>% 
  filter(dof2==1) %>%
  ggplot(aes(x = x, y = d))+#, linetype = dof1,color=dof2))+
  geom_line()+
  geom_vline(
    data = tmp %>% 
      filter(dof2==1) %>%
      group_by(dof1) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  labs(
    title = "varying dof1, dof2 = 1",
  )+
  geom_text(
    data = tmp %>% 
      filter(dof2==1) %>%
      group_by(dof1) %>% 
      summarise(
        y = max(d),
        x = x[which.max(d)]),
    aes(x = x,y = y, label = round(y,digits = 2)),
    nudge_x = 0.25,
    nudge_y = 0.1,
    )+
  labs(
    title = "varying dof1, dof2 = 1",
  )+
  theme_few()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  facet_wrap(~dof1,labeller = label_both)


plt_dof2_max <- tmp %>% 
  filter(dof2==100) %>%
  ggplot(aes(x = x, y = d))+#, linetype = dof1,color=dof2))+
  geom_line()+
  geom_vline(
    data = tmp %>% 
      filter(dof2==100) %>%
      group_by(dof1) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  labs(
    title = "varying dof1, dof2 = 100",
  )+
  geom_text(
    data = tmp %>% 
      filter(dof2==100) %>%
      group_by(dof1) %>% 
      summarise(
        y = max(d),
        x = x[which.max(d)]),
    aes(x = x,y = y, label = round(y,digits = 2)),
    nudge_x = 0.25,
    nudge_y = 0.1,
    )+
  labs(
    title = "varying dof1, dof2 = 1",
  )+
  theme_few()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  facet_wrap(~dof1,labeller = label_both)


max_dat <- expand_grid(
  # dof1 = seq(2,30,3),
  dof1 = seq(1,100), 
  dof2 = seq(1,100),
  # dof2 = seq(2,30,3)
  ) %>% 
  mutate(
    x = list(x_vec),
    d = map2(.x = dof1, .y = dof2, function(x,y) comp_f(dof1 = x,dof2 = y, x = x_vec))
  ) %>% 
  mutate(
    dof1 = as.factor(dof1),
    dof2 = as.factor(dof2)) %>% 
  unnest(cols = c(d,x)) %>% 
  group_by(dof1,dof2) %>% 
  summarise(
    max_d = max(d),
    max_x = x[which.max(d)],
    mean_d = mean(d)
  ) %>% 
  mutate(
    dof1 = as.numeric(as.character(dof1)),
    dof2 = as.numeric(as.character(dof2)),
  ) 

plt_max <- max_dat %>% 
  ggplot(aes(x = dof1, y = dof2))+
  geom_tile(aes(fill = max_d))+
  geom_contour(aes(z = max_d,color = max_d),color = "white",bins = 20)+
  labs(
    title = "maximum density vs. dof1 and dof2"
  )+
  scale_fill_gradient2(low = "white", high = 'black')+
  scale_x_continuous(expand = c(0,0,0,0))+
  scale_y_continuous(expand = c(0,0,0,0))+
  theme_few(
    base_size = 8
  )

plt_max_line <- tmp %>% 
  ggplot(aes(x = x, y = d))+
  geom_path()+
  geom_vline(
    data = tmp %>% 
      # filter(dof2==1) %>%
      group_by(dof1,dof2) %>% 
      summarise(
        x = x[which.max(d)]),
    aes(xintercept = x)
    )+
  labs(
    title = "F - density plots of dof1 vs. dof2"
  )+
  facet_grid(dof1~dof2)+
  theme_few(
    base_size = 8
  )


  
```

::: {.r-stack}

::: {.fragment .fade-out}

* is a "new" distribution function that arises from **two** randomly distributed variables
* "connects" two distributions functions 

\begin{align}
F_{m,n} = \frac{\chi^2_m/m}{\chi^2_n/n} \nonumber
\end{align}

:::

::: {.fragment .fade-in-then-out}

![](img/chi_question.svg)

:::

::: {.fragment .fade-in-then-out}
```{r}
plt1
```

:::

::: {.fragment .fade-in-then-out}
```{r}
plt2
```

:::

::: {.fragment .fade-in-then-out}
```{r}
plt3
```

:::

::: {.fragment .fade-in-then-out}

\begin{align}
F_{m,n} = \frac{\chi^2_m/m}{\chi^2_n/n} \nonumber
\end{align}

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 100%
#| 
plt_max_line
```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 100%
plt_max
```

:::

:::

#### Gage R&R "standardized output"

```{r}
#| echo: false
#| output: false

my.rr <- ss.rr(var = voltage, part = battery,appr = voltmeter,data = ss.data.batteries,main = "Six Sigma Gage R&R Measure",sub = "Batteries Project MSA",print_plot = FALSE,lsl = 1, usl = 2)

```

##### AVNOVA table

```{r}
my.rr$anovaTable
```

##### ANOVA reduced model

```{r}
my.rr$anovaRed
```

##### Variance decomposition

```{r}
my.rr$varComp
```

##### Study Variance

```{r}
my.rr$studyVar
```

##### ndc - number of distinct categories

```{r}
my.rr$ncat
```

##### standardized graphical output

![](img/msa2_ss.png)

#### Gage R&R in the classroom

* 3 parts
* 3 volunteers
* 1 recorder
* 1 gage
* 10 experiments
* 3 repetitions

* randomize the trials
* now do it



#### Attribute Agreement Analysis

Attribute Agreement Analysis (AAA) is a statistical method used to evaluate the agreement among multiple observers when assigning categorical ratings to items. 
It involves defining attributes, selecting observers, collecting ratings, and analyzing the data to determine the level of agreement. 
This helps ensure the reliability of assessments and informs decision-making processes.

##### Setup

![](img/aaa.svg)

#### Results

```{r}
#| include: false

aaa_raw <- data.frame(
  appraiser = c(1,1,1,2,2,2,1,1,1,2,2,2),
  runs = c(1,1,1,1,1,1,2,2,2,2,2,2),
  units = c(3,1,2,3,1,2,3,1,2,3,1,2)
)

reference <- c(
    "bad",
    "good",
    "bad",
    "bad",
    "good",
    "bad",
    "good",
    "bad",
    "bad",
    "good",
    "bad",
    "bad"
  )

results <- c(
  "bad",
  "good",
  "good",
  "good",
  "good",
  "good",
  "good",
  "bad",
  "bad",
  "bad",
  "bad",
  "good"
)


aaa_df <- 
  bind_cols(
    aaa_raw,
    reference = reference,
    results = results
  )

counts <- aaa_df %>% 
  mutate(ref_res = if_else(reference == results, 1, 0))

overall_agreement <- 
  counts %>% 
  summarise(
    sum_ref_res = sum(ref_res),
    count_n = n()
  ) %>% 
  mutate(
    overall_agreement = (sum_ref_res/count_n)*100 
  )%>% pull(overall_agreement) %>% round(.,digits = 1)

single_appraiser_agreement <- 
  counts %>% 
  group_by(
    appraiser
  ) %>% 
  summarise(
    sum_ref_res = sum(ref_res),
    count_n = n()
  ) %>% 
  mutate(
    overall_agreement = (sum_ref_res/count_n)*100 %>% round(.,digits = 1)
  ) 

reference_agreement <- 
  counts %>% 
  group_by(
    reference
  ) %>% 
  summarise(
    sum_ref_res = sum(ref_res),
    count_n = n()
  ) %>% 
  mutate(
    overall_agreement = (sum_ref_res/count_n)*100 %>% round(.,digits = 1)
  ) 

run_agreement <- 
  counts %>% 
  group_by(
    runs
  ) %>% 
  summarise(
    sum_ref_res = sum(ref_res),
    count_n = n()
  ) %>% 
  mutate(
    overall_agreement = (sum_ref_res/count_n)*100 %>% round(.,digits = 1)
  ) 


appraiser_ref_agreement <- 
  counts %>% 
  group_by(
    appraiser,
    reference
  ) %>% 
  summarise(
    sum_ref_res = sum(ref_res),
    count_n = n()
  ) %>% 
  mutate(
    overall_agreement = (sum_ref_res/count_n)*100 %>% round(.,digits = 1)
  ) 

```

```{r}
#| label: tbl-aaa

aaa_df %>% 
  gt()

```

##### Overall agreement

\begin{align}
Agreement_{overall} = 100 \times \frac{X}{N}
\end{align}

$X$
:   number of times appraisers agree with reference

$N$
:   number of rows with valid data

::: {.fragment .fade-in}

\begin{align}
Agreement_{overall} = `r overall_agreement`\% \nonumber
\end{align}

:::

##### Appraiser Agreement

\begin{align}
Agreement_{appraiser} = 100 \times \frac{X}{N}
\end{align}

$X$
:   number of times the single appraisers agrees with reference

$N_i$
:   number of runs for the $i$-th appraiser

::: {.fragment .fade-in}

\begin{align}
Appraiser_{1} &= `r single_appraiser_agreement$overall_agreement[[1]] %>% round(.,digits = 1)`\% \nonumber \\
Appraiser_{2} &= `r single_appraiser_agreement$overall_agreement[[2]] %>% round(.,digits = 1)`\% \nonumber
\end{align}

:::

##### Reference Agreement

\begin{align}
Agreement_{reference} = 100 \times \frac{X}{N}
\end{align}

$X$
:   number of times result agrees with the reference

$N_i$
:   number of runs for the $i$-th result

::: {.fragment .fade-in}

\begin{align}
Reference_{bad} &= `r reference_agreement$overall_agreement[[1]] %>% round(.,digits = 1)`\% \nonumber \\
Reference_{good} &= `r reference_agreement$overall_agreement[[2]] %>% round(.,digits = 1)`\% \nonumber
\end{align}

:::

##### Run agreement

\begin{align}
Agreement_{run} = 100 \times \frac{X}{N}
\end{align}

$X$
:   number of reference agreement in runs

$N_i$
:   number of runs for the $i$-th run

::: {.fragment .fade-in}

\begin{align}
Reference_{1} &= `r run_agreement$overall_agreement[[1]] %>% round(.,digits = 1)`\% \nonumber \\
Reference_{2} &= `r run_agreement$overall_agreement[[2]] %>% round(.,digits = 1)`\% \nonumber
\end{align}

:::

##### Appraiser and reference agreement

\begin{align}
Agreement_{appraiser \; ref} = 100 \times \frac{X}{N}
\end{align}

$X$
:   number of reference agreement in for appraisers in reference class

$N_i$
:   number of agreements for the $i$-th appraiser and the $i$-th standard


```{r}
#| label: tbl-app-ref-agree
#| out-width: 75%

appraiser_ref_agreement %>% 
  select(
    appraiser,
    reference,
    overall_agreement
  ) %>% 
  mutate(
    overall_agreement = overall_agreement/100
  ) %>% 
  ungroup() %>% 
  gt() %>% 
  fmt_percent(columns = overall_agreement) 

```

##### graphical representation

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| out-width: 95%

plt_sngl_app <- single_appraiser_agreement %>% 
  ggplot(
    aes(
      x = appraiser,
      y = overall_agreement
    )
  )+
  geom_col()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  scale_x_continuous(
    breaks = c(1,2)
  )+
  labs(
    title = "single appraiser agreement to reference"
  )+
  theme_minimal(base_size = 20)

plt_sngl_app

```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 95%

plt_ref_agr <- reference_agreement %>% 
  ggplot(
    aes(
      x = reference,
      y = overall_agreement
    )
  )+
  geom_col()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    title = "single reference agreement to reference"
  )+
  theme_minimal(base_size = 20)

plt_ref_agr

```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| out-width: 95%

plt_sngl_run <- run_agreement %>% 
  ggplot(
    aes(
      x = runs,
      y = overall_agreement
    )
  )+
  geom_col()+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  scale_x_continuous(
    breaks = c(1,2)
  )+
  labs(
    title = "single run agreement to reference"
  )+
  theme_minimal(base_size = 20)

plt_sngl_run

```

:::

::: {.fragment .fade-in-then-out}


```{r}
#| out-width: 75%

plt_appr_ref_agr <- appraiser_ref_agreement %>% 
  ggplot(
    aes(
      x = appraiser,
      y = overall_agreement,
      fill = reference
    )
  )+
  geom_col(position = "dodge")+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  scale_x_continuous(
    breaks = c(1,2)
  )+
  scale_fill_brewer(
    palette = "Set1"
  )+
  theme_minimal(base_size = 20)


plt_appr_ref_agr

```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| layout-ncol: 2

plt_sngl_app
plt_sngl_run
plt_ref_agr
plt_appr_ref_agr

```

:::

:::

### Missing values

::: {.r-stack}

::: {.fragment .fade-out}

![](img/missing_values_01.gif)

:::

::: {.fragment .fade-in}

![](img/missing_values_02.gif)

:::

:::

#### What is a missing value?

::: {.r-fit-text .v-c}

Missing data is defined as the values or data that is not stored (or not present) for some variable/s in the given dataset.

:::

#### Representation

::: {.incremental}

1. `Null` or `NA`

2. Blank or Empty Cells

3. Placeholder Values: `-9999`, `-1`, values that are unlikely to occur in the actual data.

4. Special Characters: `?`, `*`


:::

#### Types of missing values 

{{< include img/_missing_data_types_anim.qmd >}}

::: {.attribution}
[@ChristinaMack2018]
:::

##### (M)issing (C)ompletly (A)t (R)andom 

::: {.incremental}

- same probability of data missing for all observations
- completely independent of other data (no pattern)
- human error
- system/equipment failure
- statistical analysis remains unbiased

:::

::: {.attribution}
[@ChristinaMack2018]
:::

##### (M)issing (A)t (R)andom 

::: {.incremental}

- can be explained by other variables (on which information is available)
- only missing in sub-samples of the data
- e.g. `Gender` is complete but `Age` is not
- often Females do not want to reveal their age
- probability depends on variable `Gender` (but missing value can not be predicted)

:::

::: {.attribution}
[@ChristinaMack2018]
:::

##### (M)issing (N)ot (A)t (R)andom 

::: {.incremental}

- not MCAR or MAR
- probability depends on unobserved data
- other observed data **can not** explain it
- e.g. High-income individuals may be more likely to withhold income information
- this may be due to concerns about taxation, privacy or social stigma
- the probability of data missing directly depends on the data itself

:::

::: {.attribution}
[@ChristinaMack2018]
:::

#### treating missing values 

::: {.incremental}

- Deleting missing values
- Imputing missing values
- Imputing missing values for categorical features
- Use as feature

:::

::: {.attribution}
[@Little_2002]
:::

##### Deleting {.smaller}

Pros:

::: {.incremental}

* Easy
* Improves the quality of the dataset by removing incomplete or unreliable observations.
* Prevents bias that may arise from imputing missing values, ensuring more accurate results.
* Reduces computational complexity, as imputation methods can be resource-intensive.

:::

Cons:

::: {.incremental}

* Reduces the size of the dataset, potentially leading to loss of information and statistical power.
* Deletes potentially valuable data points, especially if missingness is not completely random.
* Can introduce bias if missingness is related to the outcome or other variables of interest.
* Eliminates the opportunity to explore patterns or reasons behind missingness, which could provide valuable insights.

:::

##### Imputing missing values 


![](img/ch02-publications-1.png)


::: {.attribution}
[@flexible_buurengroup_2021]
:::

##### Mean/median imputing (Continous Data)

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| output: true
#| echo: true

# Create a vector with missing values
vec <- c(1, 2, NA, 4, NA, 5)

# Check for missing values
is.na(vec)
        
```

```{r}
#| out-width: 75%

data.frame(
  raw_vec = vec
  ) %>% 
  rowid_to_column(var = "varid") %>% 
  ggplot(
    aes(
      x = varid
    )
  )+
  geom_point(
    aes(
      y = raw_vec
    ),
    size = 3,
    shape = 8,
    stroke = 2
  )+
  geom_vline(
    xintercept = c(3,5),
    linewidth = 2
  )+
  scale_x_continuous(
    breaks = seq(0,10)
  )+
  labs(
    title = "raw data with imputed values"
  )+
  theme_bw(
    base_size = 20
  )

```


:::

::: {.fragment .fade-in-then-out}

```{r}
#| echo: true
#| output: true

imputed_vec <- impute(vec, fun = mean)
imputed_vec

```


```{r}
#| out-width: 75%



data.frame(
  imputed_vec = imputed_vec,
  raw_vec = vec,
  spec = c("raw","raw","imputed","raw","imputed","raw")
  ) %>% 
  rowid_to_column(var = "varid") %>% 
  ggplot(
    aes(
      x = varid
    )
  )+
  geom_point(
    aes(
      y = imputed_vec,
      color = spec
    ),
    size = 3,
    stroke = 2,
    shape = 2
  )+
  labs(
    title = "raw data with missing values",
    color = ""
  )+
  theme_bw(
    base_size = 20
  )+
  scale_color_brewer(
    palette = "Set1"
  )+
  theme(
    legend.position = "bottom"
  )

```

:::
:::


##### Mode imputing (Catgorical Data)

```{r}
#| include: false

getmode <- function(v){
  v=v[nchar(as.character(v))>0]
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}


```


::: {.r-stack}

::: {.fragment .fade-out}

```{r}
cat_vec <- c("A", "B", NA, "B", "A", "A")

# Create a vector with missing values
cat_df <- data.frame(
  category = cat_vec
  ) 

cat_df %>% gt()

```


```{r}

cat_df %>% 
  tbl_summary()
```


:::

::: {.fragment .fade-in-then-out}

```{r}
cat_vec <- c("A", "B", NA, "B", "A", "A") %>% impute(.,fun = getmode)

# Create a vector with missing values
cat_df <- data.frame(
  category = cat_vec
  ) 

cat_df %>% gt()

```


```{r}

cat_df %>% 
  mutate(category = as.character(category)) %>% 
  tbl_summary()
```


:::

:::

##### Regression imputing (Continous Data)

::: {.r-stack}

::: {.fragment .fade-out}

```{r}
#| output: true
#| echo: true
#| include: false

# Create a vector with missing values
vec <- c(1, 2, NA, 4, NA, 5)


miss_df <- data.frame(
  raw_vec = vec
  ) %>% 
  rowid_to_column(var = "varid") 

fit <- with(miss_df, lm(raw_vec ~ varid))

```

```{r}
#| out-width: 95%
#| fig-align: "center"

miss_df %>% 
  ggplot(
    aes(
      x = varid
    )
  )+
  geom_point(
    aes(
      y = raw_vec
    ),
    size = 3,
    shape = 8,
    stroke = 2
  )+
  geom_vline(
    xintercept = c(3,5),
    linewidth = 2
  )+
  scale_x_continuous(
    breaks = seq(0,10)
  )+
  labs(
    title = "raw data with missing values"
  )+
  theme_bw(
    base_size = 20
  )

```

:::

::: {.fragment .fade-in-then-out}

```{r}
#| output: true
#| echo: true
#| include: false

# Create a vector with missing values

imp <- mice(miss_df, method = "norm.predict", m = 1, maxit = 1)

new_df <- complete(imp) %>% 
  add_column(
    spec = c("raw","raw","imputed","raw","imputed","raw")
    )

```

```{r}
#| out-width: 95%

new_df %>% 
  ggplot(
    aes(
      x = varid
    )
  )+
  geom_point(
    aes(
      y = raw_vec,
      color = spec
    ),
    size = 3,
    stroke = 2,
    shape = 2
  )+
  labs(
    title = "raw data with imputed values",
    color = ""
  )+
  theme_bw(
    base_size = 20
  )+
  scale_x_continuous(
    breaks = seq(0,10)
  )+
  scale_color_brewer(
    palette = "Set1"
  )+
  theme(
    legend.position = "bottom"
  )


```

:::

:::

##### Imputation is not prediction

::: {style="font-size: 85%;"}

Assessing the discrepancy between true data and the imputed data may seem a simple and attractive way to select the best imputation method. 

Selecting such methods may be harmful as these might increase the rate of false positives. 

In essence, there was not really any information missing in the first place, it was only coded in a different form.

:::

:::{.r-stack}

::: {style="font-size: 120%;"}

Imputation is not prediction.

:::

:::

::: {.attribution}
[@flexible_buurengroup_2021]
:::


## Data Quality summary {.smaller}

::: {.incremental}

- Data Quality is hard
- Top Down or Bottom up?
- simple things go wrong in the real world
- test stuff
- be specific
- there is no recipe
- garbage in, garbage out
- classic ways are not fancy, but useful. Use this to your advantage
- never trust data
- invest early, profit soon

:::

# References
