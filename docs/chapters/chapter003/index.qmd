---
title: "Data Quality"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(ggthemes)
library(SensorLab)
library(rstatix)
library(ggh4x)
library(gt)
library(qqplotr)

```

# Data Quality

![](img/ML_def.jpg){fig-align="center"}

## Quality

Quality (dictionary)
: How good or bad something is. [Cambridge Dictionary](https://dictionary.cambridge.org/dictionary/english/quality)

"... Quality is a perceptual, conditional, and somewhat subjective attribute and may be understood differently by different people. ..." [Wiki](https://en.wikipedia.org/wiki/Quality_(business))

## The Cathedral and the Bazaar 

{{< include img/_cathedral_and_bazaar_anim.qmd >}}

::: {.attribution}
[@9780596001087]
:::

## ISO 8000 

ISO
: International Standards Organization

::: {.incremental}

* set of standards for data quality for the exchange between organizations and systems

* Master data in the form of characteristic data that are 
  - exchanged between organizations and systems, and that 
  - conform to the data specifications that 
  - can be validated by computer software.

:::

::: {.attribution}
[@iso8000]
:::

### Part 1: General Requirements {.smaller}

::: {.incremental}

* Part 1.a The master data message shall unambiguously state all information necessary for the receiver to determine its meaning
* Part 1.b A formal syntax must be specified using a formal notation
* Part 1.c Any data specification required by the message shall be in a computer interpretable language
* Part 1.d The message must explicitly indicate both the data specifications it  fulfills and the formal syntax (or syntaxes) to which it complies
* Part 1.e It must be possible to check the correctness of the master data message against both its formal syntax and its data specifications
* Part 1.f The references within the master data message to data dictionary entries must be in the form of unambiguous identifiers conforming to an internationally recognized scheme

:::


### Part 2: Syntax of the message

::: {.incremental}

* The message shall contain in its header a reference to the formal syntax to which it complies.
* The reference shall be an unambiguous identifier for the specific version of the formal syntax used to encode the message.
* The formal syntax shall be available to all interested parties.
* ```<?xml version=1.0”?>```

:::

### Part 3: Semantic encoding {.smaller}

* Part 1.f The references within the master data message to data dictionary entries must be in the form of unambiguous identifiers conforming to an internationally recognized scheme

URI
:   Uniform Resource Identifier - a unique sequence of characters that identifies an abstract or physical resource, such as resources on a webpage, mail address, phone number, books, real-world objects such as people and places, concepts.

RFC
:   Request for Comments - the first technical specifications of the inner workings of the internet [RFC 1149—A standard for the transmission of IP datagrams on avian carriers](https://www.rfc-editor.org/rfc/rfc1149.txt)

### Semantic encoding example

(property1, value1), (property2, value2), ., (propertyN, valueN)

~~Message: (Name, “John Doe”), (Income, “A”), ...~~

Message: (ICTIP.Property.ABC.101, “John Doe”), (ICTIP.Property.ABC.105, “A”), ...

#### Data dictionary!!!

::: {.incremental}

1. unique identifier (URI) - [RFC 3986](https://datatracker.ietf.org/doc/html/rfc3986)

2. term (name)

3. clear definition
:::

```
Identifier: ICTIP.Property.ABC.105
Term: Individual_Income_Bracket
Definition:  Range of individual income given in increments of $25,000 starting at $0 and coded with single letters “A”, “B”, “C”, “D”, and “E” 
“A” for [0 - 25,000]
“B” for [25,001 - 50,000]
“C” for [50,001 - 75,000]
“D” for [75,001 - 100,000]
“E” for [100,001 and above]
```
## Data Quality  {.smaller}

"... Data are of high quality if they are “Fit for Use” in their intended operational,
decision-making and other roles. ... "

![](img/data_quality_dimensions.svg){fig-align="center"}

::: {.attribution}
[@978-0-387-69502-0]
:::

## Data Quality issues

::: {.incremental}

* [Data Entry for the London Summer Olympics](https://www.bbc.com/news/uk-16409480?utm_source=SolvinFinance&utm_medium=Blog_Post&utm_campaign=The_Wall_of_Shame_for_the_Worst_Excel_Errors)
  + an employee simple typed $20,000$ seats available, while there were only $10,000$
  + buyers were able to exchange the already bought tickets

* [The London Whale](https://en.wikipedia.org/wiki/2012_JPMorgan_Chase_trading_loss)
  + using a risk model that involved copy and pasting different multiple spreadsheats
  + not only were $\$2bn$ losses wrongly calculated, they were actually counted as gains summing up to a loss of $\$6.2bn$

:::

## Data Quality Tools

### Data Elements

![](img/data_elements.png)

#### Data Types

![Possible Data Type](img/013_DataTypes.svg)

#### Nominal Data 

![Examples of Nominal Data](img/014_NominalData.svg)

#### Ordinal Data

![Example Ordinal Data](img/015_OrdinalData.svg)

#### Discrete Data

![Example Discrete Data](img/016_DiscreteData.svg)

#### Continous Data

![Example Continous Data](img/017_ContinousData.svg)

“... All actual sample spaces are discrete, and all observable random variables have discrete distributions. 
The continuous distribution is a mathematical construction, suitable for mathematical treatment, but not practically observable. ...” 

::: {.attribution}
[@Bibby_1980]
:::

### Requirements Documents

::: {.incremental}

* Why should the database be established?
* How are the data to be collected?
* How do analysts plan to use the data?
* What database-design issues may affect these data uses?

:::

### Tests

#### Deterministic Tests

::: {.incremental}

* Range Test
* If-then Test
* Ratio Control Test (similar to range test)
* Zero Control Test (the components of a sum a listed seperartly)
* Other internal consistency tests

:::

#### Statistical Tests

::: {.incremental}

* Outlier detection
  + well covered and not detailed here
* (E)xploratory (D)ata (A)nalysis 

:::

#### Minimizing Processing Errors 

::: {.incremental}

* "...  review of the codes assigned on a schedule is oftentimes not a matter of correcting wrong codes, but merely a matter of honest differences of opinion between coder and reviewer. ..."

* "... even though the two sets of instructions supposedly say the same thing in different words. ..."

* "... it is impossible to define a perfect job of coding except in terms of the <span style="color:red;">distributions</span> produced because there is no way of determining whether the individual codes have been assigned correctly. ..."

:::

::: {.attribution}
[@DemingUnknownTitle2006]
:::

#### Recommendations 

::: {.incremental}

* Readily correct obvious errors that are easy to fix

* Preserve the orginal dataset (Excel)

* Save intermediate versions (Version Control)

* Do not over edit.

* Do consistency checks.

* Build in redundancy (ZIP Code AND postal adress)

:::

::: {.attribution}
[@978-0-387-69502-0]
:::


### Measuring Data Quality 

::: {.incremental}

* "... trust and data quality are inextricably linked ..."

* "... The business user should invest in, on a prioritized basis, establishing data quality metrics for datasets and stores of interest, as well as in building data quality reporting processes for them. ...

:::

::: {.attribution}
[@FleckensteinUnknownTitle2018]
:::

### Data Quality Tools 

{{< include img/_data_quality_tools_anim.qmd >}}

---

::: {.r-fit-text .v-c}

It's a mess!

:::

::: {.attribution}
[@FleckensteinUnknownTitle2018]
:::

### Data Quality in Production

::: {.r-stack}

::: {.fragment .fade-out}

“... All actual sample spaces are discrete, and all observable random variables have discrete distributions. 
The continuous distribution is a mathematical construction, suitable for mathematical treatment, but not practically observable. ...” 

::: {.attribution}
[@Bibby_1980]
:::

:::

::: {.fragment .fade-in-then-out}

![](img/production_probabilities.svg)

:::

::: {.fragment .fade-in}

```{r}
#| out-width: 95%

fun.1 <- function(x) 0.9^x
fun.2 <- function(x) 0.95^x
fun.3 <- function(x) 0.997^x
fun.4 <- function(x) 0.99975^x
fun.5 <- function(x) 0.98^x


limit_x <- c(0,5)

ggplot(data = data.frame(x = 0), mapping = aes(x = x))+
  stat_function(fun = fun.2,mapping = aes(linetype = "95%"),lwd=1) + 
  stat_function(fun = fun.3,mapping = aes(linetype = "99.7%"),lwd=1) + 
  stat_function(fun = fun.4,mapping = aes(linetype = "99.975%"),lwd=1) + 
  stat_function(fun = fun.5,mapping = aes(linetype = "98%"),lwd=1) + 
  labs(title = 'Probability for success in sequence',
       x='Step Nr',
       y='Probability for success',
       linetype='Probability in\nsingle step')+
  scale_x_continuous(limits = c(1,limit_x[2]),breaks = seq(limit_x[1],limit_x[2],by=1),expand = c(0,0))+
  scale_y_continuous(breaks = seq(0.2,1,by=0.025))+
  scale_color_brewer(palette = 'Spectral')+
  theme_minimal(
    base_size = 15
  )+
  theme(panel.grid.minor = element_blank())

```


:::

:::

#### How good is good enough

{{< include img/_six_sigma_anim.qmd >}}


#### Process Capability - idea

![](img/003_bullseye.svg)


#### High Accuracy - Low Precision

![](img/004_HALP.svg)


#### Low Accuracy - Low Precision

![](img/005_LALP.svg)

#### Low Accuracy - High Precision

![](img/006_LAHP.svg)

#### High Accuracy - High Precision

![](img/007_HAHP.svg)

#### Computing Process Capabilities

::: {.r-stack}

::: {.fragment .fade-out}

\begin{align}
C_{p} &= \frac{USL-LSL}{6\sigma}\\
C_{pk} &= \frac{\min(USL-\mu,\mu-LSL)}{3\sigma}\\
\end{align}

:::

::: {.fragment .fade-in}

```{r}
#| out-width: 95%

# source("gt_Cpk_ppm.R")

ppm <- data.frame(
  Cpk = seq(0.5,2,0.01)
) %>% 
  mutate(
    ppm = cmp_ppm_cpk(Cpk)
  )


ppm %>% 
  ggplot(aes(x = Cpk,y= ppm))+
  geom_line(
    linewidth = 2
  )+
  scale_x_continuous(
    breaks = seq(0,3,0.2),
    expand = c(0,0,0,0),
  )+
  scale_y_continuous(
    labels = scales::label_number()
  )+
  geom_vline(
    xintercept = 1.33,
    linetype = "dashed"
  )+
  geom_vline(
    xintercept = 1.68,
    linetype = "dashed"
  )+
  geom_text(
    aes(
      x = 1.33,
      y = 70000,
      label = paste0('ppm: ',cmp_ppm_cpk(1.33)," at Cpk: 1.33")
    ),
    angle = 90,
    nudge_x = -0.05,
    size = 8
  )+
  geom_text(
    aes(
      x = 1.67,
      y = 70000,
      label = paste0('ppm: ',cmp_ppm_cpk(1.67)," at Cpk: 1.67")
    ),
    angle = 90,
    nudge_x = -0.05,
    size = 8
  )+
  geom_text(
    aes(
      x = 2,
      y = 70000,
      label = paste0('ppm: ',cmp_ppm_cpk(2)," at Cpk: 2.00")
    ),
    angle = 90,
    nudge_x = -0.05,
    size = 8
  )+
  labs(
    title = "ppm vs.Cpk"
  )+
  theme_few()
  
```

:::

:::

#### Measurement System Analysis Type 1 (MSA1)


A type 1 gage study assesses only the variation that comes from the gage. Specifically, this study assesses the effects of bias and repeatability on measurements from one operator and one reference part.

::: {.incremental}

* Focus on gage as the only source of variation
* First Step
* 50 Measurements
* repeated on a reference part

:::

##### Potential Capability index $C_g$ {.smaller}

\begin{align}
C_g = \frac{K/100*Tol}{L*\sigma} \label{Cg}
\end{align}

$Tol$
:   Tolerance

$C_g$
:   Capability Gage

K
: percentage of the tolerance ($20\%$)

$\sigma$
:   standard deviations of the tolerance

L
:   number of standard deviations that represent the process (6)

##### Capability index with systematic error $C_{gk}$ {.smaller}

\begin{align}
C_{gk} = \frac{(0.5*K/100)*Tol - |\bar{x}-x_{true}|}{3*\sigma} \label{Cgk}
\end{align}

$Tol$
:   Tolerance

$\bar{x}$
:   mean of the measurements

K
: percentage of the tolerance ($20\%$)

$x_{true}$
:   the "true" value of the reference (calibration)

$\sigma$
:   standard deviation of the measurements

##### theoretical example

::: {.r-stack}

::: {.fragment .fade-out}

![ring gauge $20mm$](img/msa1_ring.jpg){width=50%}

* $x_{true} = 20.3020$

:::

::: {.fragment .fade-in-then-out}

![](img/msa1_data.gif){width=95%}

:::

:::

---

{{< include img/_will_it_blend_anim.qmd >}}


##### Disitribution of the data

::: {.r-stack}


::: {.fragment .fade-in}

```{r}
#| include: false

measured_data <- c(20.3030,
                   20.2960,
                   20.3110,
                   20.2980,
                   20.3110,
                   20.3080,
                   20.3130,
                   20.3030,
                   20.3060,
                   20.3020,
                   20.3010,
                   20.3010,
                   20.2970,
                   20.2950,
                   20.3090,
                   20.3020,
                   20.3030,
                   20.3100,
                   20.2960,
                   20.3030,
                   20.3040,
                   20.3000,
                   20.2950,
                   20.3010,
                   20.3080,
                   20.2940,
                   20.3080,
                   20.3040,
                   20.3060,
                   20.3070,
                   20.3030,
                   20.3070,
                   20.3020,
                   20.3070,
                   20.3040,
                   20.3020,
                   20.2980,
                   20.3090,
                   20.2990,
                   20.3030,
                   20.3060,
                   20.3050,
                   20.3040,
                   20.3120,
                   20.2980,
                   20.3040,
                   20.3060,
                   20.3050,
                   20.3000,
                   20.3050)

measured_data <- data.frame(measured_data = measured_data) %>% 
  rowid_to_column(var = "No")

x_true <- 20.3020

Tol <- 0.3

measured_data %>% 
  shapiro_test(measured_data)

binwidth <- 0.001

p_hist <- measured_data %>% 
  ggplot(
    aes(
      x = measured_data
    )
  )+
  geom_histogram(
    binwidth = binwidth,
    color = "white"
  )+
  scale_x_continuous(
    expand = c(0,0,0,0),
    breaks = scales::pretty_breaks(n = 10)
  )+
  scale_y_continuous(
    breaks = scales::pretty_breaks(n = 5)
  )+
  labs(
    title = "Distribution of measured values",
    y = "count",
    x = "measured data"
  )+
  theme_minimal()

p_dens1 <- p_hist+geom_density(
    aes(
      y = after_stat(count*binwidth)
    ),
    linewidth = 1.5,
    color = "red"
  )

p_dens2 <- p_dens1+stat_theodensity(
    aes(
      y = after_stat(count*binwidth)
    ),
    geom = "area",
    fill = "gray",
    alpha = 0.6
  )


x_mean <- measured_data %>% 
  pull(measured_data) %>% 
  mean() %>% 
  round(.,digits = 4)

x_sd <- measured_data %>% 
  pull(measured_data) %>% 
  sd()%>% 
  round(.,digits = 4)

Cg <- round(((20/100)*Tol)/(6*x_sd),digits = 2)
  

Cgk <-  round(((0.5*20/100)*Tol-abs(x_mean-x_true))/(3*x_sd),digits = 2)

```


```{r}
#| out-width: 95%

p_hist

```

:::

::: {.fragment .fade-in}


```{r}
#| out-width: 95%

p_dens1

```

:::

::: {.fragment .fade-in}


```{r}
#| out-width: 95%

p_dens2

```

:::

::: {.fragment .fade-in}


```{r}
#| out-width: 95%


# qq_data <- data.frame(
#   dataset = measured_data
# )
# 
# glimpse(measured_data)

measured_data |>
  ggplot(
    aes(
      sample = measured_data
    )
  )+
  qqplotr::stat_qq_band()+
  qqplotr::stat_qq_line()+
  qqplotr::stat_qq_point()+
  theme_minimal(
    base_size = 15
  )

```

:::

:::

##### Formal test for distribution 

::: {.incremental}

* $H_0$: Data is normal distributed.
* $H_a$: Data is not normal distributed.

:::

::: {.fragment .fade-in}

```{r}
#| echo: true
#| output: true


measured_data %>% 
  shapiro_test(measured_data)

```


:::

::: {.attribution}
[@shapiro1965analysis]
:::

##### $C_g$ and $C_{gk}$

* $C_g, C_{gk} > 1.33$ empirical, must be cross-checked with customer
* calculation according to $C_g$ and $C_{gk}$


```{r}
#| label: tbl-CgCgk
#| out-width: 75%
#| tbl-cap: $C_g, C_{gk}$ for the measured values

data.frame(Cg = Cg, Cgk = Cgk) %>% 
  gt() %>% 
  tab_options(
  table.font.size = 30
)

```

##### graphical depiction


```{r}
#| label: fig-msa1-plt
#| out-width: 75%
#| fig-cap: MSA 1 graphical depiction.



plt_msa <- measured_data %>% 
  ggplot(
    aes(
      x = No,
      y = measured_data
    )
  )+
  geom_point()+
  geom_line()+
  geom_hline(
    aes(
      yintercept = mean(measured_data),
      linetype = "mean"
    )
  )+
  geom_hline(
    aes(
      yintercept = mean(measured_data)+0.1*Tol/2,
      linetype = "20% Tol"
    ),
    color = "green4",
    linewidth = 1.5
  )+
  geom_hline(
    aes(
      yintercept = mean(measured_data)-0.1*Tol/2,
      linetype = "20% Tol"
    ),
    color = "green4",
    linewidth = 1.5
  )+
  geom_hline(
    aes(
      yintercept = mean(measured_data)+sd(measured_data),
      linetype = "+/-sd"
    )
  )+
  geom_hline(
    aes(
      yintercept = mean(measured_data)-sd(measured_data),
      linetype = "+/-sd"
    )
  )+
  geom_hline(
    aes(
      yintercept = x_true,
      linetype = "x_true"
    ),
    linewidth = 1
  )+
  labs(
    title = "MSA 1 Data vs. index",
    x = "No",
    y = "measured data",
    linetype = ""
  )+
  scale_x_continuous(
    breaks = seq(0,100,5)
  )+
  scale_y_continuous(
    breaks = scales::pretty_breaks(n = 10)
  )+
  theme_minimal(base_size = 15)+
  theme(
    legend.position = "bottom"
  )

plt_msa

```

##### ... the era of AI?

::: {.r-stack}

::: {.fragment .fade-out}

![](img/i_blistering.svg){width=1500px}

:::

::: {.fragment .fade-in-then-out}

![](img/mm_algo_wf.svg){width=2500px}

:::

::: {.fragment .fade-in-then-out}

![](img/mm_algo_out01.svg){width=2500px}

:::

::: {.fragment .fade-in-then-out}

![](img/mm_algo_out02.svg){width=2500px}

:::

::: {.fragment .fade-in-then-out}

![](img/res_algo_rr.svg){width=2500px}

:::

:::


# References
