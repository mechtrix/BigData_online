---
title: "Agglomerative Clustering"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false

library(tidyverse)

```

## Agglomerative Clustering

- Each observation is treated as a single cluster in the beginning (a leaf)
- the most similar clusters are successively merged until there is only one single big cluster (root)
- result is a tree-like representation of the objects, known as **dendogram**

## The idea

{{< include img/_agg_anim.qmd >}}

## What's a dendogram

{{< include img/_dendogram_anim.qmd >}}

## What is similar?

![](img/similar.png)

::: {.fragment .fade-in}

... the classification of observations into groups requires some methods for computing the **distance** or the **(dis)similarity** between each pair of observations. The result of this computation is known as a **dissimilarity** or **distance matrix**.  [@kassambara2017practical]

:::

## distance measures

::: {.incremental}

- Classical
  * Euclidean
  * Manhatten

- Correlation based
  * Pearson correlation
  * Eisen cosine correlation distance
  * Spearman correlation distance
  * Kendall correlation distance

:::

### Manhatten distance


![](img/manhatten.svg){width=2500px fig-align="center"}

\begin{align}
d_{man}(x,y) = \sum_{i=1}^n|x_i-y|
\end{align}

---

::: {style="font-size: 60%;"}

Pros:

1. **Robustness to Outliers**:
   - Less sensitive to outliers compared to Euclidean distance.

2. **Computational Simplicity**:
   - Involves only addition and subtraction, making it computationally simpler and faster.

3. **Suitability for High-Dimensional Data**:
   - Performs better in high-dimensional spaces, less affected by the "curse of dimensionality."

Cons:

1. **Ignores Diagonal Distance**:
   - Only considers movements along the coordinate axes, ignoring diagonal relationships.

2. **Dependence on Scale**:
   - Affected by the scale and units of features, requiring proper normalization.

3. **Less Effective for Spherical Clusters**:
   - May not capture the true geometric relationships in spherical or circular clusters as accurately as Euclidean distance.

:::

# References
