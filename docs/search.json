[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Big Data lecture slides",
    "section": "",
    "text": "This page is directly linked to the Big Data exercises.\nAdministrative stuff is always announced via ilearn.\nThis is the lecture material for the courses “Big Data” and “Big Data Processing and Analytics”. Feel free to reference for your self study. You are not allowed to copy without the prior consent of the author. Your are not allowed to use material without prior consent. Ask - it’s a form of politeness. You will most probable get permission.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nIntroduction\n\n\nProf. Dr. Tim Weber\n\n\n\n\nData Sources\n\n\nProf. Dr. Tim Weber\n\n\n\n\nData Quality\n\n\nProf. Dr. Tim Weber\n\n\n\n\nData Bias\n\n\nProf. Dr. Tim Weber\n\n\n\n\nTraditional Methods\n\n\nProf. Dr. Tim Weber\n\n\n\n\nMap Reduce\n\n\nProf. Dr. Tim Weber\n\n\n\n\nGradient Descent\n\n\nProf. Dr. Tim Weber\n\n\n\n\nMaximum Likelihood Estimation\n\n\nProf. Dr. Tim Weber\n\n\n\n\nPrincipal Component Analysis\n\n\nProf. Dr. Tim Weber\n\n\n\n\nk - Nearest Neighbours\n\n\nProf. Dr. Tim Weber\n\n\n\n\nk- Means\n\n\nProf. Dr. Tim Weber\n\n\n\n\n(P)artitioning (A)round (M)edoids\n\n\nProf. Dr. Tim Weber\n\n\n\n\nDensity Based Spatial Clustering and Application with Noise (DBSCAN)\n\n\nProf. Dr. Tim Weber\n\n\n\n\nAgglomerative Clustering\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chapters/chapter554_dbscan/index.html#use-cases",
    "href": "chapters/chapter554_dbscan/index.html#use-cases",
    "title": "Density Based Spatial Clustering and Application with Noise (DBSCAN)",
    "section": "Use Cases",
    "text": "Use Cases\n\n\nClassical cluster algorithms work best for:\n\nspherical\nconvex\ncompact and well separated data\n\n\n\n\nDBSCAN groups data points based on density, making it effective for identifying clusters of various shapes and sizes.\n\nCore Points: Points with at least a minimum number of neighbors (MinPts) within a specified radius (ε).\nBorder Points: Points within the ε radius of core points but with fewer neighbors.\nNoise Points: Outliers that don’t belong to any cluster.\n\nClusters are formed by connecting core points and their reachable neighbors, separating high-density regions from low-density areas.\n\n\n\n\n(Ester et al. 1996)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Density Based Spatial Clustering and Application with Noise (DBSCAN)"
    ]
  },
  {
    "objectID": "chapters/chapter552_kMeans/index.html#use-cases",
    "href": "chapters/chapter552_kMeans/index.html#use-cases",
    "title": "k- Means",
    "section": "Use Cases",
    "text": "Use Cases\n\n\n\n\nMarketing: Segment customers.\nCompression: Reduce image colors.\nOrganization: Cluster documents.\nSecurity: Detect anomalies.\nTargeting: Segment markets.\nNetworking: Identify communities.\n\n\n\n\n\n\n\nBiology: Group genes/proteins.\nRecommendations: Enhance systems.\nGeography: Cluster regions.\nLogistics: Optimize inventory.\nHealthcare: Cluster patients.\nIoT: Analyze sensor data.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "k- Means"
    ]
  },
  {
    "objectID": "chapters/chapter540_PCA/index.html#principal-component-analysis-pca",
    "href": "chapters/chapter540_PCA/index.html#principal-component-analysis-pca",
    "title": "Principal Component Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)",
    "crumbs": [
      "Data crunching",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "chapters/chapter520_GradientDescent/index.html#gradient-descent",
    "href": "chapters/chapter520_GradientDescent/index.html#gradient-descent",
    "title": "Gradient Descent",
    "section": "Gradient Descent",
    "text": "Gradient Descent",
    "crumbs": [
      "Data crunching",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "chapters/chapter500_TraditionalMeasures/index.html#central-tendency",
    "href": "chapters/chapter500_TraditionalMeasures/index.html#central-tendency",
    "title": "Traditional Methods",
    "section": "Central Tendency",
    "text": "Central Tendency",
    "crumbs": [
      "Data crunching",
      "Traditional Methods"
    ]
  },
  {
    "objectID": "chapters/chapter500_TraditionalMeasures/index.html#measures-of-spread",
    "href": "chapters/chapter500_TraditionalMeasures/index.html#measures-of-spread",
    "title": "Traditional Methods",
    "section": "Measures of Spread",
    "text": "Measures of Spread",
    "crumbs": [
      "Data crunching",
      "Traditional Methods"
    ]
  },
  {
    "objectID": "chapters/chapter500_TraditionalMeasures/index.html#measures-for-categoricals",
    "href": "chapters/chapter500_TraditionalMeasures/index.html#measures-for-categoricals",
    "title": "Traditional Methods",
    "section": "Measures for Categoricals",
    "text": "Measures for Categoricals",
    "crumbs": [
      "Data crunching",
      "Traditional Methods"
    ]
  },
  {
    "objectID": "chapters/chapter500_TraditionalMeasures/index.html#beware-of-summarized-data",
    "href": "chapters/chapter500_TraditionalMeasures/index.html#beware-of-summarized-data",
    "title": "Traditional Methods",
    "section": "Beware of summarized Data",
    "text": "Beware of summarized Data\n\n\n\n\n\n\n\n\n\n\ndataset\nmean_x\nmean_y\nstd_dev_x\nstd_dev_y\ncorr_x_y\n\n\n\n\naway\n54.266\n47.835\n16.770\n26.940\n−0.064\n\n\nbullseye\n54.269\n47.831\n16.769\n26.936\n−0.069\n\n\ncircle\n54.267\n47.838\n16.760\n26.930\n−0.068\n\n\ndino\n54.263\n47.832\n16.765\n26.935\n−0.064\n\n\ndots\n54.260\n47.840\n16.768\n26.930\n−0.060\n\n\nh_lines\n54.261\n47.830\n16.766\n26.940\n−0.062\n\n\nhigh_lines\n54.269\n47.835\n16.767\n26.940\n−0.069\n\n\nslant_down\n54.268\n47.836\n16.767\n26.936\n−0.069\n\n\nslant_up\n54.266\n47.831\n16.769\n26.939\n−0.069\n\n\nstar\n54.267\n47.840\n16.769\n26.930\n−0.063\n\n\nv_lines\n54.270\n47.837\n16.770\n26.938\n−0.069\n\n\nwide_lines\n54.267\n47.832\n16.770\n26.938\n−0.067\n\n\nx_shape\n54.260\n47.840\n16.770\n26.930\n−0.066",
    "crumbs": [
      "Data crunching",
      "Traditional Methods"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#quality",
    "href": "chapters/chapter300/index.html#quality",
    "title": "Data Quality",
    "section": "Quality",
    "text": "Quality\n\nQuality (dictionary)\n\nHow good or bad something is. Cambridge Dictionary\n\n\n“… Quality is a perceptual, conditional, and somewhat subjective attribute and may be understood differently by different people. …” Wiki",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#the-cathedral-and-the-bazaar",
    "href": "chapters/chapter300/index.html#the-cathedral-and-the-bazaar",
    "title": "Data Quality",
    "section": "The Cathedral and the Bazaar",
    "text": "The Cathedral and the Bazaar\n\n\n(Raymond 2001)",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#iso-8000",
    "href": "chapters/chapter300/index.html#iso-8000",
    "title": "Data Quality",
    "section": "ISO 8000",
    "text": "ISO 8000\n\nISO\n\nInternational Standards Organization\n\n\n\n\nset of standards for data quality for the exchange between organizations and systems\nMaster data in the form of characteristic data that are\n\nexchanged between organizations and systems, and that\nconform to the data specifications that\ncan be validated by computer software.\n\n\n\n\n(ISO 2022)",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#data-quality-1",
    "href": "chapters/chapter300/index.html#data-quality-1",
    "title": "Data Quality",
    "section": "Data Quality",
    "text": "Data Quality\n“… Data are of high quality if they are “Fit for Use” in their intended operational, decision-making and other roles. … ”\n\n\n(Herzog, Scheuren, and Winkler May )",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#data-quality-issues",
    "href": "chapters/chapter300/index.html#data-quality-issues",
    "title": "Data Quality",
    "section": "Data Quality issues",
    "text": "Data Quality issues\n\n\nData Entry for the London Summer Olympics\n\nan employee simple typed \\(20,000\\) seats available, while there were only \\(10,000\\)\nbuyers were able to exchange the already bought tickets\n\nThe London Whale\n\nusing a risk model that involved copy and pasting different multiple spreadsheats\nnot only were \\(\\$2bn\\) losses wrongly calculated, they were actually counted as gains summing up to a loss of \\(\\$6.2bn\\)",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#data-quality-tools",
    "href": "chapters/chapter300/index.html#data-quality-tools",
    "title": "Data Quality",
    "section": "Data Quality Tools",
    "text": "Data Quality Tools",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#data-quality-summary",
    "href": "chapters/chapter300/index.html#data-quality-summary",
    "title": "Data Quality",
    "section": "Data Quality summary",
    "text": "Data Quality summary\n\n\nData Quality is hard\nTop Down or Bottom up?\nsimple things go wrong in the real world\ntest stuff\nbe specific\nthere is no recipe\ngarbage in, garbage out\nclassic ways are not fancy, but useful. Use this to your advantage\nnever trust data\ninvest early, profit soon",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#the-end-of-theory",
    "href": "chapters/chapter100/index.html#the-end-of-theory",
    "title": "Introduction",
    "section": "The End of Theory?",
    "text": "The End of Theory?\n\n\n(Nast 2024) J. Craig Venter Institute",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#information-growth",
    "href": "chapters/chapter100/index.html#information-growth",
    "title": "Introduction",
    "section": "Information growth",
    "text": "Information growth\n\n\n(Luraschi, Kuo, and Ruiz 2019)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#drinking-leads-to-amazon-purchases",
    "href": "chapters/chapter100/index.html#drinking-leads-to-amazon-purchases",
    "title": "Introduction",
    "section": "Drinking leads to Amazon purchases",
    "text": "Drinking leads to Amazon purchases\n\n\nTyler Vigen",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#central-limit-theorem-clt",
    "href": "chapters/chapter100/index.html#central-limit-theorem-clt",
    "title": "Introduction",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#law-of-large-numbers",
    "href": "chapters/chapter100/index.html#law-of-large-numbers",
    "title": "Introduction",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#history-of-big-data",
    "href": "chapters/chapter100/index.html#history-of-big-data",
    "title": "Introduction",
    "section": "History of Big Data",
    "text": "History of Big Data",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#evolution-of-big-data",
    "href": "chapters/chapter100/index.html#evolution-of-big-data",
    "title": "Introduction",
    "section": "Evolution of Big Data",
    "text": "Evolution of Big Data",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#big-data-ingredients",
    "href": "chapters/chapter100/index.html#big-data-ingredients",
    "title": "Introduction",
    "section": "Big Data ingredients",
    "text": "Big Data ingredients",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#production-environment",
    "href": "chapters/chapter100/index.html#production-environment",
    "title": "Introduction",
    "section": "Production Environment",
    "text": "Production Environment",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#six-sigma",
    "href": "chapters/chapter100/index.html#six-sigma",
    "title": "Introduction",
    "section": "Six Sigma",
    "text": "Six Sigma",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#crisp-dm",
    "href": "chapters/chapter100/index.html#crisp-dm",
    "title": "Introduction",
    "section": "CRISP-DM",
    "text": "CRISP-DM",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#unifying-approach",
    "href": "chapters/chapter100/index.html#unifying-approach",
    "title": "Introduction",
    "section": "unifying approach",
    "text": "unifying approach\n\n\n(Schäfer et al. 2019)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter200/index.html#which-do-you-want",
    "href": "chapters/chapter200/index.html#which-do-you-want",
    "title": "Data Sources",
    "section": "Which do you want?",
    "text": "Which do you want?",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "chapters/chapter200/index.html#databases",
    "href": "chapters/chapter200/index.html#databases",
    "title": "Data Sources",
    "section": "Databases",
    "text": "Databases",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "chapters/chapter400/index.html#why-data-bias-matters",
    "href": "chapters/chapter400/index.html#why-data-bias-matters",
    "title": "Data Bias",
    "section": "Why Data Bias matters",
    "text": "Why Data Bias matters",
    "crumbs": [
      "Data Bias",
      "slides"
    ]
  },
  {
    "objectID": "chapters/chapter400/index.html#types-of-data-bias",
    "href": "chapters/chapter400/index.html#types-of-data-bias",
    "title": "Data Bias",
    "section": "Types of Data Bias",
    "text": "Types of Data Bias",
    "crumbs": [
      "Data Bias",
      "slides"
    ]
  },
  {
    "objectID": "chapters/chapter400/index.html#bias-and-variance",
    "href": "chapters/chapter400/index.html#bias-and-variance",
    "title": "Data Bias",
    "section": "Bias and Variance",
    "text": "Bias and Variance\n\nBias:\n\nerror introduced by approximating a real-world problem with a simplified model\nleads to underfitting\n\n\n\n\nVariance:\n\nsensitivity to fluctuations in the data, such as random noise, as if it were true patterns\nleads to overfitting",
    "crumbs": [
      "Data Bias",
      "slides"
    ]
  },
  {
    "objectID": "chapters/chapter400/index.html#the-bias-variance-trade-off",
    "href": "chapters/chapter400/index.html#the-bias-variance-trade-off",
    "title": "Data Bias",
    "section": "The Bias Variance Trade Off",
    "text": "The Bias Variance Trade Off\n\n\n\n\n\nfamiliar?",
    "crumbs": [
      "Data Bias",
      "slides"
    ]
  },
  {
    "objectID": "chapters/chapter510_MapReduce/index.html#mapreduce",
    "href": "chapters/chapter510_MapReduce/index.html#mapreduce",
    "title": "Map Reduce",
    "section": "MapReduce",
    "text": "MapReduce",
    "crumbs": [
      "Data crunching",
      "Map Reduce"
    ]
  },
  {
    "objectID": "chapters/chapter510_MapReduce/index.html#mapreduce-summary",
    "href": "chapters/chapter510_MapReduce/index.html#mapreduce-summary",
    "title": "Map Reduce",
    "section": "MapReduce summary",
    "text": "MapReduce summary\n\n\n\nMapReduce is significant for its role in enabling the processing of massive datasets efficiently across distributed computing clusters.\nIt revolutionized big data processing by providing a scalable and fault-tolerant framework for handling large-scale computations.\nIts simplicity and scalability made it accessible to a wide range of industries and applications, from web search engines to scientific research.\nMapReduce paved the way for the development of other big data processing frameworks and technologies, influencing the evolution of distributed computing paradigms.\nIts impact extends beyond its original implementation, as concepts and principles from MapReduce have influenced the design of subsequent systems and architectures for big data processing.",
    "crumbs": [
      "Data crunching",
      "Map Reduce"
    ]
  },
  {
    "objectID": "chapters/chapter530_MLE/index.html#maximum-likelihood-estimation-mle",
    "href": "chapters/chapter530_MLE/index.html#maximum-likelihood-estimation-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "Maximum Likelihood Estimation (MLE)",
    "text": "Maximum Likelihood Estimation (MLE)\n\nMaximum Likelihood Estimation (MLE) is a statistical method used for estimating the parameters of a probability distribution by maximizing a likelihood function. This method finds the parameter values that make the observed data most probable.\n\n\n\nConsistency: As the sample size increases, the MLE converges to the true parameter value.\nAsymptotic Normality: The distribution of the MLE approaches a normal distribution as the sample size grows.\nEfficiency: Among unbiased estimators, the MLE has the lowest possible variance asymptotically.",
    "crumbs": [
      "Data crunching",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "chapters/chapter551_kNN/index.html#uses",
    "href": "chapters/chapter551_kNN/index.html#uses",
    "title": "k - Nearest Neighbours",
    "section": "Uses",
    "text": "Uses\n\n\n\n\nClassification:\n\nMedical diagnosis\nImage recognition\nDocument classification\n\nRegression:\n\nStock price prediction\nWeather forecasting\n\nRecommendation Systems:\n\nProduct recommendations\nMovie recommendations\n\n\n\n\n\n\n\n\nAnomaly Detection:\n\nFraud detection\nNetwork security\n\nPattern Recognition:\n\nSpeech recognition\nHandwriting recognition\n\nData Imputation:\n\nFilling missing values",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "k - Nearest Neighbours"
    ]
  },
  {
    "objectID": "chapters/chapter551_kNN/index.html#the-idea",
    "href": "chapters/chapter551_kNN/index.html#the-idea",
    "title": "k - Nearest Neighbours",
    "section": "The idea",
    "text": "The idea",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "k - Nearest Neighbours"
    ]
  },
  {
    "objectID": "chapters/chapter551_kNN/index.html#knn-summary",
    "href": "chapters/chapter551_kNN/index.html#knn-summary",
    "title": "k - Nearest Neighbours",
    "section": "knn summary",
    "text": "knn summary\n\n\nPros:\n\n\nNon-parametric: Does not assume a fixed form for the mapping function, allowing for flexibility in modeling complex relationships.\nAdaptable: Can easily adapt to new data by simply storing additional instances, making it suitable for dynamic environments.\nInterpretable: Easy to interpret results as the output is based on the majority class among neighbors or average of nearest points.\n\n\n\n\n\nCons:\n\n\nMemory Intensive: Requires storing the entire training dataset, which can be problematic for very large datasets.\nFeature Scaling: Performance depends heavily on the scale of the features, often necessitating normalization or standardization.\nImbalanced Data: Struggles with imbalanced datasets where the minority class may be overshadowed by the majority class due to the simple majority voting mechanism.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "k - Nearest Neighbours"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#robustness-to-noise-and-outliers",
    "href": "chapters/chapter553_pam/index.html#robustness-to-noise-and-outliers",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Robustness to Noise and Outliers",
    "text": "Robustness to Noise and Outliers\n\nPAM: Uses medoids (actual data points) as cluster centers, which makes it more robust to noise and outliers. The medoid is less influenced by outliers compared to the centroid used in K-means.\nK-means: Uses centroids (mean of the points in a cluster) as cluster centers, which can be significantly influenced by outliers and noise.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#interpretability",
    "href": "chapters/chapter553_pam/index.html#interpretability",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Interpretability",
    "text": "Interpretability\n\nPAM: Since medoids are actual data points from the dataset, the cluster centers are more interpretable and meaningful in real-world scenarios.\nK-means: Centroids are not actual data points but rather the mean of the cluster members, which might be less intuitive to interpret.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#suitable-for-arbitrary-distance-metrics",
    "href": "chapters/chapter553_pam/index.html#suitable-for-arbitrary-distance-metrics",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Suitable for Arbitrary Distance Metrics",
    "text": "Suitable for Arbitrary Distance Metrics\n\nPAM: Can use any distance metric, making it more flexible for different types of data (e.g., categorical, ordinal, or non-Euclidean).\nK-means: Typically uses Euclidean distance, which may not be suitable for all types of data or similarity measures.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#cluster-shape",
    "href": "chapters/chapter553_pam/index.html#cluster-shape",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Cluster Shape",
    "text": "Cluster Shape\n\nPAM: Can handle clusters of various shapes and sizes better than K-means because it doesn’t assume spherical clusters centered around a mean.\nK-means: Assumes clusters are spherical and equally sized, which might not always be the case.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#convergence-to-a-global-optimum",
    "href": "chapters/chapter553_pam/index.html#convergence-to-a-global-optimum",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Convergence to a Global Optimum",
    "text": "Convergence to a Global Optimum\n\nPAM: Uses a more exhaustive search (swap between medoids and non-medoids) to find the optimal set of medoids, which can lead to better overall clustering results.\nK-means: Relies on iterative refinement, which can get stuck in local optima and is sensitive to the initial placement of centroids.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#disadvantages-to-k-means",
    "href": "chapters/chapter553_pam/index.html#disadvantages-to-k-means",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Disadvantages to k-means",
    "text": "Disadvantages to k-means\n\nScalability:\n\nPAM: Computationally more expensive and less scalable to large datasets.\nK-means: Computationally efficient and more suitable for large datasets.\n\nImplementation Complexity:\n\nPAM: More complex to implement and requires more computation time.\nK-means: Simpler and faster to implement.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#the-idea",
    "href": "chapters/chapter553_pam/index.html#the-idea",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "the idea",
    "text": "the idea",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#doing-it",
    "href": "chapters/chapter553_pam/index.html#doing-it",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "doing it",
    "text": "doing it\n\ndata(\"USArrests\")\n\\(50\\) observations\n\\(4\\) variables:\n\nMurder: number of murder arrests per \\(100,000\\)\nAssault: number of assault arrests per \\(100,000\\)\nUrbanPop: percent urban population\nRape: number of rape arrests per \\(100,000\\)\n\n\n\n(Kassambara 2017)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#histograms",
    "href": "chapters/chapter553_pam/index.html#histograms",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "histograms",
    "text": "histograms",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#density",
    "href": "chapters/chapter553_pam/index.html#density",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "density",
    "text": "density",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#multivariate",
    "href": "chapters/chapter553_pam/index.html#multivariate",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "multivariate",
    "text": "multivariate",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#optimal-number-of-clusters",
    "href": "chapters/chapter553_pam/index.html#optimal-number-of-clusters",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "optimal number of clusters",
    "text": "optimal number of clusters",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#average-silhouette-method",
    "href": "chapters/chapter553_pam/index.html#average-silhouette-method",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "average silhouette method",
    "text": "average silhouette method\n\ncompute the average distance (\\(a(i)\\)) of each point within its cluster (within cluster distance)\ncompute the distance of each point to all points in the nearest cluster (next closest cluster)\n\nSilhouette Score:\n\\[\\begin{align}\ns(i) = \\frac{b(i)-a(i)}{max(a(i),(b(i)))}\n\\end{align}\\]",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#average-silhouette-method-example-on-toy-data",
    "href": "chapters/chapter553_pam/index.html#average-silhouette-method-example-on-toy-data",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "average silhouette method: example on toy data",
    "text": "average silhouette method: example on toy data",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#modeling-k-2",
    "href": "chapters/chapter553_pam/index.html#modeling-k-2",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "modeling (k = 2)",
    "text": "modeling (k = 2)\n\nclust_mdl &lt;- pam(cluster_data,2)\n\npoint_assigned &lt;- augment(clust_mdl,cluster_data)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#silhouette-plot-n-2",
    "href": "chapters/chapter553_pam/index.html#silhouette-plot-n-2",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "silhouette plot (n = 2)",
    "text": "silhouette plot (n = 2)\n\n\n  cluster size ave.sil.width\n1       1    7          0.33\n2       2    7          0.35",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#silhouette-plot-n-2345",
    "href": "chapters/chapter553_pam/index.html#silhouette-plot-n-2345",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "silhouette plot (n = 2,3,4,5)",
    "text": "silhouette plot (n = 2,3,4,5)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#cluster-plot-on-toy-data",
    "href": "chapters/chapter553_pam/index.html#cluster-plot-on-toy-data",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "cluster plot on toy data",
    "text": "cluster plot on toy data",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#back-to-business",
    "href": "chapters/chapter553_pam/index.html#back-to-business",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "back to business",
    "text": "back to business",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#cluster-plot",
    "href": "chapters/chapter553_pam/index.html#cluster-plot",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "cluster plot",
    "text": "cluster plot",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#cluster-output",
    "href": "chapters/chapter553_pam/index.html#cluster-output",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "cluster output",
    "text": "cluster output\n\n\nMedoids:\n           ID     Murder    Assault   UrbanPop       Rape\nNew Mexico 31  0.8292944  1.3708088  0.3081225  1.1603196\nNebraska   27 -0.8008247 -0.8250772 -0.2445636 -0.5052109\nClustering vector:\n       Alabama         Alaska        Arizona       Arkansas     California \n             1              1              1              2              1 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             1              2              2              1              1 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             2              2              1              2              2 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             2              2              1              2              1 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             2              1              2              1              1 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             2              2              1              2              2 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             1              1              1              2              2 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             2              2              2              2              1 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             2              1              1              2              2 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             2              2              2              2              2 \nObjective function:\n   build     swap \n1.441358 1.368969 \n\nAvailable components:\n [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#agglomerative-clustering",
    "href": "chapters/chapter555_agglomerative/index.html#agglomerative-clustering",
    "title": "Agglomerative Clustering",
    "section": "Agglomerative Clustering",
    "text": "Agglomerative Clustering\n\nEach observation is treated as a single cluster in the beginning (a leaf)\nthe most similar clusters are successively merged until there is only one single big cluster (root)\nresult is a tree-like representation of the objects, known as dendogram",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#the-idea",
    "href": "chapters/chapter555_agglomerative/index.html#the-idea",
    "title": "Agglomerative Clustering",
    "section": "The idea",
    "text": "The idea",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#whats-a-dendogram",
    "href": "chapters/chapter555_agglomerative/index.html#whats-a-dendogram",
    "title": "Agglomerative Clustering",
    "section": "What’s a dendogram",
    "text": "What’s a dendogram",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#what-is-similar",
    "href": "chapters/chapter555_agglomerative/index.html#what-is-similar",
    "title": "Agglomerative Clustering",
    "section": "What is similar?",
    "text": "What is similar?\n\n\n… the classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix. (Kassambara 2017)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#distance-measures",
    "href": "chapters/chapter555_agglomerative/index.html#distance-measures",
    "title": "Agglomerative Clustering",
    "section": "distance measures",
    "text": "distance measures\n\n\nClassical\n\nEuclidean\nManhatten\n\nCorrelation based\n\nPearson correlation\nEisen cosine correlation distance\nSpearman correlation distance\nKendall correlation distance",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  }
]