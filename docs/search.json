[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Big Data lecture slides",
    "section": "",
    "text": "This page is directly linked to the Big Data exercises.\nAdministrative stuff is always announced via ilearn.\nThis is the lecture material for the courses “Big Data” and “Big Data Processing and Analytics”. Feel free to reference for your self study. You are not allowed to copy without the prior consent of the author. Your are not allowed to use material without prior consent. Ask - it’s a form of politeness. You will most probable get permission.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nIntroduction\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nData Sources\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nData Quality\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nData Bias\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nTraditional Methods\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nMap Reduce\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nFrequent Itemset Mining - FIM\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nGradient Descent\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nPrincipal Component Analysis\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nk - Nearest Neighbours\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nk- Means\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\n(P)artitioning (A)round (M)edoids\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nDensity Based Spatial Clustering and Application with Noise (DBSCAN)\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nAgglomerative Clustering\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nChecking Cluster Validity\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nModel Based Clustering\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nIntroduction\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nData Sources\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nData Quality\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nData Bias\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nTraditional Methods\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nMap Reduce\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nFrequent Itemset Mining - FIM\n\n\nProf. Dr. Tim Weber\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/chapters/chapter510_MapReduce/index.html#mapreduce",
    "href": "docs/chapters/chapter510_MapReduce/index.html#mapreduce",
    "title": "Map Reduce",
    "section": "MapReduce",
    "text": "MapReduce"
  },
  {
    "objectID": "docs/chapters/chapter510_MapReduce/index.html#mapreduce-summary",
    "href": "docs/chapters/chapter510_MapReduce/index.html#mapreduce-summary",
    "title": "Map Reduce",
    "section": "MapReduce summary",
    "text": "MapReduce summary\n\n\nMapReduce is significant for its role in enabling the processing of massive datasets efficiently across distributed computing clusters.\nIt revolutionized big data processing by providing a scalable and fault-tolerant framework for handling large-scale computations.\nIts simplicity and scalability made it accessible to a wide range of industries and applications, from web search engines to scientific research.\nMapReduce paved the way for the development of other big data processing frameworks and technologies, influencing the evolution of distributed computing paradigms.\nIts impact extends beyond its original implementation, as concepts and principles from MapReduce have influenced the design of subsequent systems and architectures for big data processing."
  },
  {
    "objectID": "docs/chapters/chapter400/index.html#why-data-bias-matters",
    "href": "docs/chapters/chapter400/index.html#why-data-bias-matters",
    "title": "Data Bias",
    "section": "Why Data Bias matters",
    "text": "Why Data Bias matters"
  },
  {
    "objectID": "docs/chapters/chapter400/index.html#types-of-data-bias",
    "href": "docs/chapters/chapter400/index.html#types-of-data-bias",
    "title": "Data Bias",
    "section": "Types of Data Bias",
    "text": "Types of Data Bias"
  },
  {
    "objectID": "docs/chapters/chapter400/index.html#bias-and-variance",
    "href": "docs/chapters/chapter400/index.html#bias-and-variance",
    "title": "Data Bias",
    "section": "Bias and Variance",
    "text": "Bias and Variance\n\nBias:\n\nerror introduced by approximating a real-world problem with a simplified model\nleads to underfitting\n\n\n\n\nVariance:\n\nsensitivity to fluctuations in the data, such as random noise, as if it were true patterns\nleads to overfitting"
  },
  {
    "objectID": "docs/chapters/chapter400/index.html#the-bias-variance-trade-off",
    "href": "docs/chapters/chapter400/index.html#the-bias-variance-trade-off",
    "title": "Data Bias",
    "section": "The Bias Variance Trade Off",
    "text": "The Bias Variance Trade Off\n\n\n\n\n\nfamiliar?"
  },
  {
    "objectID": "docs/chapters/chapter200/index.html#which-do-you-want",
    "href": "docs/chapters/chapter200/index.html#which-do-you-want",
    "title": "Data Sources",
    "section": "Which do you want?",
    "text": "Which do you want?"
  },
  {
    "objectID": "docs/chapters/chapter200/index.html#databases",
    "href": "docs/chapters/chapter200/index.html#databases",
    "title": "Data Sources",
    "section": "Databases",
    "text": "Databases"
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#model-based-clustering",
    "href": "chapters/chapter557_mbc/index.html#model-based-clustering",
    "title": "Model Based Clustering",
    "section": "model based clustering",
    "text": "model based clustering\n\ntraditional clustering methods …\n\n\n\n… are not based on formal models\n\n\n\n\n… require the user to specify the number of clusters\n\n\n\nmodel based clustering …\n\n\n\n… assumes data coming from a mixture of two or more clusters\n\n\n\n\n… uses a soft assignment (each point has a certain probability of belonging to a cluster)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#concept-of-model-based-clustering",
    "href": "chapters/chapter557_mbc/index.html#concept-of-model-based-clustering",
    "title": "Model Based Clustering",
    "section": "Concept of model based clustering",
    "text": "Concept of model based clustering\nEach component (cluster) \\(k\\) is modeled by the normal distribution (Kassambara 2017)\n\n\n\\(\\mu\\)\n\nmean vector\n\n\n\n\n\n\\(\\sum_k\\)\n\ncovariance matrix\n\n\n\n\n… an associated probability in the mixture. Each point has a probability of belonging to each cluster",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#introductory-example",
    "href": "chapters/chapter557_mbc/index.html#introductory-example",
    "title": "Model Based Clustering",
    "section": "introductory example",
    "text": "introductory example\n\n\n\n\n3 cluster?\n3 ellipses similar in terms of volume, shape and orientation\n\nhomogenous covariance matrix?",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#estimating-model-parameters",
    "href": "chapters/chapter557_mbc/index.html#estimating-model-parameters",
    "title": "Model Based Clustering",
    "section": "Estimating model parameters",
    "text": "Estimating model parameters\n\n(E)xpectation-(M)aximization as initialized by hierarchical clustering\ngeometric features (shape, volume, orientation) are determined by the covariance matrix\ndifferent parametrizations of \\(\\sum_k\\)\navailable model options: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV, VVV",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#available-models",
    "href": "chapters/chapter557_mbc/index.html#available-models",
    "title": "Model Based Clustering",
    "section": "available models:",
    "text": "available models:\n\n1st identifier refers to volume, 2nd to shape, 3rd to orientation\nE for equal, V for variable, I for coordinate axes\n\nEVI denotes a model with Equal volume, Variable shape and the orientation is the Identity\nEEE means that the clusters have Equal volume, Equal shape and Equal orientation",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#choosing-the-best-model",
    "href": "chapters/chapter557_mbc/index.html#choosing-the-best-model",
    "title": "Model Based Clustering",
    "section": "choosing the best model",
    "text": "choosing the best model\n\nuse mle to fit all models for a range of \\(k\\) components\nmodel selection based on the (B)ayesian (I)nformation (C)riterion (BIC)\na greater BIC score is considered better in mbc\n\n\n\n\n\n\n\n\nImportant\n\n\nThe BIC shall only be used to compare model within one method (as in model based cluster vs. model based cluster) not across different modeling methods (as in linear vs. logisitc regression)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#bayersion-information-criterion",
    "href": "chapters/chapter557_mbc/index.html#bayersion-information-criterion",
    "title": "Model Based Clustering",
    "section": "Bayersion Information Criterion",
    "text": "Bayersion Information Criterion\n\\[\\begin{align}\nBIC = k \\ln(n)-2ln(\\hat{L})\n\\end{align}\\]\n\n\n\\(\\hat{L}\\)\n\nthe maximized value of the likelihood function of the model\n\n\\(n\\)\n\nnumber of data points\n\n\\(k\\)\n\nthe number of parameters estimated by the model\n\n\nA model could perform better by overfitting. The BIC introduces a penalty to the model parameters. Compare \\(r^2\\) and \\(r^2_{adjusted}\\)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#data-for-clustering",
    "href": "chapters/chapter557_mbc/index.html#data-for-clustering",
    "title": "Model Based Clustering",
    "section": "data for clustering",
    "text": "data for clustering\n\ndata(\"diabetes\")\n\nhead(diabetes)\n\n   class glucose insulin sspg\n1 Normal      80     356  124\n2 Normal      97     289  117\n3 Normal     105     319  143\n4 Normal      90     356  199\n5 Normal      90     323  240\n6 Normal      86     381  157\n\n\n\nclass: diagnosis: normal, chemically diabetic and overly diabetic. Will be excluded\nglucose: plasma glucose response to oral glucose\ninsulin: plasma insulin respnse to oral glucose\nsspg: steady-state plasma glucose (measures insuline resistance)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#model-output",
    "href": "chapters/chapter557_mbc/index.html#model-output",
    "title": "Model Based Clustering",
    "section": "model output",
    "text": "model output\n\ndf &lt;- scale(diabetes[,-1])\nmc &lt;- Mclust(df)\n\nsummary(mc)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVV (ellipsoidal, varying volume, shape, and orientation) model with 3\ncomponents: \n\n log-likelihood   n df       BIC       ICL\n      -169.0908 145 29 -482.5069 -501.4662\n\nClustering table:\n 1  2  3 \n81 36 28",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#detailed-model-output",
    "href": "chapters/chapter557_mbc/index.html#detailed-model-output",
    "title": "Model Based Clustering",
    "section": "detailed model output",
    "text": "detailed model output\n\nmc$modelName\n\n[1] \"VVV\"\n\nmc$G\n\n[1] 3",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#visualize-cluster-output",
    "href": "chapters/chapter557_mbc/index.html#visualize-cluster-output",
    "title": "Model Based Clustering",
    "section": "visualize cluster output",
    "text": "visualize cluster output",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#geysers",
    "href": "chapters/chapter557_mbc/index.html#geysers",
    "title": "Model Based Clustering",
    "section": "geysers?",
    "text": "geysers?",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#univariate-clustering",
    "href": "chapters/chapter557_mbc/index.html#univariate-clustering",
    "title": "Model Based Clustering",
    "section": "univariate clustering",
    "text": "univariate clustering",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#clustering",
    "href": "chapters/chapter557_mbc/index.html#clustering",
    "title": "Model Based Clustering",
    "section": "clustering",
    "text": "clustering\n\nuv_clust &lt;- densityMclust(acidity)\n\n\n\n\n\n\n\nuv_clust &lt;- densityMclust(acidity, modelName = \"V\" )\n\n\n\n\n\n\n\nsummary(uv_clust)\n\n------------------------------------------------------- \nDensity estimation via Gaussian finite mixture modeling \n------------------------------------------------------- \n\nMclust V (univariate, unequal variance) model with 3 components: \n\n log-likelihood   n df       BIC       ICL\n      -178.7817 155  8 -397.9108 -458.8648\n\ntmp &lt;- acidity |&gt; \n  add_column(\n    classification = uv_clust$classification\n  ) |&gt; \n  add_column(\n    uncertainty = uv_clust$uncertainty\n  )\n\ntmp |&gt; \n  ggplot(\n    aes(\n      x = classification,\n      y = acidity,\n      size = uncertainty\n    )\n  )+\n  geom_jitter()+\n  geom_text(\n    aes(\n      label = uncertainty |&gt; round(digits = 2)\n    ),\n    size = 9\n  )",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#modeling-visualization",
    "href": "chapters/chapter557_mbc/index.html#modeling-visualization",
    "title": "Model Based Clustering",
    "section": "modeling visualization",
    "text": "modeling visualization",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#cluster-visualization",
    "href": "chapters/chapter557_mbc/index.html#cluster-visualization",
    "title": "Model Based Clustering",
    "section": "cluster visualization",
    "text": "cluster visualization",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter557_mbc/index.html#more-viz",
    "href": "chapters/chapter557_mbc/index.html#more-viz",
    "title": "Model Based Clustering",
    "section": "more viz",
    "text": "more viz",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Model Based Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#agglomerative-clustering",
    "href": "chapters/chapter555_agglomerative/index.html#agglomerative-clustering",
    "title": "Agglomerative Clustering",
    "section": "Agglomerative Clustering",
    "text": "Agglomerative Clustering\n\nEach observation is treated as a single cluster in the beginning (a leaf)\nthe most similar clusters are successively merged until there is only one single big cluster (root)\nresult is a tree-like representation of the objects, known as dendogram",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#the-idea",
    "href": "chapters/chapter555_agglomerative/index.html#the-idea",
    "title": "Agglomerative Clustering",
    "section": "The idea",
    "text": "The idea",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#whats-a-dendogram",
    "href": "chapters/chapter555_agglomerative/index.html#whats-a-dendogram",
    "title": "Agglomerative Clustering",
    "section": "What’s a dendogram",
    "text": "What’s a dendogram",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#what-is-similar",
    "href": "chapters/chapter555_agglomerative/index.html#what-is-similar",
    "title": "Agglomerative Clustering",
    "section": "What is similar?",
    "text": "What is similar?\n\n\n… the classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix. (Kassambara 2017)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#distance-measures",
    "href": "chapters/chapter555_agglomerative/index.html#distance-measures",
    "title": "Agglomerative Clustering",
    "section": "distance measures",
    "text": "distance measures\n\nClassical\n\nEuclidean\nManhatten\n\nCorrelation based\n\nPearson correlation\nEisen cosine correlation distance\nSpearman correlation distance\nKendall correlation distance",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#dendogram",
    "href": "chapters/chapter555_agglomerative/index.html#dendogram",
    "title": "Agglomerative Clustering",
    "section": "dendogram",
    "text": "dendogram",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#verify-the-tree",
    "href": "chapters/chapter555_agglomerative/index.html#verify-the-tree",
    "title": "Agglomerative Clustering",
    "section": "verify the tree",
    "text": "verify the tree\n\nres.coph &lt;- cophenetic(res.hc)\n\ncor(res.dist,res.coph)\n\n[1] 0.6975266",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#so-how-many-groups",
    "href": "chapters/chapter555_agglomerative/index.html#so-how-many-groups",
    "title": "Agglomerative Clustering",
    "section": "so… how many groups?",
    "text": "so… how many groups?\n… you need to find that yourself",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#comparing-dendograms---visual",
    "href": "chapters/chapter555_agglomerative/index.html#comparing-dendograms---visual",
    "title": "Agglomerative Clustering",
    "section": "Comparing Dendograms - visual",
    "text": "Comparing Dendograms - visual",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#comparing-dendogram---quantitative",
    "href": "chapters/chapter555_agglomerative/index.html#comparing-dendogram---quantitative",
    "title": "Agglomerative Clustering",
    "section": "comparing dendogram - quantitative",
    "text": "comparing dendogram - quantitative",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#visualizing-dendograms",
    "href": "chapters/chapter555_agglomerative/index.html#visualizing-dendograms",
    "title": "Agglomerative Clustering",
    "section": "visualizing dendograms",
    "text": "visualizing dendograms",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#mtcars",
    "href": "chapters/chapter555_agglomerative/index.html#mtcars",
    "title": "Agglomerative Clustering",
    "section": "mtcars?",
    "text": "mtcars?\n\ndata(\"mtcars\") # get the data\n\ndf &lt;- scale(mtcars) # scale the data\n\nres.dist.spea &lt;- get_dist(df, method = \"spearman\") # compute distance metric\nres.dist.kend &lt;- get_dist(df, method = \"kendall\") # compute distance metric\n\n\nres.hc.spea &lt;- hclust(d = res.dist.spea, method = \"ward.D2\") # use the linkage function\nres.hc.kend &lt;- hclust(d = res.dist.kend, method = \"ward.D2\") # use the linkage function",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#verify-mtcars-clustering",
    "href": "chapters/chapter555_agglomerative/index.html#verify-mtcars-clustering",
    "title": "Agglomerative Clustering",
    "section": "verify mtcars clustering",
    "text": "verify mtcars clustering\n\nres.coph.spea &lt;- cophenetic(res.hc.spea)\nres.coph.kend &lt;- cophenetic(res.hc.kend)\n\ncor(res.dist.spea,res.coph.spea)\n\n[1] 0.8138052\n\ncor(res.dist.kend,res.coph.kend)\n\n[1] 0.8177303",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#groups-in-mtcars",
    "href": "chapters/chapter555_agglomerative/index.html#groups-in-mtcars",
    "title": "Agglomerative Clustering",
    "section": "groups in mtcars",
    "text": "groups in mtcars",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#comparing-dendograms---visual-1",
    "href": "chapters/chapter555_agglomerative/index.html#comparing-dendograms---visual-1",
    "title": "Agglomerative Clustering",
    "section": "Comparing Dendograms - visual",
    "text": "Comparing Dendograms - visual",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#spearman---dendogram-correlation",
    "href": "chapters/chapter555_agglomerative/index.html#spearman---dendogram-correlation",
    "title": "Agglomerative Clustering",
    "section": "spearman - dendogram correlation",
    "text": "spearman - dendogram correlation",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#kendall---dendogram-correlation",
    "href": "chapters/chapter555_agglomerative/index.html#kendall---dendogram-correlation",
    "title": "Agglomerative Clustering",
    "section": "kendall - dendogram correlation",
    "text": "kendall - dendogram correlation",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#complete-comparison",
    "href": "chapters/chapter555_agglomerative/index.html#complete-comparison",
    "title": "Agglomerative Clustering",
    "section": "complete comparison",
    "text": "complete comparison",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#spearman-ward-vs.-kendall-ward",
    "href": "chapters/chapter555_agglomerative/index.html#spearman-ward-vs.-kendall-ward",
    "title": "Agglomerative Clustering",
    "section": "Spearman Ward vs. Kendall Ward",
    "text": "Spearman Ward vs. Kendall Ward",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter555_agglomerative/index.html#final-result",
    "href": "chapters/chapter555_agglomerative/index.html#final-result",
    "title": "Agglomerative Clustering",
    "section": "final result",
    "text": "final result\n\n\n“Pluralitas non est ponenda sine neccesitate”",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Agglomerative Clustering"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#robustness-to-noise-and-outliers",
    "href": "chapters/chapter553_pam/index.html#robustness-to-noise-and-outliers",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Robustness to Noise and Outliers",
    "text": "Robustness to Noise and Outliers\n\nPAM: Uses medoids (actual data points) as cluster centers, which makes it more robust to noise and outliers. The medoid is less influenced by outliers compared to the centroid used in K-means.\nK-means: Uses centroids (mean of the points in a cluster) as cluster centers, which can be significantly influenced by outliers and noise.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#interpretability",
    "href": "chapters/chapter553_pam/index.html#interpretability",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Interpretability",
    "text": "Interpretability\n\nPAM: Since medoids are actual data points from the dataset, the cluster centers are more interpretable and meaningful in real-world scenarios.\nK-means: Centroids are not actual data points but rather the mean of the cluster members, which might be less intuitive to interpret.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#suitable-for-arbitrary-distance-metrics",
    "href": "chapters/chapter553_pam/index.html#suitable-for-arbitrary-distance-metrics",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Suitable for Arbitrary Distance Metrics",
    "text": "Suitable for Arbitrary Distance Metrics\n\nPAM: Can use any distance metric, making it more flexible for different types of data (e.g., categorical, ordinal, or non-Euclidean).\nK-means: Typically uses Euclidean distance, which may not be suitable for all types of data or similarity measures.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#cluster-shape",
    "href": "chapters/chapter553_pam/index.html#cluster-shape",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Cluster Shape",
    "text": "Cluster Shape\n\nPAM: Can handle clusters of various shapes and sizes better than K-means because it doesn’t assume spherical clusters centered around a mean.\nK-means: Assumes clusters are spherical and equally sized, which might not always be the case.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#convergence-to-a-global-optimum",
    "href": "chapters/chapter553_pam/index.html#convergence-to-a-global-optimum",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Convergence to a Global Optimum",
    "text": "Convergence to a Global Optimum\n\nPAM: Uses a more exhaustive search (swap between medoids and non-medoids) to find the optimal set of medoids, which can lead to better overall clustering results.\nK-means: Relies on iterative refinement, which can get stuck in local optima and is sensitive to the initial placement of centroids.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#disadvantages-to-k-means",
    "href": "chapters/chapter553_pam/index.html#disadvantages-to-k-means",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "Disadvantages to k-means",
    "text": "Disadvantages to k-means\n\nScalability:\n\nPAM: Computationally more expensive and less scalable to large datasets.\nK-means: Computationally efficient and more suitable for large datasets.\n\nImplementation Complexity:\n\nPAM: More complex to implement and requires more computation time.\nK-means: Simpler and faster to implement.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#the-idea",
    "href": "chapters/chapter553_pam/index.html#the-idea",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "the idea",
    "text": "the idea",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#doing-it",
    "href": "chapters/chapter553_pam/index.html#doing-it",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "doing it",
    "text": "doing it\n\ndata(\"USArrests\")\n\\(50\\) observations\n\\(4\\) variables:\n\nMurder: number of murder arrests per \\(100,000\\)\nAssault: number of assault arrests per \\(100,000\\)\nUrbanPop: percent urban population\nRape: number of rape arrests per \\(100,000\\)\n\n\n\n(Kassambara 2017)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#histograms",
    "href": "chapters/chapter553_pam/index.html#histograms",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "histograms",
    "text": "histograms",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#density",
    "href": "chapters/chapter553_pam/index.html#density",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "density",
    "text": "density",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#multivariate",
    "href": "chapters/chapter553_pam/index.html#multivariate",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "multivariate",
    "text": "multivariate",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#optimal-number-of-clusters",
    "href": "chapters/chapter553_pam/index.html#optimal-number-of-clusters",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "optimal number of clusters",
    "text": "optimal number of clusters",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#average-silhouette-method",
    "href": "chapters/chapter553_pam/index.html#average-silhouette-method",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "average silhouette method",
    "text": "average silhouette method\n\ncompute the average distance (\\(a(i)\\)) of each point within its cluster (within cluster distance)\ncompute the distance of each point to all points in the nearest cluster (next closest cluster)\n\nSilhouette Score:\n\\[\\begin{align}\ns(i) = \\frac{b(i)-a(i)}{max(a(i),(b(i)))}\n\\end{align}\\]",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#average-silhouette-method-example-on-toy-data",
    "href": "chapters/chapter553_pam/index.html#average-silhouette-method-example-on-toy-data",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "average silhouette method: example on toy data",
    "text": "average silhouette method: example on toy data",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#modeling-k-2",
    "href": "chapters/chapter553_pam/index.html#modeling-k-2",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "modeling (k = 2)",
    "text": "modeling (k = 2)\n\nclust_mdl &lt;- pam(cluster_data,2)\n\npoint_assigned &lt;- augment(clust_mdl,cluster_data)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#silhouette-plot-n-2",
    "href": "chapters/chapter553_pam/index.html#silhouette-plot-n-2",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "silhouette plot (n = 2)",
    "text": "silhouette plot (n = 2)\n\n\n  cluster size ave.sil.width\n1       1    7          0.33\n2       2    7          0.35",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#silhouette-plot-n-2345",
    "href": "chapters/chapter553_pam/index.html#silhouette-plot-n-2345",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "silhouette plot (n = 2,3,4,5)",
    "text": "silhouette plot (n = 2,3,4,5)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#cluster-plot-on-toy-data",
    "href": "chapters/chapter553_pam/index.html#cluster-plot-on-toy-data",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "cluster plot on toy data",
    "text": "cluster plot on toy data",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#back-to-business",
    "href": "chapters/chapter553_pam/index.html#back-to-business",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "back to business",
    "text": "back to business",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#cluster-plot",
    "href": "chapters/chapter553_pam/index.html#cluster-plot",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "cluster plot",
    "text": "cluster plot",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter553_pam/index.html#cluster-output",
    "href": "chapters/chapter553_pam/index.html#cluster-output",
    "title": "(P)artitioning (A)round (M)edoids",
    "section": "cluster output",
    "text": "cluster output\n\n\nMedoids:\n           ID     Murder    Assault   UrbanPop       Rape\nNew Mexico 31  0.8292944  1.3708088  0.3081225  1.1603196\nNebraska   27 -0.8008247 -0.8250772 -0.2445636 -0.5052109\nClustering vector:\n       Alabama         Alaska        Arizona       Arkansas     California \n             1              1              1              2              1 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             1              2              2              1              1 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             2              2              1              2              2 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             2              2              1              2              1 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             2              1              2              1              1 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             2              2              1              2              2 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             1              1              1              2              2 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             2              2              2              2              1 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             2              1              1              2              2 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             2              2              2              2              2 \nObjective function:\n   build     swap \n1.441358 1.368969 \n\nAvailable components:\n [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "(P)artitioning (A)round (M)edoids"
    ]
  },
  {
    "objectID": "chapters/chapter551_kNN/index.html#uses",
    "href": "chapters/chapter551_kNN/index.html#uses",
    "title": "k - Nearest Neighbours",
    "section": "Uses",
    "text": "Uses\n\n\n\n\nClassification:\n\nMedical diagnosis\nImage recognition\nDocument classification\n\nRegression:\n\nStock price prediction\nWeather forecasting\n\nRecommendation Systems:\n\nProduct recommendations\nMovie recommendations\n\n\n\n\n\n\n\n\nAnomaly Detection:\n\nFraud detection\nNetwork security\n\nPattern Recognition:\n\nSpeech recognition\nHandwriting recognition\n\nData Imputation:\n\nFilling missing values",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "k - Nearest Neighbours"
    ]
  },
  {
    "objectID": "chapters/chapter551_kNN/index.html#the-idea",
    "href": "chapters/chapter551_kNN/index.html#the-idea",
    "title": "k - Nearest Neighbours",
    "section": "The idea",
    "text": "The idea",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "k - Nearest Neighbours"
    ]
  },
  {
    "objectID": "chapters/chapter551_kNN/index.html#knn-summary",
    "href": "chapters/chapter551_kNN/index.html#knn-summary",
    "title": "k - Nearest Neighbours",
    "section": "knn summary",
    "text": "knn summary\n\n\nPros:\n\n\nNon-parametric: Does not assume a fixed form for the mapping function, allowing for flexibility in modeling complex relationships.\nAdaptable: Can easily adapt to new data by simply storing additional instances, making it suitable for dynamic environments.\nInterpretable: Easy to interpret results as the output is based on the majority class among neighbors or average of nearest points.\n\n\n\n\n\nCons:\n\n\nMemory Intensive: Requires storing the entire training dataset, which can be problematic for very large datasets.\nFeature Scaling: Performance depends heavily on the scale of the features, often necessitating normalization or standardization.\nImbalanced Data: Struggles with imbalanced datasets where the minority class may be overshadowed by the majority class due to the simple majority voting mechanism.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "k - Nearest Neighbours"
    ]
  },
  {
    "objectID": "chapters/chapter530_MLE/index.html#maximum-likelihood-estimation-mle",
    "href": "chapters/chapter530_MLE/index.html#maximum-likelihood-estimation-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "Maximum Likelihood Estimation (MLE)",
    "text": "Maximum Likelihood Estimation (MLE)\n\nMaximum Likelihood Estimation (MLE) is a statistical method used for estimating the parameters of a probability distribution by maximizing a likelihood function. This method finds the parameter values that make the observed data most probable.\n\n\nConsistency: As the sample size increases, the MLE converges to the true parameter value.\nAsymptotic Normality: The distribution of the MLE approaches a normal distribution as the sample size grows.\nEfficiency: Among unbiased estimators, the MLE has the lowest possible variance asymptotically.",
    "crumbs": [
      "Data crunching",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "chapters/chapter511_FIM/index.html#fim",
    "href": "chapters/chapter511_FIM/index.html#fim",
    "title": "Frequent Itemset Mining - FIM",
    "section": "FIM",
    "text": "FIM\nFIM TST\n(Michael Hahsler 2005)",
    "crumbs": [
      "Data crunching",
      "FIM"
    ]
  },
  {
    "objectID": "chapters/chapter500_TraditionalMeasures/index.html#central-tendency",
    "href": "chapters/chapter500_TraditionalMeasures/index.html#central-tendency",
    "title": "Traditional Methods",
    "section": "Central Tendency",
    "text": "Central Tendency",
    "crumbs": [
      "Data crunching",
      "Traditional Methods"
    ]
  },
  {
    "objectID": "chapters/chapter500_TraditionalMeasures/index.html#measures-of-spread",
    "href": "chapters/chapter500_TraditionalMeasures/index.html#measures-of-spread",
    "title": "Traditional Methods",
    "section": "Measures of Spread",
    "text": "Measures of Spread",
    "crumbs": [
      "Data crunching",
      "Traditional Methods"
    ]
  },
  {
    "objectID": "chapters/chapter500_TraditionalMeasures/index.html#measures-for-categoricals",
    "href": "chapters/chapter500_TraditionalMeasures/index.html#measures-for-categoricals",
    "title": "Traditional Methods",
    "section": "Measures for Categoricals",
    "text": "Measures for Categoricals",
    "crumbs": [
      "Data crunching",
      "Traditional Methods"
    ]
  },
  {
    "objectID": "chapters/chapter500_TraditionalMeasures/index.html#beware-of-summarized-data",
    "href": "chapters/chapter500_TraditionalMeasures/index.html#beware-of-summarized-data",
    "title": "Traditional Methods",
    "section": "Beware of summarized Data",
    "text": "Beware of summarized Data\n\n\n\n\n\n\n\n\n\n\ndataset\nmean_x\nmean_y\nstd_dev_x\nstd_dev_y\ncorr_x_y\n\n\n\n\naway\n54.266\n47.835\n16.770\n26.940\n−0.064\n\n\nbullseye\n54.269\n47.831\n16.769\n26.936\n−0.069\n\n\ncircle\n54.267\n47.838\n16.760\n26.930\n−0.068\n\n\ndino\n54.263\n47.832\n16.765\n26.935\n−0.064\n\n\ndots\n54.260\n47.840\n16.768\n26.930\n−0.060\n\n\nh_lines\n54.261\n47.830\n16.766\n26.940\n−0.062\n\n\nhigh_lines\n54.269\n47.835\n16.767\n26.940\n−0.069\n\n\nslant_down\n54.268\n47.836\n16.767\n26.936\n−0.069\n\n\nslant_up\n54.266\n47.831\n16.769\n26.939\n−0.069\n\n\nstar\n54.267\n47.840\n16.769\n26.930\n−0.063\n\n\nv_lines\n54.270\n47.837\n16.770\n26.938\n−0.069\n\n\nwide_lines\n54.267\n47.832\n16.770\n26.938\n−0.067\n\n\nx_shape\n54.260\n47.840\n16.770\n26.930\n−0.066",
    "crumbs": [
      "Data crunching",
      "Traditional Methods"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#quality",
    "href": "chapters/chapter300/index.html#quality",
    "title": "Data Quality",
    "section": "Quality",
    "text": "Quality\n\nQuality (dictionary)\n\nHow good or bad something is. Cambridge Dictionary\n\n\n“… Quality is a perceptual, conditional, and somewhat subjective attribute and may be understood differently by different people. …” Wiki",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#the-cathedral-and-the-bazaar",
    "href": "chapters/chapter300/index.html#the-cathedral-and-the-bazaar",
    "title": "Data Quality",
    "section": "The Cathedral and the Bazaar",
    "text": "The Cathedral and the Bazaar\n\n\n(Raymond 2001)",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#iso-8000",
    "href": "chapters/chapter300/index.html#iso-8000",
    "title": "Data Quality",
    "section": "ISO 8000",
    "text": "ISO 8000\n\nISO\n\nInternational Standards Organization\n\n\n\nset of standards for data quality for the exchange between organizations and systems\nMaster data in the form of characteristic data that are\n\nexchanged between organizations and systems, and that\nconform to the data specifications that\ncan be validated by computer software.\n\n\n\n(ISO 2022)",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#data-quality-1",
    "href": "chapters/chapter300/index.html#data-quality-1",
    "title": "Data Quality",
    "section": "Data Quality",
    "text": "Data Quality\n“… Data are of high quality if they are “Fit for Use” in their intended operational, decision-making and other roles. … ”\n\n\n(Herzog, Scheuren, and Winkler May)",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#data-quality-issues",
    "href": "chapters/chapter300/index.html#data-quality-issues",
    "title": "Data Quality",
    "section": "Data Quality issues",
    "text": "Data Quality issues\n\nData Entry for the London Summer Olympics\n\nan employee simple typed \\(20,000\\) seats available, while there were only \\(10,000\\)\nbuyers were able to exchange the already bought tickets\n\nThe London Whale\n\nusing a risk model that involved copy and pasting different multiple spreadsheats\nnot only were \\(\\$2bn\\) losses wrongly calculated, they were actually counted as gains summing up to a loss of \\(\\$6.2bn\\)",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#data-quality-tools",
    "href": "chapters/chapter300/index.html#data-quality-tools",
    "title": "Data Quality",
    "section": "Data Quality Tools",
    "text": "Data Quality Tools",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter300/index.html#data-quality-summary",
    "href": "chapters/chapter300/index.html#data-quality-summary",
    "title": "Data Quality",
    "section": "Data Quality summary",
    "text": "Data Quality summary\n\nData Quality is hard\nTop Down or Bottom up?\nsimple things go wrong in the real world\ntest stuff\nbe specific\nthere is no recipe\ngarbage in, garbage out\nclassic ways are not fancy, but useful. Use this to your advantage\nnever trust data\ninvest early, profit soon",
    "crumbs": [
      "Data Quality"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#the-end-of-theory",
    "href": "chapters/chapter100/index.html#the-end-of-theory",
    "title": "Introduction",
    "section": "The End of Theory?",
    "text": "The End of Theory?\n\n\n(Nast 2024) J. Craig Venter Institute",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#information-growth",
    "href": "chapters/chapter100/index.html#information-growth",
    "title": "Introduction",
    "section": "Information growth",
    "text": "Information growth\n\n\n(Luraschi, Kuo, and Ruiz 2019)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#drinking-leads-to-amazon-purchases",
    "href": "chapters/chapter100/index.html#drinking-leads-to-amazon-purchases",
    "title": "Introduction",
    "section": "Drinking leads to Amazon purchases",
    "text": "Drinking leads to Amazon purchases\n\n\nTyler Vigen",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#central-limit-theorem-clt",
    "href": "chapters/chapter100/index.html#central-limit-theorem-clt",
    "title": "Introduction",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#law-of-large-numbers",
    "href": "chapters/chapter100/index.html#law-of-large-numbers",
    "title": "Introduction",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#history-of-big-data",
    "href": "chapters/chapter100/index.html#history-of-big-data",
    "title": "Introduction",
    "section": "History of Big Data",
    "text": "History of Big Data",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#evolution-of-big-data",
    "href": "chapters/chapter100/index.html#evolution-of-big-data",
    "title": "Introduction",
    "section": "Evolution of Big Data",
    "text": "Evolution of Big Data",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#big-data-ingredients",
    "href": "chapters/chapter100/index.html#big-data-ingredients",
    "title": "Introduction",
    "section": "Big Data ingredients",
    "text": "Big Data ingredients",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#production-environment",
    "href": "chapters/chapter100/index.html#production-environment",
    "title": "Introduction",
    "section": "Production Environment",
    "text": "Production Environment",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#six-sigma",
    "href": "chapters/chapter100/index.html#six-sigma",
    "title": "Introduction",
    "section": "Six Sigma",
    "text": "Six Sigma",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#crisp-dm",
    "href": "chapters/chapter100/index.html#crisp-dm",
    "title": "Introduction",
    "section": "CRISP-DM",
    "text": "CRISP-DM",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter100/index.html#unifying-approach",
    "href": "chapters/chapter100/index.html#unifying-approach",
    "title": "Introduction",
    "section": "unifying approach",
    "text": "unifying approach\n\n\n(Schäfer et al. 2019)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/chapter200/index.html#which-do-you-want",
    "href": "chapters/chapter200/index.html#which-do-you-want",
    "title": "Data Sources",
    "section": "Which do you want?",
    "text": "Which do you want?",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "chapters/chapter200/index.html#databases",
    "href": "chapters/chapter200/index.html#databases",
    "title": "Data Sources",
    "section": "Databases",
    "text": "Databases",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "chapters/chapter400/index.html#why-data-bias-matters",
    "href": "chapters/chapter400/index.html#why-data-bias-matters",
    "title": "Data Bias",
    "section": "Why Data Bias matters",
    "text": "Why Data Bias matters",
    "crumbs": [
      "Data Bias",
      "slides"
    ]
  },
  {
    "objectID": "chapters/chapter400/index.html#types-of-data-bias",
    "href": "chapters/chapter400/index.html#types-of-data-bias",
    "title": "Data Bias",
    "section": "Types of Data Bias",
    "text": "Types of Data Bias",
    "crumbs": [
      "Data Bias",
      "slides"
    ]
  },
  {
    "objectID": "chapters/chapter400/index.html#bias-and-variance",
    "href": "chapters/chapter400/index.html#bias-and-variance",
    "title": "Data Bias",
    "section": "Bias and Variance",
    "text": "Bias and Variance\n\nBias:\n\nerror introduced by approximating a real-world problem with a simplified model\nleads to underfitting\n\n\n\n\nVariance:\n\nsensitivity to fluctuations in the data, such as random noise, as if it were true patterns\nleads to overfitting",
    "crumbs": [
      "Data Bias",
      "slides"
    ]
  },
  {
    "objectID": "chapters/chapter400/index.html#the-bias-variance-trade-off",
    "href": "chapters/chapter400/index.html#the-bias-variance-trade-off",
    "title": "Data Bias",
    "section": "The Bias Variance Trade Off",
    "text": "The Bias Variance Trade Off\n\n\n\n\n\nfamiliar?",
    "crumbs": [
      "Data Bias",
      "slides"
    ]
  },
  {
    "objectID": "chapters/chapter510_MapReduce/index.html#mapreduce",
    "href": "chapters/chapter510_MapReduce/index.html#mapreduce",
    "title": "Map Reduce",
    "section": "MapReduce",
    "text": "MapReduce",
    "crumbs": [
      "Data crunching",
      "Map Reduce"
    ]
  },
  {
    "objectID": "chapters/chapter510_MapReduce/index.html#mapreduce-summary",
    "href": "chapters/chapter510_MapReduce/index.html#mapreduce-summary",
    "title": "Map Reduce",
    "section": "MapReduce summary",
    "text": "MapReduce summary\n\n\nMapReduce is significant for its role in enabling the processing of massive datasets efficiently across distributed computing clusters.\nIt revolutionized big data processing by providing a scalable and fault-tolerant framework for handling large-scale computations.\nIts simplicity and scalability made it accessible to a wide range of industries and applications, from web search engines to scientific research.\nMapReduce paved the way for the development of other big data processing frameworks and technologies, influencing the evolution of distributed computing paradigms.\nIts impact extends beyond its original implementation, as concepts and principles from MapReduce have influenced the design of subsequent systems and architectures for big data processing.",
    "crumbs": [
      "Data crunching",
      "Map Reduce"
    ]
  },
  {
    "objectID": "chapters/chapter520_GradientDescent/index.html#gradient-descent",
    "href": "chapters/chapter520_GradientDescent/index.html#gradient-descent",
    "title": "Gradient Descent",
    "section": "Gradient Descent",
    "text": "Gradient Descent",
    "crumbs": [
      "Data crunching",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "chapters/chapter540_PCA/index.html#principal-component-analysis-pca",
    "href": "chapters/chapter540_PCA/index.html#principal-component-analysis-pca",
    "title": "Principal Component Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)",
    "crumbs": [
      "Data crunching",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "chapters/chapter552_kMeans/index.html#use-cases",
    "href": "chapters/chapter552_kMeans/index.html#use-cases",
    "title": "k- Means",
    "section": "Use Cases",
    "text": "Use Cases\n\n\n\n\nMarketing: Segment customers.\nCompression: Reduce image colors.\nOrganization: Cluster documents.\nSecurity: Detect anomalies.\nTargeting: Segment markets.\nNetworking: Identify communities.\n\n\n\n\n\n\n\nBiology: Group genes/proteins.\nRecommendations: Enhance systems.\nGeography: Cluster regions.\nLogistics: Optimize inventory.\nHealthcare: Cluster patients.\nIoT: Analyze sensor data.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "k- Means"
    ]
  },
  {
    "objectID": "chapters/chapter554_dbscan/index.html#use-cases",
    "href": "chapters/chapter554_dbscan/index.html#use-cases",
    "title": "Density Based Spatial Clustering and Application with Noise (DBSCAN)",
    "section": "Use Cases",
    "text": "Use Cases\n\n\nClassical cluster algorithms work best for:\n\nspherical\nconvex\ncompact and well separated data\n\n\n\n\nDBSCAN groups data points based on density, making it effective for identifying clusters of various shapes and sizes.\n\nCore Points: Points with at least a minimum number of neighbors (MinPts) within a specified radius (ε).\nBorder Points: Points within the ε radius of core points but with fewer neighbors.\nNoise Points: Outliers that don’t belong to any cluster.\n\nClusters are formed by connecting core points and their reachable neighbors, separating high-density regions from low-density areas.\n\n\n\n\n(Ester et al. 1996)",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Density Based Spatial Clustering and Application with Noise (DBSCAN)"
    ]
  },
  {
    "objectID": "chapters/chapter556_validity/index.html#cluster-tendency",
    "href": "chapters/chapter556_validity/index.html#cluster-tendency",
    "title": "Checking Cluster Validity",
    "section": "Cluster tendency",
    "text": "Cluster tendency\nDoes the data contain meaningful cluster (non-random structures)?\nThis is called: assessing cluster tendency",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Checking Cluster Validity"
    ]
  },
  {
    "objectID": "chapters/chapter556_validity/index.html#the-data",
    "href": "chapters/chapter556_validity/index.html#the-data",
    "title": "Checking Cluster Validity",
    "section": "the data",
    "text": "the data\n\niris data set\nrandom generated data\n\n\ndata(\"iris\")\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ndf &lt;- iris[,-5]",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Checking Cluster Validity"
    ]
  },
  {
    "objectID": "chapters/chapter556_validity/index.html#random-data",
    "href": "chapters/chapter556_validity/index.html#random-data",
    "title": "Checking Cluster Validity",
    "section": "random data",
    "text": "random data",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Checking Cluster Validity"
    ]
  },
  {
    "objectID": "chapters/chapter556_validity/index.html#visual-inspection",
    "href": "chapters/chapter556_validity/index.html#visual-inspection",
    "title": "Checking Cluster Validity",
    "section": "visual inspection",
    "text": "visual inspection",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Checking Cluster Validity"
    ]
  },
  {
    "objectID": "chapters/chapter556_validity/index.html#quality-again",
    "href": "chapters/chapter556_validity/index.html#quality-again",
    "title": "Checking Cluster Validity",
    "section": "Quality … again?",
    "text": "Quality … again?",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Checking Cluster Validity"
    ]
  },
  {
    "objectID": "chapters/chapter556_validity/index.html#quantitative---hopkins-statistic",
    "href": "chapters/chapter556_validity/index.html#quantitative---hopkins-statistic",
    "title": "Checking Cluster Validity",
    "section": "quantitative - Hopkins statistic",
    "text": "quantitative - Hopkins statistic\nThe Hopkins statistic (HOPKINS and SKELLAM 1954) is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution.",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Checking Cluster Validity"
    ]
  },
  {
    "objectID": "chapters/chapter556_validity/index.html#how-it-works",
    "href": "chapters/chapter556_validity/index.html#how-it-works",
    "title": "Checking Cluster Validity",
    "section": "how it works",
    "text": "how it works\n\nLet \\(D\\) be a real dataset, the Hopkins statistic is calculated as follows:\n\nSample uniformly \\(n\\) points ($p_1, , p_n $) from \\(D\\)\nFor each point \\(p_i \\in D\\), find its nearest neighbor \\(p_j\\), then compute the distance between \\(p_i\\) and \\(p_j\\) and denote it as \\(x_i = dist(p_i,p_j)\\)\nGenerate a simulated dataset (\\(random_D\\)) drawn from a random uniform distribution with \\(n-\\)points and the same variation as the original real dataset \\(D\\)\nFor each point \\(q_i \\in random_D\\), find its nearest neighbor \\(q_j\\) in \\(D\\); then compute the distance between \\(q_i\\) and \\(q_j\\) and denote it \\(y_i = dist(q_i,q_j)\\)\nCalculate the Hopkins statistics (\\(H\\)) as the mean nearest neighbor distance in the random dataset divided by the sum of the mean nearest neighbor distances in the real and across the simulated dataset.\n\n\n\\[\\begin{align}\nH = \\frac{\\sum^n_{i = 1} y_i}{\\sum^n_{i = 1} x_i+\\sum^n_{i = 1} y_i}\n\\end{align}\\]",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Checking Cluster Validity"
    ]
  },
  {
    "objectID": "chapters/chapter556_validity/index.html#visual-assessment-of-tendency-vat",
    "href": "chapters/chapter556_validity/index.html#visual-assessment-of-tendency-vat",
    "title": "Checking Cluster Validity",
    "section": "visual assessment of tendency (VAT)",
    "text": "visual assessment of tendency (VAT)\n\nCompute the (dis)similarity matrix (DM) between the objects in the dataset using the Euclidean distance measure\nReorder the (DM) so that similar objects are close to another. This process creates an ordered dissimilarity matrix (ODM)\nThe ODM is displayed as an ordered dissimilarity image (ODI), which is the visual output of VAT",
    "crumbs": [
      "Data crunching",
      "Cluster Analysis",
      "Checking Cluster Validity"
    ]
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#the-end-of-theory",
    "href": "docs/chapters/chapter100/index.html#the-end-of-theory",
    "title": "Introduction",
    "section": "The End of Theory?",
    "text": "The End of Theory?\n\n\n(Nast 2024) J. Craig Venter Institute"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#information-growth",
    "href": "docs/chapters/chapter100/index.html#information-growth",
    "title": "Introduction",
    "section": "Information growth",
    "text": "Information growth\n\n\n(Luraschi, Kuo, and Ruiz 2019)"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#drinking-leads-to-amazon-purchases",
    "href": "docs/chapters/chapter100/index.html#drinking-leads-to-amazon-purchases",
    "title": "Introduction",
    "section": "Drinking leads to Amazon purchases",
    "text": "Drinking leads to Amazon purchases\n\n\nTyler Vigen"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#central-limit-theorem-clt",
    "href": "docs/chapters/chapter100/index.html#central-limit-theorem-clt",
    "title": "Introduction",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#law-of-large-numbers",
    "href": "docs/chapters/chapter100/index.html#law-of-large-numbers",
    "title": "Introduction",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#history-of-big-data",
    "href": "docs/chapters/chapter100/index.html#history-of-big-data",
    "title": "Introduction",
    "section": "History of Big Data",
    "text": "History of Big Data"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#evolution-of-big-data",
    "href": "docs/chapters/chapter100/index.html#evolution-of-big-data",
    "title": "Introduction",
    "section": "Evolution of Big Data",
    "text": "Evolution of Big Data"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#big-data-ingredients",
    "href": "docs/chapters/chapter100/index.html#big-data-ingredients",
    "title": "Introduction",
    "section": "Big Data ingredients",
    "text": "Big Data ingredients"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#production-environment",
    "href": "docs/chapters/chapter100/index.html#production-environment",
    "title": "Introduction",
    "section": "Production Environment",
    "text": "Production Environment"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#six-sigma",
    "href": "docs/chapters/chapter100/index.html#six-sigma",
    "title": "Introduction",
    "section": "Six Sigma",
    "text": "Six Sigma"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#crisp-dm",
    "href": "docs/chapters/chapter100/index.html#crisp-dm",
    "title": "Introduction",
    "section": "CRISP-DM",
    "text": "CRISP-DM"
  },
  {
    "objectID": "docs/chapters/chapter100/index.html#unifying-approach",
    "href": "docs/chapters/chapter100/index.html#unifying-approach",
    "title": "Introduction",
    "section": "unifying approach",
    "text": "unifying approach\n\n\n(Schäfer et al. 2019)"
  },
  {
    "objectID": "docs/chapters/chapter300/index.html#quality",
    "href": "docs/chapters/chapter300/index.html#quality",
    "title": "Data Quality",
    "section": "Quality",
    "text": "Quality\n\nQuality (dictionary)\n\nHow good or bad something is. Cambridge Dictionary\n\n\n“… Quality is a perceptual, conditional, and somewhat subjective attribute and may be understood differently by different people. …” Wiki"
  },
  {
    "objectID": "docs/chapters/chapter300/index.html#the-cathedral-and-the-bazaar",
    "href": "docs/chapters/chapter300/index.html#the-cathedral-and-the-bazaar",
    "title": "Data Quality",
    "section": "The Cathedral and the Bazaar",
    "text": "The Cathedral and the Bazaar\n\n\n(Raymond 2001)"
  },
  {
    "objectID": "docs/chapters/chapter300/index.html#iso-8000",
    "href": "docs/chapters/chapter300/index.html#iso-8000",
    "title": "Data Quality",
    "section": "ISO 8000",
    "text": "ISO 8000\n\nISO\n\nInternational Standards Organization\n\n\n\nset of standards for data quality for the exchange between organizations and systems\nMaster data in the form of characteristic data that are\n\nexchanged between organizations and systems, and that\nconform to the data specifications that\ncan be validated by computer software.\n\n\n\n(ISO 2022)"
  },
  {
    "objectID": "docs/chapters/chapter300/index.html#data-quality-1",
    "href": "docs/chapters/chapter300/index.html#data-quality-1",
    "title": "Data Quality",
    "section": "Data Quality",
    "text": "Data Quality\n“… Data are of high quality if they are “Fit for Use” in their intended operational, decision-making and other roles. … ”\n\n\n(Herzog, Scheuren, and Winkler May)"
  },
  {
    "objectID": "docs/chapters/chapter300/index.html#data-quality-issues",
    "href": "docs/chapters/chapter300/index.html#data-quality-issues",
    "title": "Data Quality",
    "section": "Data Quality issues",
    "text": "Data Quality issues\n\nData Entry for the London Summer Olympics\n\nan employee simple typed \\(20,000\\) seats available, while there were only \\(10,000\\)\nbuyers were able to exchange the already bought tickets\n\nThe London Whale\n\nusing a risk model that involved copy and pasting different multiple spreadsheats\nnot only were \\(\\$2bn\\) losses wrongly calculated, they were actually counted as gains summing up to a loss of \\(\\$6.2bn\\)"
  },
  {
    "objectID": "docs/chapters/chapter300/index.html#data-quality-tools",
    "href": "docs/chapters/chapter300/index.html#data-quality-tools",
    "title": "Data Quality",
    "section": "Data Quality Tools",
    "text": "Data Quality Tools"
  },
  {
    "objectID": "docs/chapters/chapter300/index.html#data-quality-summary",
    "href": "docs/chapters/chapter300/index.html#data-quality-summary",
    "title": "Data Quality",
    "section": "Data Quality summary",
    "text": "Data Quality summary\n\nData Quality is hard\nTop Down or Bottom up?\nsimple things go wrong in the real world\ntest stuff\nbe specific\nthere is no recipe\ngarbage in, garbage out\nclassic ways are not fancy, but useful. Use this to your advantage\nnever trust data\ninvest early, profit soon"
  },
  {
    "objectID": "docs/chapters/chapter500_TraditionalMeasures/index.html#central-tendency",
    "href": "docs/chapters/chapter500_TraditionalMeasures/index.html#central-tendency",
    "title": "Traditional Methods",
    "section": "Central Tendency",
    "text": "Central Tendency"
  },
  {
    "objectID": "docs/chapters/chapter500_TraditionalMeasures/index.html#measures-of-spread",
    "href": "docs/chapters/chapter500_TraditionalMeasures/index.html#measures-of-spread",
    "title": "Traditional Methods",
    "section": "Measures of Spread",
    "text": "Measures of Spread"
  },
  {
    "objectID": "docs/chapters/chapter500_TraditionalMeasures/index.html#measures-for-categoricals",
    "href": "docs/chapters/chapter500_TraditionalMeasures/index.html#measures-for-categoricals",
    "title": "Traditional Methods",
    "section": "Measures for Categoricals",
    "text": "Measures for Categoricals"
  },
  {
    "objectID": "docs/chapters/chapter500_TraditionalMeasures/index.html#beware-of-summarized-data",
    "href": "docs/chapters/chapter500_TraditionalMeasures/index.html#beware-of-summarized-data",
    "title": "Traditional Methods",
    "section": "Beware of summarized Data",
    "text": "Beware of summarized Data\n\n\n\n\n\n\n\n\n\n\ndataset\nmean_x\nmean_y\nstd_dev_x\nstd_dev_y\ncorr_x_y\n\n\n\n\naway\n54.266\n47.835\n16.770\n26.940\n−0.064\n\n\nbullseye\n54.269\n47.831\n16.769\n26.936\n−0.069\n\n\ncircle\n54.267\n47.838\n16.760\n26.930\n−0.068\n\n\ndino\n54.263\n47.832\n16.765\n26.935\n−0.064\n\n\ndots\n54.260\n47.840\n16.768\n26.930\n−0.060\n\n\nh_lines\n54.261\n47.830\n16.766\n26.940\n−0.062\n\n\nhigh_lines\n54.269\n47.835\n16.767\n26.940\n−0.069\n\n\nslant_down\n54.268\n47.836\n16.767\n26.936\n−0.069\n\n\nslant_up\n54.266\n47.831\n16.769\n26.939\n−0.069\n\n\nstar\n54.267\n47.840\n16.769\n26.930\n−0.063\n\n\nv_lines\n54.270\n47.837\n16.770\n26.938\n−0.069\n\n\nwide_lines\n54.267\n47.832\n16.770\n26.938\n−0.067\n\n\nx_shape\n54.260\n47.840\n16.770\n26.930\n−0.066"
  },
  {
    "objectID": "docs/chapters/chapter511_FIM/index.html#fim",
    "href": "docs/chapters/chapter511_FIM/index.html#fim",
    "title": "Frequent Itemset Mining - FIM",
    "section": "FIM",
    "text": "FIM\nFIM TST\n(Michael Hahsler 2005)"
  }
]