---
title: "Data Crunching"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
    include-in-header: 
      - pseudocode.html
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
  - pseudocode
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false

library(datasauRus)
library(tidyverse)
library(gt)
library(here)
library(quarto)
library(gganimate)

```

# AI

![](img/data_crunching.webp){fig-align="center"}

# Classic Measures

## Central Tendency

### Mean

{{< include img/_mean_anim.qmd >}}

### Median

{{< include img/_median_anim.qmd >}}

### comparison of mean and median

{{< include img/_median_mean_compare_anim.qmd >}}

## Measures of Spread

### Standard deviation and variance

{{< include img/_sd_anim.qmd >}}

### Interquartile Range

{{< include img/_iqr_anim.qmd >}}

## Measures for Categoricals

### Mode and Frequencies

{{< include img/_mode_anim.qmd >}}

### Ratios

```{r}

vec <- c(1,2,3,4,5,6,6,7,8)

table(vec) |> as.data.frame() |> 
  mutate(
    ratio_total = Freq/sum(Freq)
  ) |> 
  gtsummary::tbl_summary()

```

## Beware of summarized Data 

::: {.r-stack}

:::{.fragment .fade-out}

```{r}
datasaurus_dozen %>% 
    group_by(dataset) %>% 
    summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
    ) %>% 
  gt() %>% 
   fmt_number(decimals = 3) %>% 
  tab_options(
    # table.font.size = 25
  ) 

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="away"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="bullseye"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="circle"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="dino"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="dots"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="h_lines"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="high_lines"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="slant_down"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="slant_up"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="star"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="v_lines"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="wide_lines"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="x_shape"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen , aes(x = x, y = y))+
    geom_point()+
    theme_void(base_size = 10)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::

# Algorithms

## MapReduce

### Single Node Architecture

![](img/single_node_arch.svg)

### Cluster Architecture

![](img/cluster_arch.svg)

Google has an estimated 1M servers <http://bit.ly/Shh0RO>

### SERVERS!!!

![](img/servers.jpg){.r-stretch}

### Large Scale Computing

- Challenges:
  - How do you distribute computations?
  - How can we make it easy to write distributed programs

Machines fail:

. . .

- One Server may stay up 3 years (1000 days)
- If there are 1000 servers, expect to loose 1/day
- Google estimate: 1M servers $\rightarrow$ 1000 machines fail EVERY DAY


### Issue and Solution

- Issue: Copying data over a network takes time
- Idea
  - Bring computation close to the data
  - Store files multiple times for reliability

. . .

- MapReduce adresses these problems
  - Elegant way to work with big data
  - Storage Infrastructure - File System (Google: GFS, Hadoop: HDFS)
  - Programming Model: MapReduce


### (D)istributed (F)ile (S)ystem

- Chunk servers
  - File is split into contiguous chunks
  - Typically each chunk is 16-64MB
  - Each chunk replicated (usually 2x or 3x)
  - Try to keep replicas in different racks

### Chunk Servers

![](img/chunk_servers.svg)

### Theory

{{< include img/_MapReduce_anim.qmd >}}

### Classroom

We will simulate MapReduce in the classroom

::::{style="font-size: 85%;"}

1. step: 4 students count the words in the raw data. Time is ticking.
2. step: 2 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
3. step: 4 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
4. step: 5 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
5. step: 10 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
6. step: 20 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.

::::


#### Classroom Evaluation

1. Which method took the longest?
2. What was the most balanced method?
3. Would more "servers" help?

### using Software

```{r}
#| include: false
#| 
clean <- function(x) {
  gsub('[,.;:\'"()]','',tolower(x))
}

word_count_simple <- function(lines) {
  chunks <- strsplit(clean(lines),'\\s')
  words <- do.call(c, chunks)
  table(words)
}

lorem <- read_lines(here("data","loremipsum_source.txt"))

lorem_smpl <- sample(
  lorem, 
  size = 500, 
  replace = TRUE,
  # prob = rep(c(0.05),500)
  )

writeLines(lorem_smpl, here("data","exchange.txt"))

quarto_render(input = here("data","_MapReduce_classroom.qmd"))

lorem_freq_tbl <- word_count_simple(lorem_smpl)

lorem_df <- lorem_freq_tbl %>%
  as.data.frame() %>%
  mutate(
    percentage = Freq/sum(Freq),
    percentage_round = round(percentage,digits = 4)
    ) |>
arrange(
  desc(Freq)
  ) |> 
  filter(percentage>0.02)

```

::: {.r-stack}

::: {.fragment .fade-out}

```{r}

lorem_df |> 
  ggplot(
    aes(
      x = Freq,
      y = reorder(words,Freq)
    )
  )+
  geom_col(
    fill = "steelblue"
  )+
  scale_x_continuous(
    expand = c(0,0,0.05,0)
  )+
  theme_bw(
    base_size = 15
  )+
  labs(
    title = "absolute frequencies of words",
    x = "count",
    y = ""
  )

```

:::

::: {.fragment .fade-in}

```{r}

lorem_df |> 
  ggplot(
    aes(
      x = percentage_round,
      y = reorder(words,percentage_round)
    )
  )+
  geom_col(
    fill = "steelblue"
  )+
  scale_x_continuous(
    labels = scales::percent,
    expand = c(0,0,0.05,0)
  )+
  theme_bw(
    base_size = 15
  )+
  labs(
    title = "relative frequencies of words",
    x = "percentage",
    y = ""
  )

```

:::

:::

### Dealing with Failures 

::: {.incremental}

::::{style="font-size: 85%;"}

- Map worker failure
  - Map tasks completed or in-progress at worker are reset to idle
  - Reduce workers are notified when task is rescheduled on another worker
- Reduce worker failure
  - Only in-progress tasks are reset to idle 
  - Reduce task is restarted
- Master failure
  - MapReduce task is aborted and client is notified

::::

:::

### How many MapReduce jobs?

$M$ map tasks, $R$ reduce tasks

Rule of thumb:

- Make $M$ much larger than the number of nodes in the cluster
- One DFS chunk per Map is common
- Improves dynammics load balancing and speeds up recovery from worker failures

. . .

Usually $R$ is smaller than $M$

- Because output is spread across $R$ files

## MapReduce summary

::: {.incremental}

::::{style="font-size: 85%;"}

- MapReduce is significant for its role in enabling the processing of massive datasets efficiently across distributed computing clusters.
- It revolutionized big data processing by providing a scalable and fault-tolerant framework for handling large-scale computations.
- Its simplicity and scalability made it accessible to a wide range of industries and applications, from web search engines to scientific research.
- MapReduce paved the way for the development of other big data processing frameworks and technologies, influencing the evolution of distributed computing paradigms.
- Its impact extends beyond its original implementation, as concepts and principles from MapReduce have influenced the design of subsequent systems and architectures for big data processing.

::::

:::

## Gradient Descent

### Use Cases: General

::: {.incremental}
::::{style="font-size: 70%;"}

1. **Machine Learning:** Used to train models like neural networks, improving accuracy by minimizing error functions.
  
2. **Deep Learning:** Vital for optimizing complex neural network architectures in tasks like image recognition and natural language processing.

3. **Reinforcement Learning:** Enables agents to learn optimal strategies by updating policies or value functions in interaction with environments.

4. **Optimization Problems:** Applies to various domains, including physics, engineering, finance, and healthcare, to minimize costs or maximize utility functions.

5. **Computer Vision:** Key for training convolutional neural networks to classify objects, detect features, and segment images accurately.

6. **Finance and Economics:** Helps in financial modeling, algorithmic trading, and risk management by optimizing trading strategies and pricing financial instruments.

7. **Healthcare and Biology:** Utilized in drug discovery, genomic analysis, and medical image analysis to optimize models for diagnosis and treatment planning.
::::
:::

### Use Cases: Statistical Learning

::: {.incremental}
::::{style="font-size: 70%;"}

1. **Parameter Estimation:** Used in maximum likelihood and maximum a posteriori estimation to find model parameters that best fit observed data.

2. **Regression:** Employed in linear and logistic regression to minimize errors between observed and predicted values.

3. **Regularization:** Implements L1 and L2 regularization to prevent overfitting in models.

4. **Dimensionality Reduction:** Utilized in PCA and factor analysis to reduce data dimensionality while preserving information.

5. **Clustering:** Optimizes objective functions in k-means and Gaussian mixture models for better cluster formation.

6. **Survival Analysis:** Estimates parameters in survival models like Cox proportional hazards model for analyzing time-to-event data.

::::
:::

### The idea

{{< include img/_gd_anim.qmd >}}

### The loss function - formulation

::::{style="font-size: 85%;"}

* The loss function is at the heart of the gradient descent algorithm. We start with linear regression.

::::

:::{.fragment .fade-in}

\begin{align}
Y(x_i) = \theta_0 + \theta_1 x_i
\end{align}

::::{style="font-size: 85%;"}

* The loss function needs to be differentiable and convex, which is satisfied for the MSE.
::::

:::

:::{.fragment .fade-in}

\begin{align}
J(\theta_1,\theta_0) = \overbrace{\frac{1}{N}\sum_{i=1}^N(y(x_i)-y_i)^2}^{MSE}
\end{align}

:::

### The loss function - partial derivative

::::{style="font-size: 45%;"}

In order to solve for the parameters we do some math and geht the partial derivatives:

\begin{align}
\frac{\partial \text{MSE}}{\partial \theta_0} &= -\frac{2}{n} \sum_{i=1}^n (y_i - \hat{y}_i) \\
\frac{\partial \text{MSE}}{\partial \theta_1} &= -\frac{2}{n} \sum_{i=1}^n (y_i - \hat{y}_i) x_i
\end{align}

$\theta_0$ and $\theta_1$ are then iteratively updated:

::::

::::{style="font-size: 65%;"}

\begin{align}
\theta_0 &\leftarrow \theta_0 - \alpha \frac{\partial \text{MSE}}{\partial \theta_0} \\
\theta_0 &\leftarrow \theta_0 + \alpha \left( \frac{2}{n} \sum_{i=1}^n (y_i - \hat{y}_i) \right)\\
\theta_1 &\leftarrow \theta_1 - \alpha \frac{\partial \text{MSE}}{\partial \theta_1}\\
\theta_1 &\leftarrow \theta_1 + \alpha \left( \frac{2}{n} \sum_{i=1}^n (y_i - \hat{y}_i) x_i \right)
\end{align}

::::

### The learning rate $\alpha$

- Hyperparameter
- scales the gradient, influencing how model parameters are adjusted

![](img/learning_rate.svg)

### Classroom

![](img/gd_classroom.svg){fig-align="center"}

### Algorithm

::::{style="font-size: 50%;"}


```pseudocode
#| html-line-number: true

\begin{algorithm}
\caption{gradient descent}
\begin{algorithmic}
\Require cost function $J(\theta)$, learning rate $\alpha$, number of iterations $n$
\State $i \gets 0$
\State random $\theta$
\Procedure{GradientDescent}{$J(\theta),\theta, \alpha, n$}
\For{$i$ \To $n$}
\For{$j = 1$ \To number of training examples $m$}
\State \textbf{Compute gradient of $J$ wrt $\theta$}
\State gradient = $\nabla J(\theta, x^j, y^j)$
\State \textbf{Update the parameters $\theta$:}
\State $\theta = \theta - \alpha \times gradient$
\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

```

::::

### Simulation

\begin{align}
Y(x_i) = \theta_0 + \theta_1 x_i
\end{align}

```{r}
#| echo: true
#| code-line-numbers: "|3|4|5|6|7|8"

set.seed(1000) #reproducibility

theta_0 <- 5 # parameter 1
theta_1 <- 2 # parameter 2
n_obs <- 500 # number of datapoints
x <- rnorm(n_obs) # independent variable
y <- theta_1*x + theta_0 + rnorm(n_obs, 0, 3) # simulate data
rm(theta_0, theta_1) # get rid of ground truth
```

#### The data

```{r}
data <- tibble(x = x, y = y)

ggplot(data, aes(x = x, y = y)) + geom_point(size = 2) + theme_bw() + labs(title = 'Simulated Data')

```

```{r}
#| include: false

# check coef with ols (for lecturer)

ols_fit <- lm(y ~ x, data = data)
summary(ols_fit)
ols_fit$coefficients

cost_function <- function(theta_0, theta_1, x, y){
  pred <- theta_1*x + theta_0
  res_sq <- (y - pred)^2
  res_ss <- sum(res_sq)
  return(mean(res_ss))
}

cost_function(theta_0 = ols_fit$coefficients[1][[1]],
         theta_1 = ols_fit$coefficients[2][[1]],
         x = data$x, y = data$y)

sum(resid(ols_fit)^2)

```

#### core functions and parameters

```{r}
#| echo: true
#| code-line-numbers: "5,6,11,12,17,18"

gradient_desc <- function(theta_0, theta_1, x, y){
  N = length(x)
  pred <- theta_1*x + theta_0
  res <- y - pred
  delta_theta_0 <- (2/N)*sum(res)
  delta_theta_1 <- (2/N)*sum(res*x)
  return(c(delta_theta_0, delta_theta_1))
}
minimize_function <- function(theta_0, theta_1, x, y, alpha){
  gd <- gradient_desc(theta_0, theta_1, x, y)
  d_theta_0 <- gd[1] * alpha
  d_theta_1 <- gd[2] * alpha
  new_theta_0 <- theta_0 + d_theta_0
  new_theta_1 <- theta_1 + d_theta_1
  return(c(new_theta_0, new_theta_1))
}
alpha <- 0.1
iter <- 100

```

#### optimization

```{r}
#| echo: true
#| code-line-numbers: "4,5,6,7,8"

res <- list()
res[[1]] <- c(0, 0)

for (i in 2:iter){
  res[[i]] <- minimize_function(
    res[[i-1]][1], res[[i-1]][2], data$x, data$y, alpha
  )
}
```


```{r}
#| include: false

res <- lapply(res, function(x) as.data.frame(t(x))) %>% bind_rows()
colnames(res) <- c('theta0', 'theta1')

loss <- res %>% as_tibble() %>% rowwise() %>%
  summarise(mse = cost_function(theta0, theta1, data$x, data$y))

res <- res %>% bind_cols(loss) %>%
  mutate(iteration = seq(1, 100)) %>% as_tibble()

res %>% glimpse()
```

#### mse vs. iteration

```{r}
ggplot(res, aes(x = iteration, y = mse)) + 
  geom_point(size = 2) +
  geom_line(aes(group = 1))+
  theme_classic(base_size = 15) 

```

#### the models

```{r}

ggplot(data, aes(x = x, y = y)) +
  geom_point(size = 2) +
  geom_abline(aes(intercept = theta0, slope = theta1),
              linewidth = 1.5,
              data = res %>% slice_tail() |> select(-iteration), size = 0.5, color = 'green3')+
    geom_abline(aes(intercept = theta0, slope = theta1),
              data = res, size = 0.5, color = 'red') +
    geom_abline(aes(intercept = theta0, slope = theta1),
              data = res %>% slice_head() |> select(-iteration), size = 0.5, color = 'blue') +
  theme_classic() +
  labs(
    title = "gradient descent at work"
  )+
  transition_states(states = iteration)+
  shadow_mark(colour = 'gray85', size = 0.75)

```

#### different learning rates

```{r}

alpha001 <- 0.01
alpha1 <- 1

iter <- 100

res001 <- list()
res001[[1]] <- c(0, 0)

for (i in 2:iter){
  res001[[i]] <- minimize_function(
    res001[[i-1]][1], res001[[i-1]][2], data$x, data$y, alpha001
  )
}

res001 <- lapply(res001, function(x) as.data.frame(t(x))) %>% bind_rows()
colnames(res001) <- c('theta0', 'theta1')

loss001 <- res001 %>% as_tibble() %>% rowwise() %>%
  summarise(mse = cost_function(theta0, theta1, data$x, data$y))

res001 <- res001 %>% bind_cols(loss001) %>%
  mutate(iteration = seq(1, 100)) %>% as_tibble()


res1 <- list()
res1[[1]] <- c(0, 0)

for (i in 2:iter){
  res1[[i]] <- minimize_function(
    res1[[i-1]][1], res1[[i-1]][2], data$x, data$y, alpha1
  )
}

res1 <- lapply(res1, function(x) as.data.frame(t(x))) %>% bind_rows()
colnames(res1) <- c('theta0', 'theta1')

loss1 <- res1 %>% as_tibble() %>% rowwise() %>%
  summarise(mse = cost_function(theta0, theta1, data$x, data$y))

res1 <- res1 %>% bind_cols(loss1) %>%
  mutate(iteration = seq(1, 100)) %>% as_tibble()


out <- 
  data.frame(
    alpha = c("good","low","high")
  ) |> 
  add_column(
    result = list(res,res001,res1)
  ) |> 
  unnest(result)


out |> 
  ggplot(
    aes(
      x = iteration,
      y = mse,
      color = alpha
    )
  )+
  geom_point(size = 2)+
  geom_line()+
  scale_color_brewer(palette = "Set1")+
  theme_classic(base_size = 15)




```


# References
