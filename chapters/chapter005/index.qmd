---
title: "Data Crunching"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false

library(datasauRus)
library(tidyverse)
library(gt)
library(here)
library(quarto)

```

# AI

![](img/data_crunching.webp){fig-align="center"}

# Classic Measures

## Central Tendency

### Mean

{{< include img/_mean_anim.qmd >}}

### Median

{{< include img/_median_anim.qmd >}}

### comparison of mean and median

{{< include img/_median_mean_compare_anim.qmd >}}

## Measures of Spread

### Standard deviation and variance

{{< include img/_sd_anim.qmd >}}

### Interquartile Range

{{< include img/_iqr_anim.qmd >}}

## Measures for Categoricals

### Mode and Frequencies

{{< include img/_mode_anim.qmd >}}

### Ratios

```{r}

vec <- c(1,2,3,4,5,6,6,7,8)

table(vec) |> as.data.frame() |> 
  mutate(
    ratio_total = Freq/sum(Freq)
  ) |> 
  gtsummary::tbl_summary()

```

## Beware of summarized Data 

::: {.r-stack}

:::{.fragment .fade-out}

```{r}
datasaurus_dozen %>% 
    group_by(dataset) %>% 
    summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
    ) %>% 
  gt() %>% 
   fmt_number(decimals = 3) %>% 
  tab_options(
    # table.font.size = 25
  ) 

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="away"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="bullseye"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="circle"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="dino"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="dots"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="h_lines"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="high_lines"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="slant_down"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="slant_up"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="star"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="v_lines"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="wide_lines"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen |> filter(dataset=="x_shape"), aes(x = x, y = y))+
    geom_point()+
    theme_minimal(base_size = 25)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::{.fragment .fade-in-then-out}

```{r}
#| fig-width: 7
#| fig-height: 7
#| 
ggplot(datasaurus_dozen , aes(x = x, y = y))+
    geom_point()+
    theme_void(base_size = 10)+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 5)+
  coord_equal()

```

:::

:::

# Algorithms

## MapReduce

### Single Node Architecture

![](img/single_node_arch.svg)

### Cluster Architecture

![](img/cluster_arch.svg)

Google has an estimated 1M servers <http://bit.ly/Shh0RO>

### SERVERS!!!

![](img/servers.jpg){.r-stretch}

### Large Scale Computing

- Challenges:
  - How do you distribute computations?
  - How can we make it easy to write distributed programs

Machines fail:

. . .

- One Server may stay up 3 years (1000 days)
- If there are 1000 servers, expect to loose 1/day
- Google estimate: 1M servers $\rightarrow$ 1000 machines fail EVERY DAY


### Issue and Solution

- Issue: Copying data over a network takes time
- Idea
  - Bring computation close to the data
  - Store files multiple times for reliability

. . .

- MapReduce adresses these problems
  - Elegant way to work with big data
  - Storage Infrastructure - File System (Google: GFS, Hadoop: HDFS)
  - Programming Model: MapReduce


### (D)istributed (F)ile (S)ystem

- Chunk servers
  - File is split into contiguous chunks
  - Typically each chunk is 16-64MB
  - Each chunk replicated (usually 2x or 3x)
  - Try to keep replicas in different racks

### Chunk Servers

![](img/chunk_servers.svg)

### Theory

{{< include img/_MapReduce_anim.qmd >}}

### Classroom

We will simulate MapReduce in the classroom

::::{style="font-size: 85%;"}

1. step: 4 students count the words in the raw data. Time is ticking.
2. step: 2 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
3. step: 4 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
4. step: 5 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
5. step: 10 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
6. step: 20 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.

::::


#### Classroom Evaluation

1. Which method took the longest?
2. What was the most balanced method?
3. Would more "servers" help?

### using Software

```{r}
#| include: false
#| 
clean <- function(x) {
  gsub('[,.;:\'"()]','',tolower(x))
}

word_count_simple <- function(lines) {
  chunks <- strsplit(clean(lines),'\\s')
  words <- do.call(c, chunks)
  table(words)
}

lorem <- read_lines(here("data","loremipsum_source.txt"))

lorem_smpl <- sample(
  lorem, 
  size = 500, 
  replace = TRUE,
  # prob = rep(c(0.05),500)
  )

writeLines(lorem_smpl, here("data","exchange.txt"))

quarto_render(input = here("data","_MapReduce_classroom.qmd"))

lorem_freq_tbl <- word_count_simple(lorem_smpl)

lorem_df <- lorem_freq_tbl %>%
  as.data.frame() %>%
  mutate(
    percentage = Freq/sum(Freq),
    percentage_round = round(percentage,digits = 4)
    ) |>
arrange(
  desc(Freq)
  ) |> 
  filter(percentage>0.02)

```

::: {.r-stack}

::: {.fragment .fade-out}

```{r}

lorem_df |> 
  ggplot(
    aes(
      x = Freq,
      y = reorder(words,Freq)
    )
  )+
  geom_col(
    fill = "steelblue"
  )+
  scale_x_continuous(
    expand = c(0,0,0.05,0)
  )+
  theme_bw(
    base_size = 15
  )+
  labs(
    title = "absolute frequencies of words",
    x = "count",
    y = ""
  )

```

:::

::: {.fragment .fade-in}

```{r}

lorem_df |> 
  ggplot(
    aes(
      x = percentage_round,
      y = reorder(words,percentage_round)
    )
  )+
  geom_col(
    fill = "steelblue"
  )+
  scale_x_continuous(
    labels = scales::percent,
    expand = c(0,0,0.05,0)
  )+
  theme_bw(
    base_size = 15
  )+
  labs(
    title = "relative frequencies of words",
    x = "percentage",
    y = ""
  )

```

:::

:::

### Dealing with Failures 

::: {.incremental}

::::{style="font-size: 85%;"}

- Map worker failure
  - Map tasks completed or in-progress at worker are reset to idle
  - Reduce workers are notified when task is rescheduled on another worker
- Reduce worker failure
  - Only in-progress tasks are reset to idle 
  - Reduce task is restarted
- Master failure
  - MapReduce task is aborted and client is notified

::::

:::

### How many MapReduce jobs?

$M$ map tasks, $R$ reduce tasks

Rule of thumb:

- Make $M$ much larger than the number of nodes in the cluster
- One DFS chunk per Map is common
- Improves dynammics load balancing and speeds up recovery from worker failures

. . .

Usually $R$ is smaller than $M$

- Because output is spread across $R$ files

### MapReduce summary

::: {.incremental}

::::{style="font-size: 85%;"}

- MapReduce is significant for its role in enabling the processing of massive datasets efficiently across distributed computing clusters.
- It revolutionized big data processing by providing a scalable and fault-tolerant framework for handling large-scale computations.
- Its simplicity and scalability made it accessible to a wide range of industries and applications, from web search engines to scientific research.
- MapReduce paved the way for the development of other big data processing frameworks and technologies, influencing the evolution of distributed computing paradigms.
- Its impact extends beyond its original implementation, as concepts and principles from MapReduce have influenced the design of subsequent systems and architectures for big data processing.

::::

:::


# References
