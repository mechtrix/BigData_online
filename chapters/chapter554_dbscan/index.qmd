---
title: "Density Based Spatial Clustering and Application with Noise (DBSCAN)"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
    include-in-header:
      text: |
        <script>
        MathJax = {
          loader: {
            load: ['[tex]/boldsymbol']
          },
          tex: {
            tags: "all",
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
            packages: {
              '[+]': ['boldsymbol']
            }
          }
        };
        </script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
  - pseudocode
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false


library(tidyverse)
library(factoextra)
library(patchwork)
library(cluster)
library(scico)

```


## Use Cases

::: {.r-stack}

::: {.fragment .fade-out}

Classical cluster algorithms work best for:

- spherical
- convex
- compact and well separated data

:::

::: {.fragment .fade-in}

::::{style="font-size: 85%;"}

DBSCAN groups data points based on density, making it effective for identifying clusters of various shapes and sizes. 

1. **Core Points**: Points with at least a minimum number of neighbors (MinPts) within a specified radius (ε).
2. **Border Points**: Points within the ε radius of core points but with fewer neighbors.
3. **Noise Points**: Outliers that don't belong to any cluster.

Clusters are formed by connecting core points and their reachable neighbors, separating high-density regions from low-density areas.

::::

:::

:::

::: {.attribution}
[@ester1996densitybased]
:::

### The clustering problem

![](img/dbscan_data.png){fig-align="center"}

::: {.attribution}

[@ester1996densitybased]

:::

### the idea

{{< include img/_dbscan_anim.qmd >}}

### where it works

```{r}
data("multishapes")

multishapes |>
  mutate(
    shape = shape |> as_factor()
  ) |> 
  ggplot(
    aes(
      x = x,
      y = y
      )
  )+
  geom_point(
    size = 2
  )+
  labs(
    title = "the Data"
  )+
  theme_bw(
    base_size = 15
  )+
  theme(
    legend.position = "bottom"
  )
  
  
  
```

### we have kmeans don't we?

```{r}

df <- multishapes[, 1:2]
set.seed(123)
km.res <- kmeans(df, 5, nstart = 25)
fviz_cluster(km.res, df, ellipse = TRUE, geom = "point")+
  theme_bw(
    base_size = 15
  )+
  theme(
    legend.position = "bottom"
  )

```

### DBSCAN

Direct density reachable: 
:   A point “A” is directly density reachable from another point “B” if i) “A” is in the $\varepsilon$-neighborhood of “B” and ii) “B” is a core point.

Density reachable: 
:   A point “A” is density reachable from “B” if there are a set of core points leading from “B” to “A.

Density connected: 
:   Two points “A” and “B” are density connected if there are a core point “C”, such that both “A” and “B” are density reachable from “C”.

### lets get to it

```{r}

df <- multishapes[, 1:2]

db <- fpc::dbscan(df, eps = 0.15, MinPts = 5)
# Plot DBSCAN results
fviz_cluster(db, df, stand = FALSE, frame = FALSE, geom = "point")+
  theme_bw(
    base_size = 15
  )+
  theme(
    legend.position = "bottom"
  )

```

### the role of hyperparameters

::: {.r-stack}

::: {.fragment .fade-out}

{{< video https://www.youtube.com/watch?v=7Twnmhe948A  width="1200" height="500" >}}

:::

::: {.fragment .fade-in-then-out}

[A hyperparameter is a parameter, such as the learning rate or choice of optimizer, which specifies details of the learning process, hence the name hyperparameter. 
This is in contrast to parameters which determine the model itself.](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))

:::

::: {.fragment .fade-in-then-out}

$\epsilon$

:   Reachability Distance 

$MinPts$

:   Reachability minimum number of points 

:::

::: 

::: {.attribution}

[@ester1996densitybased]

:::


#### $MinPts$

```{r}

# db <- fpc::dbscan(df, eps = 0.15, MinPts = 5)

data("multishapes")

df <- multishapes[,1:2]

df_nst <- nest(df)

param_MinPts <- data.frame(
  MinPts = seq(0,20,5)
) |> 
  add_column(
    data = df_nst$data
  )

param_MinPts <- param_MinPts |> 
  mutate(
    dbscan_mdl = map2(
      MinPts,
      data,
      function(x,y) fpc::dbscan(y, eps = 0.15, MinPts = x) 
      ),
    dbscan_plt = map2(
      dbscan_mdl,
      MinPts,
      function(x,y) fviz_cluster(x,
                               df,
                               ellipse = TRUE,
                               geom = "point",
                               main = paste0("MinPts: ",y,", eps: 0.15"))+
        theme_bw(base_size = 15)+
        theme(legend.position = "none")
      )
  )

wrap_plots(param_MinPts$dbscan_plt)

```

#### $\epsilon$

```{r}

# db <- fpc::dbscan(df, eps = 0.15, MinPts = 5)

data("multishapes")

df <- multishapes[,1:2]

df_nst <- nest(df)

param_eps <- data.frame(
  eps = seq(0,0.6,0.15)
) |> 
  add_column(
    data = df_nst$data
  )

param_eps <- param_eps |> 
  mutate(
    dbscan_mdl = map2(
      eps,
      data,
      function(x,y) fpc::dbscan(y, eps = x, MinPts = 5) 
      ),
    dbscan_plt = map2(
      dbscan_mdl,
      eps,
      function(x,y) fviz_cluster(x,
                               df,
                               ellipse = TRUE,
                               geom = "point",
                               main = paste0("MinPts: 5, eps: ",y))+
        theme_bw(base_size = 15)+
        theme(legend.position = "none")
      )
  )

wrap_plots(param_eps$dbscan_plt)

```

#### parameter study

```{r}

data("multishapes")

df <- multishapes[,1:2]

df_nst <- nest(df)

param_study <- expand_grid(
  # eps = seq(0.05,0.5,0.15),
  eps = c(0.05,0.10,0.15,0.4),
  MinPts = seq(0,20,5)
) |> 
  add_column(
    data = df_nst$data
  )

param_study <- param_study |> 
  rowwise() |> 
  mutate(
    dbscan_mdl = list(fpc::dbscan(data, eps = eps, MinPts = MinPts)),
    clustered_data = list(dbscan_mdl |> pluck("cluster"))
      )

param_study <- param_study |> 
  unnest(cols = c(data,clustered_data)) |> 
  group_by(
    eps, MinPts, dbscan_mdl
  ) |> 
  nest() |> 
   mutate(
    sil_cmp = 
      map(
        data,
        function(x)
          silhouette(
            x |> filter(clustered_data != 0) |> pull("clustered_data"),
            dist(x |> filter(clustered_data!=0) |> select(x,y))
          )
      ),
    avg_sil = 
      map(
        sil_cmp,
        function(x)
          mean(x |> as.data.frame() |> pull("sil_width"))
      )
  )

study_plts <- param_study |> 
  ungroup() |> 
  mutate(
    plt_lbl = paste0("MinPts: ", MinPts," eps: ",eps),
    cluster_plt =
      map2(
        data,
        plt_lbl,
        function(x,y)
          x |> 
          filter(clustered_data != 0) |> 
          ggplot(
            aes(
              x = x, 
              y = y,
              color = as.factor(clustered_data)
              )
            )+
          geom_point()+
          labs(title = y)+
          scale_x_continuous(
            limits = c(-1.5,1.5)
          )+
          scale_y_continuous(
            limits = c(-3.5,1.5)
          )+
          theme_bw(
            # base_size = 15
            )+
          theme(legend.position = "none")
          ),
    sil_plt = 
      map2(
        sil_cmp,
        plt_lbl,
        function(x,y)
          fviz_silhouette(x, print.summary = F)+
          labs(caption = y)
      )
    )
  
```


```{r}
#| fig-height: 11
#| fig-width: 11
#| fig-align: "center"

wrap_plots(study_plts$cluster_plt,ncol = 5)+
  plot_annotation(
    title = "parameter study, noise points removed",
    theme = theme(plot.title = element_text(size = 25))
  )

```

#### silhouette computation

```{r}

avg_sil <- param_study |> 
  ungroup() |> 
  select(-dbscan_mdl,-data,-sil_cmp) |> 
  unnest(avg_sil) |> 
  mutate(
    eps = as_factor(eps),
    MinPts = as_factor(MinPts)
  )

avg_sil |> 
  ggplot(
    aes(
      x = eps,
      y = MinPts,
      fill = avg_sil
    )
  )+
  geom_tile()+
  labs(
    title = "parameter study",
    fill = "average silhouette width"
  )+
  scale_fill_scico(
    palette = "roma"
  )+
  scale_x_discrete(
    # expand = c(0,0,0,0)
  )+
  scale_y_discrete(
    # expand = c(0,0,0,0)
  )+
  theme_light(base_size = 15)+
  theme(
    legend.position = "bottom"
  )+
  annotate(
    geom = "tile",
    x = as.factor(c(0.05,0.1)),
    y = as.factor(c(20,20)),
    color = "white",
    fill = NA,
    size = 2
  )+
  annotate(
    geom = "tile",
    x = as.factor(0.15),
    y = as.factor(5),
    color = "blue",
    fill = NA,
    size = 2
  )

```

---

```{r}
#| fig-height: 8
#| fig-align: "center"

sil_select <- 
  study_plts |> 
  filter(eps %in% c(0.05,0.1),MinPts %in% 20)

gt_select <- 
  study_plts |> 
  filter(eps %in% c(0.15),MinPts %in% 5)

(sil_select$cluster_plt[[1]]+sil_select$cluster_plt[[2]]) / gt_select$cluster_plt[[1]] + labs(title = "ground truth at MinPts: 5 and eps = 0.15")
           
```

#### finding eps: knn $\rightarrow k = MinPts$

```{r}

findeps <- data.frame(
  MinPts = c(1,5,10,15,25)
) |> 
  mutate(
    knn_dat = map(MinPts,function(x) dbscan::kNNdist(df, k=x))
  ) |> 
  unnest(cols = knn_dat) |> 
  group_by(MinPts) |> 
  arrange(knn_dat, .by_group = TRUE) |> 
  mutate(
    idx = row_number(MinPts),
    MinPts = MinPts |> as_factor()
    )



findeps |> 
  ggplot(
    aes(
      x = idx, 
      y = knn_dat,
      linetype = MinPts
    )
  )+
  geom_line()+
  labs(
    title = "finding eps with kNN",
    x = "idx (sorted ascending)",
    y = "eps"
  )+
  scale_y_continuous(
    limits = c(0,0.4),
    expand = c(0,0,0,0),
    breaks = seq(0,1,0.025)
  )+
  scale_x_continuous(
    expand = c(0,0,0,0)
  )+
  annotate(
    geom = "label",
    x = c(100,100,100,100,100),
    y = c(0.008,0.028,0.045,0.06,0.085),
    label = c("1","5","10","15","25")
  )+
  theme_minimal(
    base_size = 15
  )

```

---

::: {.r-fit-text .v-c}

$MinPts?$

:::


---

::: {.incremental}

- $MinPts$ is selected based on the *domain knowledge.* 

- If you do not have domain understanding, a rule of thumb is to derive $MinPts$ from the number of dimensions $D$ in the data set. 

  * $MinPts \geq D + 1$ 

- For 2D data, take $MinPts = 4$. 

- For larger datasets, with much noise, it suggested to go with $MinPts = 2 * D$.

:::

##### Turns out ...

::: {.r-fit-text .v-c}

... knowing what you do helps!

:::

### Application for Accuracy - Technical Cleaniness

#### Workflow

![](img/TECSA_01.png){fig-align="center"}


#### TECSA Filter

![](img/filter.jpg){fig-align="center"}



#### Output

![](img/TECSA_02.png){fig-align="center"}



#### Repeatability?

![](img/particle_browser.png){fig-align="center"}


#### Challenge

In order to judge the process we need repeated measurements FROM the actual process.

... so we need to find the same particles again!

#### particles

![](img/particles_01.png){fig-align="center"}



#### clustering?

![](img/particles_02.png){fig-align="center"}


#### parameter search

![](img/param_search.png){fig-align="center"}


---

![](img/param_search_02.png){fig-align="center"}


#### Noise or Cluster?

![](img/noise_cluster.png){fig-align="center"}


#### Null Filter?

![](img/null_filter.png){fig-align="center"}



#### Spread?

![](img/cov.png){fig-align="center"}


#### length +/- sd

![](img/length_sd.png){fig-align="center"}



#### Cpk distribution

![](img/cpk.png){fig-align="center"}


#### Six Sigma and Big Data

![](img/six_sigma.png){fig-align="center"}




# References
