---
title: "Map Reduce"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
    include-in-header:
      text: |
        <script>
        MathJax = {
          loader: {
            load: ['[tex]/boldsymbol']
          },
          tex: {
            tags: "all",
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
            packages: {
              '[+]': ['boldsymbol']
            }
          }
        };
        </script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
  - pseudocode
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false

library(datasauRus)
library(tidyverse)
library(gt)
library(here)
library(quarto)
library(gganimate)

```



## MapReduce

### Single Node Architecture

![](img/single_node_arch.svg)

::: {.attribution}
[@LeskovecUnknownTitle2014]
:::

### Cluster Architecture

![](img/cluster_arch.svg)

Google has an estimated 1M servers <http://bit.ly/Shh0RO>

::: {.attribution}
[@LeskovecUnknownTitle2014]
:::

### SERVERS!!!

![](img/servers.jpg){.r-stretch}

### Large Scale Computing

- Challenges:
  - How do you distribute computations?
  - How can we make it easy to write distributed programs

Machines fail:

. . .

- One Server may stay up 3 years (1000 days)
- If there are 1000 servers, expect to loose 1/day
- Google estimate: 1M servers $\rightarrow$ 1000 machines fail EVERY DAY

::: {.attribution}
[@LeskovecUnknownTitle2014]
:::

### Issue and Solution

- Issue: Copying data over a network takes time
- Idea
  - Bring computation close to the data
  - Store files multiple times for reliability

. . .

- MapReduce adresses these problems
  - Elegant way to work with big data
  - Storage Infrastructure - File System (Google: GFS, Hadoop: HDFS)
  - Programming Model: MapReduce

::: {.attribution}
[@LeskovecUnknownTitle2014]
:::

### (D)istributed (F)ile (S)ystem

- Chunk servers
  - File is split into contiguous chunks
  - Typically each chunk is 16-64MB
  - Each chunk replicated (usually 2x or 3x)
  - Try to keep replicas in different racks

::: {.attribution}
[@LeskovecUnknownTitle2014]
:::

### Chunk Servers

![](img/chunk_servers.svg)

::: {.attribution}
[@LeskovecUnknownTitle2014]
:::

### Theory

{{< include img/_MapReduce_anim.qmd >}}

::: {.attribution}
[@LeskovecUnknownTitle2014]
:::

### Classroom

We will simulate MapReduce in the classroom

::::{style="font-size: 85%;"}

1. step: 4 students count the words in the raw data. Time is ticking.
2. step: 2 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
3. step: 4 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
4. step: 5 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
5. step: 10 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.
6. step: 20 groups are formed, the words are counted. One student acts as the reduce (collecting the counts in the end). Time is ticking.

::::


#### Classroom Evaluation

1. Which method took the longest?
2. What was the most balanced method?
3. Would more "servers" help?

### using Software

```{r}
#| include: false
#| 
clean <- function(x) {
  gsub('[,.;:\'"()]','',tolower(x))
}

word_count_simple <- function(lines) {
  chunks <- strsplit(clean(lines),'\\s')
  words <- do.call(c, chunks)
  table(words)
}

lorem <- read_lines(here("data","loremipsum_source.txt"))

lorem_smpl <- sample(
  lorem, 
  size = 500, 
  replace = TRUE,
  # prob = rep(c(0.05),500)
  )

writeLines(lorem_smpl, here("data","exchange.txt"))

quarto::quarto_render(input = here("data","_MapReduce_classroom.qmd"))

lorem_freq_tbl <- word_count_simple(lorem_smpl)

lorem_df <- lorem_freq_tbl %>%
  as.data.frame() %>%
  mutate(
    percentage = Freq/sum(Freq),
    percentage_round = round(percentage,digits = 4)
    ) |>
arrange(
  desc(Freq)
  ) |> 
  filter(percentage>0.02)

```

::: {.r-stack}

::: {.fragment .fade-out}

```{r}

lorem_df |> 
  ggplot(
    aes(
      x = Freq,
      y = reorder(words,Freq)
    )
  )+
  geom_col(
    fill = "steelblue"
  )+
  scale_x_continuous(
    expand = c(0,0,0.05,0)
  )+
  theme_bw(
    base_size = 15
  )+
  labs(
    title = "absolute frequencies of words",
    x = "count",
    y = ""
  )

```

:::

::: {.fragment .fade-in}

```{r}

lorem_df |> 
  ggplot(
    aes(
      x = percentage_round,
      y = reorder(words,percentage_round)
    )
  )+
  geom_col(
    fill = "steelblue"
  )+
  scale_x_continuous(
    labels = scales::percent,
    expand = c(0,0,0.05,0)
  )+
  theme_bw(
    base_size = 15
  )+
  labs(
    title = "relative frequencies of words",
    x = "percentage",
    y = ""
  )

```

:::

:::

### Dealing with Failures 

::: {.incremental}

::::{style="font-size: 85%;"}

- Map worker failure
  - Map tasks completed or in-progress at worker are reset to idle
  - Reduce workers are notified when task is rescheduled on another worker
- Reduce worker failure
  - Only in-progress tasks are reset to idle 
  - Reduce task is restarted
- Master failure
  - MapReduce task is aborted and client is notified

::::

:::

### How many MapReduce jobs?

$M$ map tasks, $R$ reduce tasks

Rule of thumb:

- Make $M$ much larger than the number of nodes in the cluster
- One DFS chunk per Map is common
- Improves dynammics load balancing and speeds up recovery from worker failures

. . .

Usually $R$ is smaller than $M$

- Because output is spread across $R$ files

## MapReduce summary

::: {.incremental}

::::{style="font-size: 85%;"}

- MapReduce is significant for its role in enabling the processing of massive datasets efficiently across distributed computing clusters.
- It revolutionized big data processing by providing a scalable and fault-tolerant framework for handling large-scale computations.
- Its simplicity and scalability made it accessible to a wide range of industries and applications, from web search engines to scientific research.
- MapReduce paved the way for the development of other big data processing frameworks and technologies, influencing the evolution of distributed computing paradigms.
- Its impact extends beyond its original implementation, as concepts and principles from MapReduce have influenced the design of subsequent systems and architectures for big data processing.

::::

:::




# References
