---
title: "Checking Cluster Validity"
bibliography: https://api.citedrive.com/bib/da291eae-8e2a-46b1-968e-0b79af55f87a/references.bib?x=eyJpZCI6ICJkYTI5MWVhZS04ZTJhLTQ2YjEtOTY4ZS0wYjc5YWY1NWY4N2EiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICI2ZDMyODMxM2QzZDQ3NmE3MGM4MDc5MzJiNzE1NjkzNjJmYmZjODYyNGMzNmJjNDBkMDk3Njk5N2RmZjQ5MTg5In0=/bibliography.bib
format:
  revealjs:
    slide-level: 5
    resource-path: 
      - "img"
    logo: img/back.svg
    footer-logo-link: "https://mechtrix.github.io/BigData_online/"
    footer: "Copyright Prof. Dr. Tim Weber, 2024"
revealjs-plugins:
  - animate
  - attribution
filters: 
  - animate
  - reveal-header
editor_options: 
  chunk_output_type: console
css: style.css
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(gt)
library(factoextra)
library(here)
library(patchwork)
library(hopkins)

```

## Cluster tendency {.incremental}

Does the data contain meaningful cluster (non-random structures)?

This is called: **assessing cluster tendency**

## the data

- iris data set
- random generated data

```{r}
#| echo: true

data("iris")

head(iris)

df <- iris[,-5]

```

## random data

```{r}

random_df <- 
  apply(df, 2,
        function(x){runif(length(x),min(x),(max(x)))})

df <- scale(df)
random_df <- scale(random_df)

```

## visual inspection

```{r}

fviz_pca_ind(
  prcomp(df),
  title = "iris data",
  habillage = iris$Species,
  palette = "jco",
  geom = "point",
  ggthemes = theme_classic(),
  legend = "bottom"
)+


fviz_pca_ind(
  prcomp(random_df),
  title = "random data",
  geom = "point",
  ggtheme = theme_classic()
)

```

## Quality ... again?

```{r}

km.res <- kmeans(random_df,3)

fviz_cluster(
  list(
    data = random_df,
    cluster = km.res$cluster
  ),
  ellipse.type = "norm",
  geom = "point",
  stand = FALSE,
  palette = "jco",
  ggtheme = theme_classic(),
  legend = "bottom",
  title = "kmeans on random data"
)+
fviz_dend(
  hclust(
    dist(
      random_df
    )
  ),
  k = 3,
  k_colors = "jco",
  as.ggplot = TRUE,
  show_labels = FALSE,
  main =  "hclust on random data"
)
```

## quantitative - Hopkins statistic

The Hopkins statistic [@HOPKINSUnknownTitle1954] is used to assess the clustering tendency of a data set by  measuring the probability that a given data set is generated by a uniform data distribution.

## how it works

::::{style="font-size: 75%;"}

Let $D$ be a real dataset, the Hopkins statistic is calculated as follows:

::: {.incremental}

1. Sample uniformly $n$ points ($p_1, \ldots , p_n $) from $D$
2. For each point $p_i \in D$, find its nearest neighbor $p_j$, then compute the distance between $p_i$ and $p_j$ and denote it as $x_i = dist(p_i,p_j)$
3. Generate a simulated dataset ($random_D$) drawn from a random uniform distribution with $n-$points and the same variation as the original real dataset $D$
4. For each point $q_i \in random_D$, find its nearest neighbor $q_j$ in $D$; then compute the distance between $q_i$ and $q_j$ and denote it $y_i = dist(q_i,q_j)$
5. Calculate the Hopkins statistics ($H$) as the mean nearest neighbor distance in the random dataset divided by the sum of the mean nearest neighbor distances in the real and across the simulated dataset.

:::

::: {.fragment .fade-in}

\begin{align}
H = \frac{\sum^n_{i = 1} y_i}{\sum^n_{i = 1} x_i+\sum^n_{i = 1} y_i}
\end{align}

:::

::::

### Interpretation

A value of $H$ about $0.5$ means that $\sum^n_{i = 1} x_i$ and $\sum^n_{i = 1} y_i$ are close to each other.

::: {.incremental}

$H_0$
:   the dataset $D$ is uniformly distributed (contains no meaningful clusters).

$H_a$
:   the dataset $D$ is **not** uniformly distributed (contains meaningful clusters).
:::

::: {.fragment .fade-in}

For the original dataset $H = `r hopkins(df,m=10) |> round(digits = 2)`>0.5$ with $p = `r hopkins.pval(hopkins(df,m=10),n = 10)`$

:::

::: {.fragment .fade-in}

For the random dataset $H = `r hopkins(random_df,m=10) |> round(digits = 2)`\approx 0.5$ with $p = `r hopkins.pval(hopkins(random_df,m=10),n = 10)`$

:::

## visual assessment of tendency (VAT)

::: {.incremental}

1. Compute the (dis)similarity matrix (DM) between the objects in the dataset using the Euclidean distance measure
2. Reorder the (DM) so that similar objects are close to another. This process creates an ordered dissimilarity matrix (ODM)
3. The ODM is displayed as an ordered dissimilarity image (ODI), which is the visual output of VAT

:::

### VAT

```{r}

fviz_dist(dist(df),
          show_labels = FALSE)+
  theme(
    legend.position = "bottom"
  )+
  labs(title = "iris data")+
fviz_dist(dist(random_df),
          show_labels = FALSE)+
  labs(title = "random data")+
  theme(
    legend.position = "bottom"
  )
  

```




# References
